{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uba40\ud2f0-GPU \uc608\uc81c\n==================\n\n\ub370\uc774\ud130 \ubcd1\ub82c \ucc98\ub9ac(Data Parallelism)\ub294 \ubbf8\ub2c8-\ubc30\uce58\ub97c \uc5ec\ub7ec \uac1c\uc758 \ub354 \uc791\uc740 \ubbf8\ub2c8-\ubc30\uce58\ub85c\n\uc790\ub974\uace0 \uac01\uac01\uc758 \uc791\uc740 \ubbf8\ub2c8\ubc30\uce58\ub97c \ubcd1\ub82c\uc801\uc73c\ub85c \uc5f0\uc0b0\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130 \ubcd1\ub82c \ucc98\ub9ac\ub294 ``torch.nn.DataParallel`` \uc744 \uc0ac\uc6a9\ud558\uc5ec \uad6c\ud604\ud569\ub2c8\ub2e4.\n``DataParallel`` \ub85c \uac10\uc300 \uc218 \uc788\ub294 \ubaa8\ub4c8\uc740 \ubc30\uce58 \ucc28\uc6d0(batch dimension)\uc5d0\uc11c\n\uc5ec\ub7ec GPU\ub85c \ubcd1\ub82c \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\nDataParallel\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\n\nclass DataParallelModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.block1 = nn.Linear(10, 20)\n\n        # wrap block2 in DataParallel\n        self.block2 = nn.Linear(20, 20)\n        self.block2 = nn.DataParallel(self.block2)\n\n        self.block3 = nn.Linear(20, 20)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CPU \ubaa8\ub4dc\uc778 \ucf54\ub4dc\ub97c \ubc14\uafc0 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n\nDataParallel\uc5d0 \ub300\ud55c \ubb38\uc11c\ub294 `\uc5ec\uae30 <http://pytorch.org/docs/nn.html#dataparallel>`_\n\uc5d0\uc11c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n**\ub798\ud551\ub41c \ubaa8\ub4c8\uc758 \uc18d\uc131**\n\n\ubaa8\ub4c8\uc744 ``DataParallel`` \ub85c \uac10\uc2fc \ud6c4\uc5d0\ub294 \ubaa8\ub4c8\uc758 \uc18d\uc131(\uc608. \uc0ac\uc6a9\uc790 \uc815\uc758 \uba54\uc18c\ub4dc)\uc5d0\n\uc811\uadfc\ud560 \uc218 \uc5c6\uac8c \ub429\ub2c8\ub2e4. \uc774\ub294 ``DataParallel`` \uc774 \uba87\uba87 \uc0c8\ub85c\uc6b4 \uba64\ubc84\ub97c \uc815\uc758\ud558\uae30 \ub584\ubb38\uc5d0\n\ub2e4\ub978 \uc18d\uc131\uc5d0 \uc811\uadfc\uc744 \ud5c8\uc6a9\ud558\ub294 \uac83\uc774 \ucda9\ub3cc\uc744 \uc77c\uc73c\ud0ac \uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\uadf8\ub798\ub3c4 \uc18d\uc131\uc5d0 \uc811\uadfc\ud558\uace0\uc790 \ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 ``DataParallel`` \uc758 \uc11c\ube0c\ud074\ub798\uc2a4\ub97c\n\uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyDataParallel(nn.DataParallel):\n    def __getattr__(self, name):\n        return getattr(self.module, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**DataParallel\uc774 \uad6c\ud604\ub41c \uae30\ubcf8\ud615(Primitive):**\n\n\n\uc77c\ubc18\uc801\uc73c\ub85c, PyTorch\uc758 `nn.parallel` \uae30\ubcf8\ud615\uc740 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uac04\ub2e8\ud55c MPI\ub958\uc758 \uae30\ubcf8\ud615\uc744 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n- \ubcf5\uc81c(replicate): \uc5ec\ub7ec \uae30\uae30\uc5d0 \ubaa8\ub4c8\uc744 \ubcf5\uc81c\ud569\ub2c8\ub2e4.\n- \ubd84\uc0b0(scatter): \uccab\ubc88\uc9f8 \ucc28\uc6d0\uc5d0\uc11c \uc785\ub825\uc744 \ubd84\uc0b0\ud569\ub2c8\ub2e4.\n- \uc218\uc9d1(gather): \uccab\ubc88\uc9f8 \ucc28\uc6d0\uc5d0\uc11c \uc785\ub825\uc744 \uc218\uc9d1\ud558\uace0 \ud569\uce69\ub2c8\ub2e4.\n- \ubcd1\ub82c\uc801\uc6a9(parallel\\_apply): \uc774\ubbf8 \ubd84\uc0b0\ub41c \uc785\ub825\uc758 \uc9d1\ud569\uc744 \uc774\ubbf8 \ubd84\uc0b0\ub41c \ubaa8\ub378\uc758\n  \uc9d1\ud569\uc5d0 \uc801\uc6a9\ud569\ub2c8\ub2e4.\n\n\ub354 \uba85\ud655\ud788 \uc54c\uc544\ubcf4\uae30 \uc704\ud574, \uc704 \uc694\uc18c \uc0ac\uc6a9\ud558\uc5ec \uad6c\uc131\ud55c ``data_parallel``\n\ud568\uc218\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def data_parallel(module, input, device_ids, output_device=None):\n    if not device_ids:\n        return module(input)\n\n    if output_device is None:\n        output_device = device_ids[0]\n\n    replicas = nn.parallel.replicate(module, device_ids)\n    inputs = nn.parallel.scatter(input, device_ids)\n    replicas = replicas[:len(inputs)]\n    outputs = nn.parallel.parallel_apply(replicas, inputs)\n    return nn.parallel.gather(outputs, output_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc758 \uc77c\ubd80\ub294 CPU, \uc77c\ubd80\ub294 GPU\uc5d0\uc11c\n--------------------------------------------\n\n\uc77c\ubd80\ub294 CPU\uc5d0\uc11c, \uc77c\ubd80\ub294 GPU\uc5d0\uc11c \uc2e0\uacbd\ub9dd\uc744 \uad6c\ud604\ud55c \uc9e7\uc740 \uc608\uc81c\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n\nclass DistributedModel(nn.Module):\n\n    def __init__(self):\n        super().__init__(\n            embedding=nn.Embedding(1000, 10),\n            rnn=nn.Linear(10, 10).to(device),\n        )\n\n    def forward(self, x):\n        # CPU\uc5d0\uc11c \uc5f0\uc0b0\ud569\ub2c8\ub2e4.\n        x = self.embedding(x)\n\n        # GPU\ub85c \ubcf4\ub0c5\ub2c8\ub2e4.\n        x = x.to(device)\n\n        # GPU\uc5d0\uc11c \uc5f0\uc0b0\ud569\ub2c8\ub2e4.\n        x = self.rnn(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc9c0\uae08\uae4c\uc9c0 \uae30\uc874 Torch \uc0ac\uc6a9\uc790\ub97c \uc704\ud55c \uac04\ub2e8\ud55c PyTorch \uac1c\uc694\ub97c \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4.\n\ubc30\uc6b8 \uac83\uc740 \uc544\uc8fc \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n``optim`` \ud328\ud0a4\uc9c0, \ub370\uc774\ud130 \ub85c\ub354 \ub4f1\uc744 \uc18c\uac1c\ud558\uace0 \uc788\ub294 \ub354 \ud3ec\uad04\uc801\uc778 \uc785\ubb38\uc6a9 \ud29c\ud1a0\ub9ac\uc5bc\uc744\n\ubcf4\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4: :doc:`/beginner/deep_learning_60min_blitz`.\n\n\ub610\ud55c, \ub2e4\uc74c\uc758 \ub0b4\uc6a9\ub4e4\ub3c4 \uc0b4\ud3b4\ubcf4\uc138\uc694.\n\n-  :doc:`Train neural nets to play video games </intermediate/reinforcement_q_learning>`\n-  `Train a state-of-the-art ResNet network on imagenet`_\n-  `Train an face generator using Generative Adversarial Networks`_\n-  `Train a word-level language model using Recurrent LSTM networks`_\n-  `\ub2e4\ub978 \uc608\uc81c\ub4e4 \ucc38\uace0\ud558\uae30`_\n-  `\ub354 \ub9ce\uc740 \ud29c\ud1a0\ub9ac\uc5bc \ubcf4\uae30`_\n-  `\ud3ec\ub7fc\uc5d0\uc11c PyTorch\uc5d0 \ub300\ud574 \uc598\uae30\ud558\uae30`_\n-  `Slack\uc5d0\uc11c \ub2e4\ub978 \uc0ac\uc6a9\uc790\uc640 \ub300\ud654\ud558\uae30`_\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}