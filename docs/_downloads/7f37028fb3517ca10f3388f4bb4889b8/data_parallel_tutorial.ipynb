{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uc120\ud0dd \uc0ac\ud56d: \ub370\uc774\ud130 \ubcd1\ub82c \ucc98\ub9ac (Data Parallelism)\n==========================\n**\uae00\uc4f4\uc774**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n**\ubc88\uc5ed**: '\uc815\uc544\uc9c4 <https://github.com/ajin-jng>'\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 ``DataParallel`` (\ub370\uc774\ud130 \ubcd1\ub82c) \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc95\uc744 \ubc30\uc6b0\uaca0\uc2b5\ub2c8\ub2e4.\n\nPyTorch\ub97c \ud1b5\ud574 GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uc27d\uc2b5\ub2c8\ub2e4. \uba3c\uc800, \ubaa8\ub378\uc744 GPU\uc5d0 \ub123\uc2b5\ub2c8\ub2e4:\n\n.. code:: python\n\n    device = torch.device(\"cuda:0\")\n    model.to(device)\n\n\uadf8 \ub2e4\uc74c\uc73c\ub85c\ub294 \ubaa8\ub4e0 Tensors \ub97c GPU\ub85c \ubcf5\uc0ac\ud569\ub2c8\ub2e4:\n\n.. code:: python\n\n    mytensor = my_tensor.to(device)\n\n''my_tensor.to(device)'' \ub97c \ud638\ucd9c \uc2dc \uc5d0\ub294 ''my_tensor'' \ub97c \ub2e4\uc2dc\uc4f0\ub294 \ub300\uc2e0 ''my_tensor'' \uc758 \ub610\ub2e4\ub978 \ubcf5\uc0ac\ubcf8\uc774 \uc0dd\uae34\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uae30\uc5b5\ud558\uc2ed\uc2dc\uc624. \n\ub2f9\uc2e0\uc740 \uadf8\uac83\uc744 \uc0c8\ub85c\uc6b4 tensor \uc5d0 \uc18c\uc18d\uc2dc\ud0a4\uace0 GPU\uc5d0 \uadf8 tensor\ub97c \uc368\uc57c\ud569\ub2c8\ub2e4.\n\n\uc5ec\ub7ec GPU\ub97c \ud1b5\ud574 \uc55e\uacfc \ub4a4\uc758 \uc804\ud30c\ub97c \uc2e4\ud589\ud558\ub294 \uac83\uc740 \ub2f9\uc5f0\ud55c \uc77c \uc785\ub2c8\ub2e4.\n\uadf8\ub7ec\ub098 PyTorch\ub294 \uae30\ubcf8\uc801 \ud558\ub098\uc758 GPU\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ``DataParallel`` \uc744 \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubcd1\ub82c\ub85c \uc2e4\ud589\ud558\uc5ec \ub2e4\uc218\uc758 GPU \uc5d0\uc11c \uc27d\uac8c \uc791\uc5c5\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n.. code:: python\n\n    model = nn.DataParallel(model)\n\n\uc774\uac83\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ud575\uc2ec\uc785\ub2c8\ub2e4. \uc790\uc138\ud55c \uc0ac\ud56d\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \uc544\ub798\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubd88\ub7ec\uc624\uae30\uc640 \ub9e4\uac1c\ubcc0\uc218\n----------------------\n\nPyTorch \ubaa8\ub4c8\uc744 \ubd88\ub7ec\uc624\uace0 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc815\uc758\ud558\uc2ed\uc2dc\uc624.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# \ub9e4\uac1c\ubcc0\uc218\uc640 DataLoaders\ninput_size = 5\noutput_size = 2\n\nbatch_size = 30\ndata_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uae30\uae30\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354\ubbf8(Dummy) \ub370\uc774\ud130\uc14b\n-------------\n\n\ub354\ubbf8(ramdom) \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4. Getitem \ub9cc \uad6c\ud604\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RandomDataset(Dataset):\n\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n                         batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac04\ub2e8\ud55c \ubaa8\ub378\n ------------\n\n \ub370\ubaa8\ub97c \uc704\ud574 \ubaa8\ub378\uc740 \uc785\ub825\uc744 \ubc1b\uace0 \uc120\ud615 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\uba70 \ucd9c\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 ``DataParallel`` \uc758 \uc5b4\ub5a4 \ubaa8\ub378 (CNN, RNN, Capsule Net \ub4f1) \uc5d0\uc11c\ub4e0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 input\uacfc output\uc758 \ud06c\uae30\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc548\uc5d0 print \ubb38\uc744 \ub123\uc5c8\uc2b5\ub2c8\ub2e4.\n \ubb34\uc5c7\uc774 \ubc30\uce58 \uc21c\uc704 (batch rank) 0 \uc5d0 \ud504\ub9b0\ud2b8 \ub418\ub294\uc9c0 \uc8fc\uc758 \uae4a\uac8c \ubd10\uc8fc\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n    # \uc6b0\ub9ac\uc758 \ubaa8\ub378\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uacfc \ub370\uc774\ud130 \ubcd1\ub82c\uc758 \uad6c\ud604\n-----------------------------\n\n\uc774\uac83\uc740 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ud575\uc2ec \ubd80\ubd84\uc785\ub2c8\ub2e4. \uba3c\uc800, model instance \ub97c \ub9cc\ub4e4\uace0 \uac00\uc9c0\uace0 \uc788\ub294 GPU\uac00 \uc5ec\ub7ec\uac1c\uc778\uc9c0 \ud655\uc778\ud574\uc57c\ud569\ub2c8\ub2e4.\n\ub9cc\uc57d \ub2e4\uc218\uc758 GPU\ub97c \ubcf4\uc720\uc911\uc774\ub77c\uba74, ``nn.DataParallel`` \uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ub798\ud551 (wrapping) \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\uadf8\ub7f0 \ub2e4\uc74c ``model.to(device)`` \ub97c \ud1b5\ud558\uc5ec \ubaa8\ub378\uc744 GPU\uc5d0 \ub123\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uc2e4\ud589\n-------------\n\n\uc774\uc81c \uc6b0\ub9ac\ub294 \uc785\ub825\uacfc \ucd9c\ub825 tensor\uc758 \ud06c\uae30\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacb0\uacfc\n-------\n\nGPU\uac00 \uc5c6\uac70\ub098 \ud558\ub098\uc778 \uacbd\uc6b0 30\uac1c\uc758 \uc785\ub825\uacfc 30\uac1c\uc758 \ucd9c\ub825\uc744 \uc77c\uad04 \ucc98\ub9ac\ud558\uba74 \ubaa8\ub378\uc774 \uc608\uc0c1\ub300\ub85c 30\uc744 \uc785\ub825\ubc1b\uace0 30\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n\ud558\uc9c0\ub9cc \ub9cc\uc57d \ub2f9\uc2e0\uc774 \ub2e4\uc218\uc758 GPU\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n2 GPUs\n~~~~~~\n\n2\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n\n.. code:: bash\n\n    # on 2 GPUs\n    Let's use 2 GPUs!\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n3 GPUs\n~~~~~~\n\n\ub9cc\uc57d \ub2f9\uc2e0\uc774 3\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n.. code:: bash\n\n    Let's use 3 GPUs!\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n8 GPUs\n~~~~~~~~~~~~~~\n\n\ub9cc\uc57d \ub2f9\uc2e0\uc774 8\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n\n.. code:: bash\n\n    Let's use 8 GPUs!\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc694\uc57d\n-------\n\nDataParallel\uc740 \ub2f9\uc2e0\uc758 \ub370\uc774\ud130\ub97c \uc790\ub3d9\uc73c\ub85c \ubd84\ud560\ud558\uace0 \uc5ec\ub7ec GPU\uc5d0 \uc788\ub294 \ub2e4\uc218\uc758 \ubaa8\ub378\uc5d0 \uc791\uc5c5\uc744 \uc9c0\uc2dc\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc774 \uc791\uc5c5\uc744 \uc644\ub8cc\ud558\uba74 DataParallel\uc740\n\uc0ac\uc6a9\uc790\uc5d0\uac8c \uacb0\uacfc\ub97c \ubc18\ud658\ud558\uae30 \uc804\uc5d0 \ubaa8\ub4e0 \uacb0\uacfc\ubb3c\ub4e4\uc744 \uc218\uc9d1\ud558\uc5ec \ubcd1\ud569\ud569\ub2c8\ub2e4.\n\n\ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc54c\uace0\uc2f6\ub2e4\uba74 \uc544\ub798 \uc8fc\uc18c\ub97c \ucc38\uace0\ud558\uc138\uc694.\nhttps://tutorials.pytorch.kr/beginner/former_torchies/parallelism_tutorial.html\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}