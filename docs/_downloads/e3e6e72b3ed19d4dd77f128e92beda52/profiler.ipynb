{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch Profiler\n====================================\nThis recipe explains how to use PyTorch profiler and measure the time and\nmemory consumption of the model's operators.\n\nIntroduction\n------------\nPyTorch includes a simple profiler API that is useful when user needs\nto determine the most expensive operators in the model.\n\nIn this recipe, we will use a simple Resnet model to demonstrate how to\nuse profiler to analyze model performance.\n\nSetup\n-----\nTo install ``torch`` and ``torchvision`` use the following command:\n\n::\n\n   pip install torch torchvision\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Steps\n-----\n\n1. Import all necessary libraries\n2. Instantiate a simple Resnet model\n3. Use profiler to analyze execution time\n4. Use profiler to analyze memory consumption\n5. Using tracing functionality\n\n1. Import all necessary libraries\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn this recipe we will use ``torch``, ``torchvision.models``\nand ``profiler`` modules:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.models as models\nimport torch.autograd.profiler as profiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Instantiate a simple Resnet model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet's create an instance of a Resnet model and prepare an input\nfor it:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Use profiler to analyze execution time\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch profiler is enabled through the context manager and accepts\na number of parameters, some of the most useful are:\n\n- ``record_shapes`` - whether to record shapes of the operator inputs;\n- ``profile_memory`` - whether to report amount of memory consumed by\n  model's Tensors;\n- ``use_cuda`` - whether to measure execution time of CUDA kernels.\n\nLet's see how we can use profiler to analyze the execution time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profiler.profile(record_shapes=True) as prof:\n    with profiler.record_function(\"model_inference\"):\n        model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we can use ``record_function`` context manager to label\narbitrary code ranges with user provided names\n(``model_inference`` is used as a label in the example above).\nProfiler allows one to check which operators were called during the\nexecution of a code range wrapped with a profiler context manager.\nIf multiple profiler ranges are active at the same time (e.g. in\nparallel PyTorch threads), each profiling context manager tracks only\nthe operators of its corresponding range.\nProfiler also automatically profiles the async tasks launched\nwith ``torch.jit._fork`` and (in case of a backward pass)\nthe backward pass operators launched with ``backward()`` call.\n\nLet's print out the stats for the execution above:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output will look like (omitting some columns):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -------------------------  --------------  ----------  ------------  ---------\n# Name                       Self CPU total   CPU total  CPU time avg  # Calls\n# -------------------------  --------------  ----------  ------------  ---------\n# model_inference            3.541ms         69.571ms    69.571ms      1\n# conv2d                     69.122us        40.556ms    2.028ms       20\n# convolution                79.100us        40.487ms    2.024ms       20\n# _convolution               349.533us       40.408ms    2.020ms       20\n# mkldnn_convolution         39.822ms        39.988ms    1.999ms       20\n# batch_norm                 105.559us       15.523ms    776.134us     20\n# _batch_norm_impl_index     103.697us       15.417ms    770.856us     20\n# native_batch_norm          9.387ms         15.249ms    762.471us     20\n# max_pool2d                 29.400us        7.200ms     7.200ms       1\n# max_pool2d_with_indices    7.154ms         7.170ms     7.170ms       1\n# -------------------------  --------------  ----------  ------------  ---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see that, as expected, most of the time is spent in convolution (and specifically in ``mkldnn_convolution``\nfor PyTorch compiled with MKL-DNN support).\nNote the difference between self cpu time and cpu time - operators can call other operators, self cpu time exludes time\nspent in children operator calls, while total cpu time includes it.\n\nTo get a finer granularity of results and include operator input shapes, pass ``group_by_input_shape=True``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n\n# (omitting some columns)\n# -------------------------  -----------  --------  -------------------------------------\n# Name                       CPU total    # Calls         Input Shapes\n# -------------------------  -----------  --------  -------------------------------------\n# model_inference            69.571ms     1         []\n# conv2d                     9.019ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]\n# convolution                9.006ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]\n# _convolution               8.982ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]\n# mkldnn_convolution         8.894ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]\n# max_pool2d                 7.200ms      1         [[5, 64, 112, 112]]\n# conv2d                     7.189ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]\n# convolution                7.180ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]\n# _convolution               7.171ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]\n# max_pool2d_with_indices    7.170ms      1         [[5, 64, 112, 112]]\n# -------------------------  -----------  --------  --------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Use profiler to analyze memory consumption\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch profiler can also show the amount of memory (used by the model's tensors)\nthat was allocated (or released) during the execution of the model's operators.\nIn the output below, 'self' memory corresponds to the memory allocated (released)\nby the operator, excluding the children calls to the other operators.\nTo enable memory profiling functionality pass ``profile_memory=True``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profiler.profile(profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------  ---------------  ---------------  ---------------\n# Name                         CPU Mem          Self CPU Mem     Number of Calls\n# ---------------------------  ---------------  ---------------  ---------------\n# empty                        94.79 Mb         94.79 Mb         123\n# resize_                      11.48 Mb         11.48 Mb         2\n# addmm                        19.53 Kb         19.53 Kb         1\n# empty_strided                4 b              4 b              1\n# conv2d                       47.37 Mb         0 b              20\n# ---------------------------  ---------------  ---------------  ---------------\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------  ---------------  ---------------  ---------------\n# Name                         CPU Mem          Self CPU Mem     Number of Calls\n# ---------------------------  ---------------  ---------------  ---------------\n# empty                        94.79 Mb         94.79 Mb         123\n# batch_norm                   47.41 Mb         0 b              20\n# _batch_norm_impl_index       47.41 Mb         0 b              20\n# native_batch_norm            47.41 Mb         0 b              20\n# conv2d                       47.37 Mb         0 b              20\n# convolution                  47.37 Mb         0 b              20\n# _convolution                 47.37 Mb         0 b              20\n# mkldnn_convolution           47.37 Mb         0 b              20\n# empty_like                   47.37 Mb         0 b              20\n# max_pool2d                   11.48 Mb         0 b              1\n# max_pool2d_with_indices      11.48 Mb         0 b              1\n# resize_                      11.48 Mb         11.48 Mb         2\n# addmm                        19.53 Kb         19.53 Kb         1\n# adaptive_avg_pool2d          10.00 Kb         0 b              1\n# mean                         10.00 Kb         0 b              1\n# ---------------------------  ---------------  ---------------  ---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Using tracing functionality\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nProfiling results can be outputted as a .json trace file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profiler.profile() as prof:\n    with profiler.record_function(\"model_inference\"):\n        model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "User can examine the sequence of profiled operators after loading the trace file\nin Chrome (``chrome://tracing``):\n\n![](../../_static/img/trace_img.png)\n\n   :scale: 25 %\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learn More\n----------\n\nTake a look at the following tutorial to learn how to visualize your model with TensorBoard:\n\n-  `Visualizing models, data, and training with TensorBoard <https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html>`_ tutorial\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}