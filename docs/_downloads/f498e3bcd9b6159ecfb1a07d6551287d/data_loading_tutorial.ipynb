{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uc0ac\uc6a9\uc790 \uc815\uc758 Dataset, Dataloader, Transforms \uc791\uc131\ud558\uae30\n==========================================================\n\n**\uc800\uc790** : Sasank Chilamkurthy <https://chsasank.github.io>\n**\ubc88\uc5ed** : \uc815\uc724\uc131 <https://github.com/Yunseong-Jeong>\n\n\uba38\uc2e0\ub7ec\ub2dd \ubb38\uc81c\ub97c \ud478\ub294 \uacfc\uc815\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc900\ube44\ud558\ub294\ub370 \ub9ce\uc740 \ub178\ub825\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\nPyTorch\ub294 \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\ub294 \uacfc\uc815\uc744 \uc27d\uac8c\ud574\uc8fc\uace0, \ub610 \uc798 \uc0ac\uc6a9\ud55c\ub2e4\uba74 \ucf54\ub4dc\uc758 \uac00\ub3c5\uc131\ub3c4 \ubcf4\ub2e4 \ub192\uc5ec\uc904 \uc218 \uc788\ub294 \ub3c4\uad6c\ub4e4\uc744\n\uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc77c\ubc18\uc801\uc774\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\uc14b\uc73c\ub85c\ubd80\ud130 \ub370\uc774\ud130\ub97c \uc77d\uc5b4\uc624\uace0\n\uc804\ucc98\ub9ac\ud558\uace0 \uc99d\uac00\ud558\ub294 \ubc29\ubc95\uc744 \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc9c4\ud589\ud558\uae30 \uc704\ud574 \uc544\ub798 \ud328\ud0a4\uc9c0\ub4e4\uc744 \uc124\uce58\ud574\uc8fc\uc138\uc694.\n\n-  ``scikit-image``: \uc774\ubbf8\uc9c0 I/O \uc640 \ubcc0\ud615\uc744 \uc704\ud574 \ud544\uc694\ud569\ub2c8\ub2e4.\n-  ``pandas``: CSV \ud30c\uc77c \ud30c\uc2f1\uc744 \ubcf4\ub2e4 \uc27d\uac8c \ud574\uc90d\ub2c8\ub2e4.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# \uacbd\uace0 \uba54\uc2dc\uc9c0 \ubb34\uc2dc\ud558\uae30\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # \ubc18\uc751\ud615 \ubaa8\ub4dc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e4\ub8f0 \ub370\uc774\ud130\uc14b\uc740 \uc544\ub798 \uc870\uac74\uacfc \uac19\uc740 \ub79c\ub4dc\ub9c8\ud06c(landmark)\uac00 \uc788\ub294 \uc5bc\uad74 \uc0ac\uc9c4\uc785\ub2c8\ub2e4.\n\n.. figure:: /_static/img/landmarked_face2.png\n   :width: 400\n\n\uac01\uac01\uc758 \uc5bc\uad74\uc5d0 68\uac1c\uc758 \uc11c\ub85c\ub2e4\ub978 \uc911\uc694 \ud3ec\uc778\ud2b8\ub4e4\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\uc774 `\ub9c1\ud06c <https://download.pytorch.org/tutorial/faces.zip>`_ \ub97c \ud1b5\ud574 \ub370\uc774\ud130\uc14b\uc744 \ub2e4\uc6b4\ub85c\ub4dc \ud574\uc8fc\uc138\uc694.\n    \ub2e4\uc6b4\ub85c\ub4dc\ud55c \ub370\uc774\ud130\uc14b\uc740 'data/faces/'\uc5d0 \uc704\uce58\ud574\uc57c \ud569\ub2c8\ub2e4.\n    \uc774 \ub370\uc774\ud130\uc14b\uc740 ImageNet\uc5d0\uc11c '\uc5bc\uad74'\uc774\ub77c\ub294 \ud0dc\uadf8\ub97c \uac00\uc9c4 \uba87\uba87 \uc774\ubbf8\uc9c0\ub4e4\uc5d0 \ub300\ud574\n    `dlib\uc758 pose estimation <https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ \uc744\n    \uc801\uc6a9\ud55c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4.</p></div>\n\n\n\ub370\uc774\ud130\uc14b\uc740 \uc544\ub798\uc640 \uac19\uc740 \ud2b9\uc9d5\uc744 \uac00\uc9c4 CSV \ud30c\uc77c\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n::\n\n    image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n    0805personali01.jpg,27,83,27,98, ... 84,134\n    1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n\n\uc774\uc81c CSV \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640\uc11c (N, 2) \ubc30\uc5f4\uc548\uc5d0 \uc788\ub294 \ub79c\ub4dc\ub9c8\ud06c\ub4e4\uc744 \uc7a1\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\nN\uc740 \ub79c\ub4dc\ub9c8\ud06c(landmarks)\uc758 \uac1c\uc218\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n\nn = 65\nimg_name = landmarks_frame.iloc[n, 0]\nlandmarks = landmarks_frame.iloc[n, 1:]\nlandmarks = np.asarray(landmarks)\nlandmarks = landmarks.astype('float').reshape(-1, 2)\n\nprint('Image name: {}'.format(img_name))\nprint('Landmarks shape: {}'.format(landmarks.shape))\nprint('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ubbf8\uc9c0\uc640 \ub79c\ub4dc\ub9c8\ud06c(landmark)\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uac04\ub2e8\ud55c \ud568\uc218\ub97c \uc791\uc131\ud574\ubcf4\uace0,\n\uc2e4\uc81c\ub85c \uc801\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    \"\"\" \ub79c\ub4dc\ub9c8\ud06c(landmark)\uc640 \uc774\ubbf8\uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # \uac31\uc2e0\uc774 \ub418\ub3c4\ub85d \uc7a0\uc2dc \uba48\ucda5\ub2c8\ub2e4.\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n               landmarks)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset \ud074\ub798\uc2a4\n----------------\n\n``torch.utils.data.Dataset`` \uc740 \ub370\uc774\ud130\uc14b\uc744 \ub098\ud0c0\ub0b4\ub294 \ucd94\uc0c1\ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n\uc5ec\ub7ec\ubd84\uc758 \ub370\uc774\ud130\uc14b\uc740 ``Dataset`` \uc5d0 \uc0c1\uc18d\ud558\uace0 \uc544\ub798\uc640 \uac19\uc774 \uc624\ubc84\ub77c\uc774\ub4dc \ud574\uc57c\ud569\ub2c8\ub2e4.\n\n-  ``len(dataset)`` \uc5d0\uc11c \ud638\ucd9c\ub418\ub294 ``__len__`` \uc740 \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30\ub97c \ub9ac\ud134\ud574\uc57c\ud569\ub2c8\ub2e4.\n-  ``dataset[i]`` \uc5d0\uc11c \ud638\ucd9c\ub418\ub294 ``__getitem__`` \uc740\n   $i$\\ \ubc88\uc9f8 \uc0d8\ud50c\uc744 \ucc3e\ub294\ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\n\uc774\uc81c \ub370\uc774\ud130\uc14b \ud074\ub798\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n``__init__`` \uc744 \uc0ac\uc6a9\ud574\uc11c CSV \ud30c\uc77c \uc548\uc5d0 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc77d\uc9c0\ub9cc,\n``__getitem__`` \uc744 \uc774\uc6a9\ud574\uc11c \uc774\ubbf8\uc9c0\uc758 \ud310\ub3c5\uc744 \ud569\ub2c8\ub2e4.\n\uc774 \ubc29\ubc95\uc740 \ubaa8\ub4e0 \uc774\ubbf8\uc9c0\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uc800\uc7a5\ud558\uc9c0 \uc54a\uace0 \ud544\uc694\ud560\ub54c\ub9c8\ub2e4 \uc77d\uae30 \ub54c\ubb38\uc5d0\n\uba54\ubaa8\ub9ac\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\ub370\uc774\ud130\uc14b\uc758 \uc0d8\ud50c\uc740  ``{'image': image, 'landmarks': landmarks}`` \uc758 \uc0ac\uc804 \ud615\ud0dc\ub97c \uac16\uc2b5\ub2c8\ub2e4.\n\uc120\ud0dd\uc801 \uc778\uc790\uc778 ``transform`` \uc744 \ud1b5\ud574 \ud544\uc694\ud55c \uc804\ucc98\ub9ac \uacfc\uc815\uc744 \uc0d8\ud50c\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub2e4\uc74c \uc7a5\uc5d0\uc11c \uc804\uc774 ``transform`` \uc758 \uc720\uc6a9\uc131\uc5d0 \ub300\ud574 \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): csv \ud30c\uc77c\uc758 \uacbd\ub85c\n            root_dir (string): \ubaa8\ub4e0 \uc774\ubbf8\uc9c0\uac00 \uc874\uc7ac\ud558\ub294 \ub514\ub809\ud1a0\ub9ac \uacbd\ub85c\n            transform (callable, optional): \uc0d8\ud50c\uc5d0 \uc801\uc6a9\ub420 Optional transform\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud074\ub798\uc2a4\ub97c \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uace0 \ub370\uc774\ud130 \uc0d8\ud50c\uc744 \ud1b5\ud574\uc11c \ubc18\ubcf5\ud574\ubd05\uc2dc\ub2e4.\n\uccab\ubc88\uc9f8 4\uac1c\uc758 \uc0d8\ud50c\uc758 \ud06c\uae30\ub97c \ucd9c\ub825 \ud558\uace0, \uc0d8\ud50c\ub4e4\uc758 \ub79c\ub4dc\ub9c8\ud06c(landmarks)\ub97c \ubcf4\uc5ec\uc904 \uac83 \uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n                                    root_dir='data/faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforms\n----------\n\n\uc704\uc5d0\uc11c \ubcfc \uc218 \uc788\uc5c8\ub358 \ud55c\uac00\uc9c0 \ubb38\uc81c\uc810\uc740 \uc0d8\ud50c\ub4e4\uc774 \ub2e4 \uac19\uc740 \uc0ac\uc774\uc988\uac00 \uc544\ub2c8\ub77c\ub294 \uac83\uc785\ub2c8\ub2e4.\n\ub300\ubd80\ubd84\uc758 \uc2e0\uacbd\ub9dd(neural networks)\uc740 \uace0\uc815\ub41c \ud06c\uae30\uc758 \uc774\ubbf8\uc9c0\ub77c\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.\n\uadf8\ub7ec\ubbc0\ub85c \uc6b0\ub9ac\ub294 \uc2e0\uacbd\ub9dd\uc5d0 \uc8fc\uae30 \uc804\uc5d0 \ucc98\ub9ac\ud560 \uacfc\uc815\uc744 \uc791\uc131\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n3\uac00\uc9c0\uc758 transforms \uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4:\n-  ``Rescale``: \uc774\ubbf8\uc9c0\uc758 \ud06c\uae30\ub97c \uc870\uc808\ud569\ub2c8\ub2e4.\n-  ``RandomCrop``: \uc774\ubbf8\uc9c0\ub97c \ubb34\uc791\uc704\ub85c \uc790\ub985\ub2c8\ub2e4.\n                   \uc774\uac83\uc744 data augmentation\uc774\ub77c \ud569\ub2c8\ub2e4.\n-  ``ToTensor``: numpy \uc774\ubbf8\uc9c0\uc5d0\uc11c torch \uc774\ubbf8\uc9c0\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.\n                 (\ucd95\ubcc0\ud658\uc774 \ud544\uc694\ud569\ub2c8\ub2e4)\n\n\uac04\ub2e8\ud55c \ud568\uc218\ub300\uc2e0\uc5d0 \ud638\ucd9c \ud560 \uc218 \uc788\ub294 \ud074\ub798\uc2a4\ub85c \uc791\uc131 \ud569\ub2c8\ub2e4.\n\uc774\ub807\uac8c \ud55c\ub2e4\uba74, \ud074\ub798\uc2a4\uac00 \ud638\ucd9c \ub420 \ub54c\ub9c8\ub2e4 \uc804\uc774(Transform)\uc758 \ub9e4\uac1c\ubcc0\uc218\uac00 \uc804\ub2ec \ub418\uc9c0 \uc54a\uc544\ub3c4 \ub429\ub2c8\ub2e4.\n\uc774\uc640 \uac19\uc774, ``__call__`` \ud568\uc218\ub97c \uad6c\ud604\ud574\uc57c \ud569\ub2c8\ub2e4.\n\ud544\uc694\ud558\ub2e4\uba74, ``__init__`` \ud568\uc218\ub3c4 \uad6c\ud604\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 \uc804\uc774(transform)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n::\n\n    tsfm = Transform(params)\n    transformed_sample = tsfm(sample)\n\n\uc544\ub798\uc5d0\uc11c\ub294 \uc774\ubbf8\uc9c0\uc640 \ub79c\ub4dc\ub9c8\ud06c(landmark)\ub4e4\uc744 \uc5b4\ub5bb\uac8c \uc801\uc6a9\ud558\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n    \"\"\"\uc8fc\uc5b4\uc9c4 \uc0ac\uc774\uc988\ub85c \uc0d8\ud50c\ud06c\uae30\ub97c \uc870\uc815\ud569\ub2c8\ub2e4.\n\n    Args:\n        output_size(tuple or int) : \uc6d0\ud558\ub294 \uc0ac\uc774\uc988 \uac12\n            tuple\uc778 \uacbd\uc6b0 \ud574\ub2f9 tuple(output_size)\uc774 \uacb0\uacfc\ubb3c(output)\uc758 \ud06c\uae30\uac00 \ub418\uace0,\n            int\ub77c\uba74 \ube44\uc728\uc744 \uc720\uc9c0\ud558\uba74\uc11c, \uae38\uc774\uac00 \uc791\uc740 \ucabd\uc774 output_size\uac00 \ub429\ub2c8\ub2e4.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        landmarks = landmarks * [new_w / w, new_h / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\n\nclass RandomCrop(object):\n    \"\"\"\uc0d8\ud50c\ub370\uc774\ud130\ub97c \ubb34\uc791\uc704\ub85c \uc790\ub985\ub2c8\ub2e4.\n\n    Args:\n        output_size (tuple or int): \uc904\uc774\uace0\uc790 \ud558\ub294 \ud06c\uae30\uc785\ub2c8\ub2e4.\n                        int\ub77c\uba74, \uc815\uc0ac\uac01\ud615\uc73c\ub85c \ub098\uc62c \uac83 \uc785\ub2c8\ub2e4.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left, top]\n\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass ToTensor(object):\n    \"\"\"numpy array\ub97c tensor(torch)\ub85c \ubcc0\ud658 \uc2dc\ucf1c\uc90d\ub2c8\ub2e4.\"\"\"\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'landmarks': torch.from_numpy(landmarks)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compose transforms\n~~~~~~~~~~~~~~~~~~\n\n\uc774\uc81c, \uc0d8\ud50c\uc5d0 \uc804\uc774(transform)\ub97c \uc801\uc6a9\ud574 \ubd05\uc2dc\ub2e4.\n\n\uc774\ubbf8\uc9c0\uc758 \uac00\uc7a5 \uc9e7\uc740 \uce21\uba74\uc744 256\uac1c\ub85c rescale\ud558\uace0,\n\uadf8\ud6c4\uc5d0 \ubb34\uc791\uc704\ub85c 224\uac1c\ub97c \uc790\ub978\ub2e4\uace0 \uac00\uc815\ud569\uc2dc\ub2e4.\n\ub2e4\uc2dc\ub9d0\ud574, ``Rescale`` \uacfc ``RandomCrop`` \uc744 \uc0ac\uc6a9\ud574\ubd05\uc2dc\ub2e4.\n\n``torchvision.transforms.Compose`` \ub294 \uc704\uc758 \ub450\uc791\uc5c5\uc744 \ud558\ub294 \uac04\ub2e8\ud55c \ud638\ucd9c\ud560 \uc218 \uc788\ub294 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scale = Rescale(256)\ncrop = RandomCrop(128)\ncomposed = transforms.Compose([Rescale(256),\n                               RandomCrop(224)])\n\n# Apply each of the above transforms on sample.\nfig = plt.figure()\nsample = face_dataset[65]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tsfrm).__name__)\n    show_landmarks(**transformed_sample)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub370\uc774\ud130\uc14b\uc744 \uc774\uc6a9\ud55c \ubc18\ubcf5\uc791\uc5c5\n-----------------------------\n\n\uc804\uc774(transform)\ub97c \uc801\uc6a9\ud55c dataset\uc744 \ub9cc\ub4e4\uae30\uc704\ud574\uc11c \ub9cc\ub4e4\uc5c8\ub358\uac83\uc744 \ub2e4 \uc9d1\uc5b4 \ub123\uc5b4 \ubd05\uc2dc\ub2e4.\n\n\uc694\uc57d\ud558\uc790\uba74, \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc0d8\ud50c\ub9c1 \ub429\ub2c8\ub2e4.\n\n-  \uc774\ubbf8\uc9c0\ub294 \ud30c\uc77c \uc804\uccb4\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uc62c\ub9ac\uc9c0\uc54a\uace0 \ud544\uc694\ud560\ub54c\ub9c8\ub2e4 \ubd88\ub7ec\uc640\uc11c \uc77d\uc2b5\ub2c8\ub2e4.\n-  \uadf8 \ud6c4\uc5d0 \uc77d\uc740 \uc774\ubbf8\uc9c0\uc5d0 Transform\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.\n-  transfroms \uc911 \ud558\ub098\uac00 \ub79c\ub364\uc774\uae30 \ub54c\ubb38\uc5d0, \ub370\uc774\ud130\ub294 \uc0d8\ud50c\ub9c1\ub54c \uc99d\uac00\ud569\ub2c8\ub2e4.\n\n\n\uc6b0\ub9ac\ub294 \uc774\uc81c \uc774\uc804\uc5d0 \uc0ac\uc6a9\ud558\ub358 \uac83 \ucc98\ub7fc ``for i in range`` \ub97c \uc0ac\uc6a9\ud574\uc11c\n\uc0dd\uc131\ub41c \ub370\uc774\ud130\uc14b\uc744 \ubc18\ubcf5 \uc791\uc5c5\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n                                           root_dir='data/faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uadf8\ub7ec\ub098, \ub370\uc774\ud130 \uc0c1\uc5d0\uc11c \ubc18\ubcf5\ud558\ub294 ``for`` \ubb38\uc740 \ub9ce\uc740 \ud2b9\uc9d5(features)\ub97c \ub193\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ud2b9\ud788, \uc544\ub798\uc640 \uac19\uc740 \uac83\uc744 \ub193\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n-  \ub370\uc774\ud130\ub97c \ubb36\ub294 \uacfc\uc815\n-  \ub370\uc774\ud130\ub97c \uc11e\ub294 \uacfc\uc815\n-  \ubcd1\ub82c\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c ``multiprocessing`` \uc744 \uc0ac\uc6a9\ud560\ub54c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\ub294 \uac83\n\n``torch.utils.data.DataLoder`` \ub294 \uc704\uc640 \uac19\uc740 \uae30\ub2a5\uc744 \ubaa8\ub450 \uc81c\uacf5\ud574\uc8fc\ub294 \ubc18\ubcf5\uc790(iterator)\uc785\ub2c8\ub2e4.\n\uc0ac\uc6a9\ub418\ub294 \ub9e4\uac1c\ubcc0\uc218(Parameters)\ub294 \uba85\ud655\ud574\uc57c \ud569\ub2c8\ub2e4.\n``collate_fn`` \ub294 \ud765\ubbf8\ub85c\uc6b4 \ub9e4\uac1c\ubcc0\uc218(Parameters) \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.\n``collate_fn`` \uc744 \uc774\uc6a9\ud558\uc5ec \uc0d8\ud50c\ub4e4\uc744 \uc815\ud655\ud558\uac8c \ubc30\uce58\ud558\ub294 \ubc29\ubc95\uc744 \uba85\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub7ec\ub098, \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0\uc5d0 \ub300\ud574\uc11c \uc815\ud655\ud558\uac8c \uc791\ub3d9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# \ubc30\uce58\ud558\ub294 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ud568\uc218\uc785\ub2c8\ub2e4.\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n    grid_border_size = 2\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Afterword: torchvision\n----------------------\n\n\uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294, \ub370\uc774\ud130\uc14b \uc791\uc131\uacfc \uc0ac\uc6a9, \uc804\uc774(transforms), \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubd24\uc2b5\ub2c8\ub2e4.\n``torchvision`` \ud328\ud0a4\uc9c0\ub294 \uba87\uba87\uc758 \uc77c\ubc18\uc801\uc778 \ub370\uc774\ud130\uc14b\uacfc \uc804\uc774(transforms)\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\ud074\ub798\uc2a4\ub4e4\uc744 \ub530\ub85c \uc791\uc131\ud558\uc9c0 \uc54a\uc544\ub3c4 \ub420 \uac83\uc785\ub2c8\ub2e4.\ntorchvision\uc5d0\uc11c\uc758 \uc0ac\uc6a9\uac00\ub2a5\ud55c \uc77c\ubc18\uc801\uc778 \ub370\uc774\ud130\uc14b \uc911 \ud558\ub098\ub294 ``ImageFolder`` \uc785\ub2c8\ub2e4.\n\uc774\uac83\uc740 \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4: ::\n\n    root/ants/xxx.png\n    root/ants/xxy.jpeg\n    root/ants/xxz.png\n    .\n    .\n    .\n    root/bees/123.jpg\n    root/bees/nsdf3.png\n    root/bees/asd932_.png\n\n\uc5ec\uae30\uc11c'ants', 'bees'\ub294 class labels\uc785\ub2c8\ub2e4.\n\ube44\uc2b7\ud558\uac8c, ``RandomHorizontalFlip`` , ``Scale`` \uacfc \uac19\uc774  ``PIL.Image`` \uc5d0\uc11c \uc791\ub3d9\ud558\ub294\n\uc77c\ubc18\uc801\uc778 \uc804\uc774(transforms)\ub3c4 \uc0ac\uc6a9\uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774\uc640 \uac19\uc774 \ub370\uc774\ud130\ub85c\ub354(dataloader)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: ::\n\n  import torch\n  from torchvision import transforms, datasets\n\n  data_transform = transforms.Compose([\n          transforms.RandomSizedCrop(224),\n          transforms.RandomHorizontalFlip(),\n          transforms.ToTensor(),\n          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n      ])\n  hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n                                             transform=data_transform)\n  dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n                                               batch_size=4, shuffle=True,\n                                               num_workers=4)\n\n training code\uc5d0 \ub300\ud55c \uc608\uc2dc\ub97c \uc54c\uace0 \uc2f6\ub2e4\uba74,\n :doc:`transfer_learning_tutorial` \ubb38\uc11c\ub97c \ucc38\uace0\ud574\uc8fc\uc138\uc694\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}