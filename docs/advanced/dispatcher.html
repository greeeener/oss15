


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dispatcher in C++ &mdash; PyTorch Tutorials 1.6.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="가지치기 기법(Pruning) 튜토리얼" href="../intermediate/pruning_tutorial.html" />
    <link rel="prev" title="Autograd in C++ Frontend" href="cpp_autograd.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<body class="pytorch-body">
  <nav class="navbar sticky-top navbar-dark fixed-top navbar-expand-lg" style="background: rgba(55,55,55,.8)">
    <div class="container-fluid">
      <div class="navbar-brand">
        <a href="https://pytorch.kr/" aria-label="PyTorch">
          <img src="../_static/images/logo-kr.svg" width="260" height="28" fill="white" />
        </a>
      </div>
      <button type="button" aria-label="Toggle navigation" class="navbar-toggler collapsed" aria-expanded="false" aria-controls="nav-collapse"><span class="navbar-toggler-icon"></span></button>
      <div id="nav-collapse" class="navbar-collapse collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a href="//pytorch.kr/" target="_self" class="nav-link">홈</a></li>
          <li class="nav-item">
            <a href="//tutorials.pytorch.kr/" target="_self" class="nav-link">튜토리얼</a>
          </li>
          <li class="nav-item">
            <a href="//pytorch.kr/about" target="_self" class="nav-link">
              소개
            </a></li>
        </ul>
      </div>
    </div>
  </nav>

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.6.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">모든 레시피 보기</a></li>
</ul>
<p class="caption"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">파이토치(PyTorch)로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/torchvision_tutorial.html">TorchVision 객체 검출 미세조정(Finetuning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_preprocessing_tutorial.html">torchaudio Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">nn.Transformer 와 TorchText 로 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">TorchText로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_translation_tutorial.html">TorchText로 언어 번역하기</a></li>
</ul>
<p class="caption"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/flask_rest_api_tutorial.html">Flask를 이용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
</ul>
<p class="caption"><span class="caption-text">프론트엔드 API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intermediate/named_tensor_tutorial.html">(prototype) Introduction to Named Tensors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_autograd.html">Autograd in C++ Frontend</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dispatcher in C++</a></li>
</ul>
<p class="caption"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/quantized_transfer_learning_tutorial.html">(beta) 컴퓨터 비전(Vision) 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
</ul>
<p class="caption"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/model_parallel_tutorial.html">단일 머신을 이용한 모델 병렬화 실습 예제</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/aws_distributed_training_tutorial.html">(advanced) PyTorch 1.0 Distributed Trainer with Amazon AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Dispatcher in C++</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced/dispatcher.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">advanced/dispatcher</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="dispatcher-in-c">
<h1>Dispatcher in C++<a class="headerlink" href="#dispatcher-in-c" title="Permalink to this headline">¶</a></h1>
<p>The dispatcher is an internal component of PyTorch which is responsible for
figuring out what code should actually get run when you call a function like
<code class="docutils literal notranslate"><span class="pre">torch::add</span></code>.  This can be nontrivial, because PyTorch operations need
to handle a lot of cross-cutting concerns that are “layered” on top of one
of another.  Here is a sampling of some of the things it handles:</p>
<ul class="simple">
<li><p>Switching between the CPU and CUDA implementations of an operator, depending
on the devices of the input tensors.</p></li>
<li><p>Switching between the autograd and backend implementations of an operator,
depending on whether or not autograd handling is necessary.</p></li>
<li><p>Applying autocasting when necessary for automatic mixed precision.</p></li>
<li><p>Applying batching rules when an operator is run under a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> call.</p></li>
<li><p>Tracing execution of operations, if you are tracing a model for export.</p></li>
</ul>
<p>If in your <a class="reference external" href="torch_script_custom_ops">custom operator code</a> you find yourself
manually writing if statements to handle these cases, the dispatcher APIs can
help organize your code.  (Conversely, if your custom operator is very simple
and is only for CPU inference, you probably don’t need to use the dispatcher,
just use the basic API.)</p>
<p>In this tutorial, we will describe how to structure a custom operator
registration to use the dispatcher to organize various components.  We’ll
assume that you are familiar with how to
<a class="reference external" href="torch_script_custom_ops">register an operator</a> and how to write
a <a class="reference external" href="cpp_autograd">custom autograd function</a>.</p>
<div class="section" id="defining-schema-and-backend-implementations">
<h2>Defining schema and backend implementations<a class="headerlink" href="#defining-schema-and-backend-implementations" title="Permalink to this headline">¶</a></h2>
<p>The general principle behind the dispatcher is that it divides the
implementation of an operator into multiple kernels, each of which implements
functionality for a specific <em>dispatch key</em>; for example, CPU, CUDA or Autograd.
The dispatcher determines what the highest priority dispatch key is at the time
you call an operator (this is done by looking at both the tensor arguments as
well as some thread local state), and transfers control to the kernel for that
dispatch key.  The end effect is that when you call an operator, we first
execute the Autograd kernel, and then we redispatch to the CPU or CUDA kernel
depending on the device types of the passed in tensors.</p>
<p>Let’s take a look at the various parts involved in making this
happen.  First, we must define the schema for the operator in question.
Unlike simple pybind11-style operator registration, we don’t actually
provide an implementation of our operator at this point; we just
provide a schema string specifying the type signature of the operator
that all of our other kernels will abide by:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;myadd(Tensor self, Tensor other) -&gt; Tensor&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Next, we need to actually provide some implementations of this operator.
For concreteness, here is a really simple implementation of addition on CPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">myadd_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self_</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other_</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">self_</span><span class="p">.</span><span class="n">sizes</span><span class="p">()</span> <span class="o">==</span> <span class="n">other_</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">other_</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="n">self</span> <span class="o">=</span> <span class="n">self_</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">other</span> <span class="o">=</span> <span class="n">other_</span><span class="p">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sizes</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>
  <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">self_ptr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">other_ptr</span> <span class="o">=</span> <span class="n">other</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">result_ptr</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">result</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">other_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We’d like to register this function as an implementation of <code class="docutils literal notranslate"><span class="pre">myops::myadd</span></code>.
However, the simple way of registering it (<code class="docutils literal notranslate"><span class="pre">def(&quot;myadd&quot;,</span> <span class="pre">myadd_cpu)</span></code>) would
register the kernel to run in all cases, even if the tensor is not a CPU
tensor!  (Internally, we refer to these as “catch-all” kernels, since they
catch all cases.)  To ensure that <code class="docutils literal notranslate"><span class="pre">myadd_cpu</span></code> is only run for
CPU tensors, we can use the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> macro:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">CPU</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> lets us register implementations for operators on
a specific dispatch key (in this case, CPU).  Each call to <code class="docutils literal notranslate"><span class="pre">impl</span></code>
associates a CPU kernel with the corresponding operator (which we previously
defined in the <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> block).  If we also have a CUDA implementation <code class="docutils literal notranslate"><span class="pre">myadd_cuda</span></code>,
we can register it in a separate <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> block:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">CUDA</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_cuda</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>These registrations can be split across files or even across library boundaries; so
for example, you could have these two <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks compiled
into a separate <code class="docutils literal notranslate"><span class="pre">myops_cpu</span></code> and <code class="docutils literal notranslate"><span class="pre">myops_cuda</span></code> dynamic libraries.  Generally,
speaking, the structure of your registrations will look like this:</p>
<ol class="arabic simple">
<li><p>A single <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code> that lists every custom operator in your namespace
in a centralized place.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> per dispatch key that registers implementations for
that key (e.g., CPU or CUDA).  If you like, you can further subdivide
<code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks into a block per operator. This is convenient
if you have a separate file per operator implementation, but don’t want to
expose the operators in a header; you can just put the registration in the
cpp file that defines your operator.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Did you know that you can also write <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code> blocks for existing
core operators in PyTorch?  This is how XLA support for PyTorch is
implemented: the <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> library contains a <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code>
that provides implementations for all basic operators on the XLA dispatch
key.</p>
</div>
</div>
<div class="section" id="adding-autograd-support">
<h2>Adding autograd support<a class="headerlink" href="#adding-autograd-support" title="Permalink to this headline">¶</a></h2>
<p>At this point, we have an operator with both CPU and CUDA implementations.  How
can we add autograd support to it?  As you might guess, we will register an
autograd kernel (similar to what’s described in the <a class="reference external" href="cpp_autograd">custom autograd function</a> tutorial)!
However, there is a twist: unlike the CPU and CUDA kernels, the autograd kernel
needs to <em>redispatch</em>: it needs to call back into the dispatcher to get to
the final CPU and CUDA implementations.</p>
<p>Thus, before we write the autograd kernel, let’s write a <em>dispatching function</em>
which calls into the dispatcher to find the right kernel for your operator.
This function constitutes the public C++ API for your operators–in fact, all of
the tensor functions in PyTorch’s C++ API all call the dispatcher in the same
way under the hood.  Here’s what the dispatching function looks like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">myadd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">static</span> <span class="k">auto</span> <span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">Dispatcher</span><span class="o">::</span><span class="n">singleton</span><span class="p">()</span>
    <span class="p">.</span><span class="n">findSchemaOrThrow</span><span class="p">(</span><span class="s">&quot;myops::myadd&quot;</span><span class="p">,</span> <span class="s">&quot;&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">typed</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">myadd</span><span class="p">)</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">op</span><span class="p">.</span><span class="n">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Let’s break it down:</p>
<ul>
<li><p>In the first line, we look up a typed operator handle from the dispatcher
corresponding to the operator that we are going to dispatch to.
<code class="docutils literal notranslate"><span class="pre">findSchemaOrThrow</span></code> takes two arguments: the (namespace qualified) name
of the operator, and the overload name of the operator (typically just
the empty string).  <code class="docutils literal notranslate"><span class="pre">typed</span></code> casts the dynamically typed handle into
a statically typed handle (doing a runtime test to make sure you’ve given
the correct C++ type), so that we can do a normal C++ call on it.  We
pass it <code class="docutils literal notranslate"><span class="pre">decltype(myadd)</span></code> since the type of the dispatching function is
the same as the type of the underlying kernels registered to the dispatcher.</p>
<p>For performance, this computation is done in a static variable, so that
we only need to do the (slow) lookup once.  If you typoed the name of the
operator you want to call, this lookup will error the first time you call this
function.</p>
</li>
<li><p>In the second line, we simply <code class="docutils literal notranslate"><span class="pre">call</span></code> the operator handle with all of the
arguments passed into the dispatching function.  This will actually invoke
the dispatcher and in the end control will be transferred to whatever kernel
is appropriate for this call.</p></li>
</ul>
<p>With the dispatch function in hand, we can now write the autograd kernel:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddFunction</span> <span class="o">:</span> <span class="k">public</span> <span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">Function</span><span class="o">&lt;</span><span class="n">MyAddFunction</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">static</span> <span class="n">Tensor</span> <span class="n">forward</span><span class="p">(</span>
      <span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">at</span><span class="o">::</span><span class="n">AutoNonVariableTypeMode</span> <span class="n">g</span><span class="p">;</span>
    <span class="k">return</span> <span class="nf">myadd</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">static</span> <span class="n">tensor_list</span> <span class="n">backward</span><span class="p">(</span><span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">tensor_list</span> <span class="n">grad_outputs</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">grad_output</span> <span class="o">=</span> <span class="n">grad_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">};</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="n">Tensor</span> <span class="nf">myadd_autograd</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="n">MyAddFunction</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The autograd function is written as normal using <code class="docutils literal notranslate"><span class="pre">torch::autograd::Function</span></code>,
except that instead of directly writing the implementation in <code class="docutils literal notranslate"><span class="pre">forward()</span></code>,
we:</p>
<ol class="arabic simple">
<li><p>Turn off autograd handling with the <code class="docutils literal notranslate"><span class="pre">at::AutoNonVariableTypeMode</span></code> RAII
guard, and then</p></li>
<li><p>Call the dispatch function <code class="docutils literal notranslate"><span class="pre">myadd</span></code> to call back into the dispatcher.</p></li>
</ol>
<p>Without (1), your calls will infinite loop (and stack overflow), because
<code class="docutils literal notranslate"><span class="pre">myadd</span></code> will send you back to this function (as the highest priority dispatch
key would still be autograd.) With (1),
autograd is excluded from the set of dispatch keys under consideration, and
we will go to the next handlers, which will either be CPU and CUDA.</p>
<p>We can now register this function in the same way we registered the CPU/CUDA
functions:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">Autograd</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_autograd</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="going-beyond-autograd">
<h2>Going beyond autograd<a class="headerlink" href="#going-beyond-autograd" title="Permalink to this headline">¶</a></h2>
<p>In some sense, the dispatcher isn’t doing all that much: all it does is
implement a glorified if-statement, along the lines of this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddFunction</span> <span class="o">:</span> <span class="p">...</span> <span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
  <span class="k">static</span> <span class="n">Tensor</span> <span class="n">forward</span><span class="p">(</span>
    <span class="n">AutogradContext</span> <span class="o">*</span><span class="n">ctx</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">add_cpu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">add_cuda</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">&quot;Unsupported device &quot;</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">().</span><span class="n">type</span><span class="p">());</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>So why use the dispatcher?  There are a few reasons:</p>
<ol class="arabic simple">
<li><p>It is decentralized.  You can assemble all of the pieces of an operator
(CPU, CUDA, Autograd) without having to write a single, centralized
if statement that refers to all of them.  Importantly, third parties can
register extra implementations for other aspects without having to patch the
original definition of an operator.</p></li>
<li><p>It supports more dispatch keys than CPU, CUDA and Autograd.  You can
see a full list of dispatch keys that are currently implemented
in PyTorch in <code class="docutils literal notranslate"><span class="pre">c10/core/DispatchKey.h</span></code>.  These dispatch keys
implement a variety of optional functionality for operators, and if you
decide you want your custom operator to support this functionality,
all you have to register a kernel for the appropriate key.</p></li>
<li><p>The dispatcher implements support for boxed fallback functions, which
are functions that can be implemented once and apply to all operators
in the system.  Boxed fallbacks can be used to provide default behavior
for a dispatch key; if you use the dispatcher to implement your operator,
you also opt into the fallbacks for all of these operations.</p></li>
</ol>
<p>Here are some particular dispatch keys which you may need to define an operator
for.</p>
<div class="section" id="autocast">
<h3>Autocast<a class="headerlink" href="#autocast" title="Permalink to this headline">¶</a></h3>
<p>The Autocast dispatch key implements support for
<a class="reference external" href="https://developer.nvidia.com/automatic-mixed-precision">automatic mixed precision</a>
(AMP).  An autocast kernel typically modifies the operation of an operator by casting the
input arguments to some precision before carrying out the operation.  For some
operations, it is numerically safe to cast to lower precision, which is how AMP
can achieve speed ups and reduced memory usage without sacrificing much
accuracy.  A nontrivial autocast kernel looks something like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span> <span class="nf">mymatmul_autocast</span><span class="p">(</span><span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="k">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ExcludeDispatchKeyGuard</span> <span class="n">no_autocast</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DispatchKey</span><span class="o">::</span><span class="n">Autocast</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">mymatmul</span><span class="p">(</span><span class="n">autocast</span><span class="o">::</span><span class="n">_cast</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="n">self</span><span class="p">),</span> <span class="n">autocast</span><span class="o">::</span><span class="n">_cast</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kHalf</span><span class="p">,</span> <span class="n">other</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Notice that, like our autograd kernels, we exclude the <code class="docutils literal notranslate"><span class="pre">Autocast</span></code> key from
dispatch before redispatching.  By default, if no autocast kernel is provided,
we simply fallthrough directly to the regular operator implementation (no
autocasting occurs.) (We didn’t use <code class="docutils literal notranslate"><span class="pre">myadd</span></code> for this example, since pointwise
addition doesn’t do autocasting and should just fall through).</p>
<p>When should an autocast kernel be registered? Unfortunately, there aren’t
cut-and-dry rules for when you should cast to a lower precision.  You can
get a sense for what operators have autocasting behavior by looking at
the <a class="reference external" href="https://pytorch.org/docs/master/amp.html#op-specific-behavior">AMP documentation</a>.  Some other
general rules:</p>
<ul class="simple">
<li><p>Operations that do reductions should be carried out in float32,</p></li>
<li><p>Any operation with multiple float tensor inputs has to standardize them
to a common precision, and</p></li>
<li><p>Any operation that does a convolution or gemm under the hood should
probably be float16</p></li>
</ul>
</div>
<div class="section" id="batched">
<h3>Batched<a class="headerlink" href="#batched" title="Permalink to this headline">¶</a></h3>
<p>Batched tensors allow you to write your code in a per-example manner, and then
have them be automatically batched when run under a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> invocation.  The
API for writing batching rules is currently under development, but once it is
stabilized, you can add support for <code class="docutils literal notranslate"><span class="pre">vmap</span></code> for your operators by registering
a kernel at the Batched dispatch key.</p>
</div>
<div class="section" id="tracer">
<h3>Tracer<a class="headerlink" href="#tracer" title="Permalink to this headline">¶</a></h3>
<p>The Tracer dispatch key implements support for recording invocations of operators
into a trace when you run <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code>.  We intend to provide a
boxed fallback that will implement tracing for arbitrary operations,
see <a class="reference external" href="https://github.com/pytorch/pytorch/issues/41478">issue #41478</a> to track
progress.</p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../intermediate/pruning_tutorial.html" class="btn btn-neutral float-right" title="가지치기 기법(Pruning) 튜토리얼" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="cpp_autograd.html" class="btn btn-neutral" title="Autograd in C++ Frontend" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="helpful-hr hr-top">
      <div class="helpful-container">
        <div class="helpful-question">이 문서가 도움이 되었나요?</div>
        <div class="helpful-question yes-link" data-behavior="was-this-helpful-event" data-response="yes">네</div>
        <div class="helpful-question no-link" data-behavior="was-this-helpful-event" data-response="no">아니오</div>
        <div class="was-helpful-thank-you">피드백을 주셔서 감사합니다.</div>
      </div>
    <hr class="helpful-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Dispatcher in C++</a><ul>
<li><a class="reference internal" href="#defining-schema-and-backend-implementations">Defining schema and backend implementations</a></li>
<li><a class="reference internal" href="#adding-autograd-support">Adding autograd support</a></li>
<li><a class="reference internal" href="#going-beyond-autograd">Going beyond autograd</a><ul>
<li><a class="reference internal" href="#autocast">Autocast</a></li>
<li><a class="reference internal" href="#batched">Batched</a></li>
<li><a class="reference internal" href="#tracer">Tracer</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-3', 'auto');
  ga('send', 'pageview');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: 'Download',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   $("[data-behavior='was-this-helpful-event']").on('click', function(){
    $(".helpful-question").hide();
    $(".was-helpful-thank-you").show();
    ga('send', {
      hitType: 'event',
      eventCategory: 'Was this Helpful?',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   if (location.pathname == "/") {
     $(".helpful-container").hide();
     $(".hr-bottom").hide();
   }
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="footer-container container">
      <div class="footer-logo-wrapper"><a href="https://pytorch.kr" class="footer-logo"></a></div>
      <div class="footer-links-wrapper pb-2">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org">PyTorch 홈페이지 (공식)</a></li>
            <li><a href="https://pytorch.org">공식 홈페이지</a></li>
            <li><a href="https://pytorch.org/tutorials">공식 튜토리얼</a></li>
            <li><a href="https://pytorch.org/docs">공식 문서</a></li>
          </ul>
        </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr">한국어 홈페이지 (비공식)</a></li>
            <li><a href="https://pytorch.kr/about" class="">사이트 소개</a></li>
            <li><a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a></li>
            <li><a href="https://github.com/9bow/PyTorch-tutorials-kr" target="_blank">한국어 튜토리얼 저장소</a></li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>