


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>챗봇 튜토리얼 &mdash; PyTorch Tutorials 1.6.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<body class="pytorch-body">
  <nav class="navbar sticky-top navbar-dark fixed-top navbar-expand-lg" style="background: rgba(55,55,55,.8)">
    <div class="container-fluid">
      <div class="navbar-brand">
        <a href="https://pytorch.kr/" aria-label="PyTorch">
          <img src="../_static/images/logo-kr.svg" width="260" height="28" fill="white" />
        </a>
      </div>
      <button type="button" aria-label="Toggle navigation" class="navbar-toggler collapsed" aria-expanded="false" aria-controls="nav-collapse"><span class="navbar-toggler-icon"></span></button>
      <div id="nav-collapse" class="navbar-collapse collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a href="//pytorch.kr/" target="_self" class="nav-link">홈</a></li>
          <li class="nav-item">
            <a href="//tutorials.pytorch.kr/" target="_self" class="nav-link">튜토리얼</a>
          </li>
          <li class="nav-item">
            <a href="//pytorch.kr/about" target="_self" class="nav-link">
              소개
            </a></li>
        </ul>
      </div>
    </div>
  </nav>

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.6.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">모든 레시피 보기</a></li>
</ul>
<p class="caption"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="deep_learning_60min_blitz.html">파이토치(PyTorch)로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/torchvision_tutorial.html">TorchVision 객체 검출 미세조정(Finetuning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="audio_preprocessing_tutorial.html">torchaudio Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="transformer_tutorial.html">nn.Transformer 와 TorchText 로 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_sentiment_ngrams_tutorial.html">TorchText로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchtext_translation_tutorial.html">TorchText로 언어 번역하기</a></li>
</ul>
<p class="caption"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/flask_rest_api_tutorial.html">Flask를 이용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
</ul>
<p class="caption"><span class="caption-text">프론트엔드 API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/named_tensor_tutorial.html">(prototype) Introduction to Named Tensors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Dispatcher in C++</a></li>
</ul>
<p class="caption"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/quantized_transfer_learning_tutorial.html">(beta) 컴퓨터 비전(Vision) 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
</ul>
<p class="caption"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/model_parallel_tutorial.html">단일 머신을 이용한 모델 병렬화 실습 예제</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="aws_distributed_training_tutorial.html">(advanced) PyTorch 1.0 Distributed Trainer with Amazon AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>챗봇 튜토리얼</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/beginner/chatbot_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">beginner/chatbot_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-beginner-chatbot-tutorial-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="sphx-glr-beginner-chatbot-tutorial-py">
<span id="id1"></span><h1>챗봇 튜토리얼<a class="headerlink" href="#sphx-glr-beginner-chatbot-tutorial-py" title="Permalink to this headline">¶</a></h1>
<dl class="simple">
<dt><strong>Author:</strong> <a class="reference external" href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/lewha0">김진현</a></p>
</dd>
</dl>
<p>이 튜토리얼에서는 순환(recurrent) 시퀀스 투 시퀀스(sequence-to-sequence)
모델의 재미있고 흥미로운 사용 예를 살펴보려 합니다. 간단한 챗봇을 학습해
볼 텐데, 사용할 데이터는 영화 대본으로 구성된 <a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs(코넬
대학교의 영화 속 대화 말뭉치 데이터</a>
입니다.</p>
<p>대화형 모델은 많은 사람들이 관심을 갖는 인공지능 분야의 연구 주제입니다.
고객 서비스와 관련된 활용, 온라인 헬프데스크 등 여러 상황에서 챗봇을
활용할 수 있습니다. 많은 챗봇이 검색 기반(retrieval-based) 모델을
사용하는데, 이는 특정한 형식을 갖춘 질문에 대해 미리 정해진 반응을
출력하는 방식입니다. 분야를 특정 회사의 IT 헬프데스크처럼 한정짓는다면
이러한 모델을 사용해도 충분합니다. 그러나 이런 모델은 좀 더 일반적인
상황에 활용할 수 있을만큼 강력하진 않습니다. 기계를 학습시켜서 사람과
여러 주제에 대해 의미 있는 대화를 하게끔 하는 것은 아직 해결되지 않은
연구 주제입니다. 그러나 최근에 딥러닝이 유행하면서 여러 가지의 강력한
생성 모델이 등장했습니다. 그러한 모델의 한 예인 구글의 <a class="reference external" href="https://arxiv.org/abs/1506.05869">신경 대화
모델(Neural Conversational Model)</a> 은
다중 도메인 대화 생성 모델(multi-domain generative conversational models)
분야에 있어서 큰 진전을 이루었습니다. 우리는 이 튜토리얼을 통해 이러한
모델을 PyTorch로 구현해보려 합니다.</p>
<div class="figure align-center">
<img alt="bot" src="../_images/bot.png" />
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>&gt; hello? (안녕하세요?)
Bot: hello . (안녕하세요.)
&gt; where am I? (여긴 어디죠?)
Bot: you re in a hospital . (병원입니다.)
&gt; who are you? (당신은 누구시죠?)
Bot: i m a lawyer . (변호사입니다.)
&gt; how are you doing? (어떻게 지내세요?)
Bot: i m fine . (잘 지냅니다.)
&gt; are you my friend? (당신은 제 친구인가요?)
Bot: no . (아뇨.)
&gt; you&#39;re under arrest (당신을 체포하겠습니다)
Bot: i m trying to help you ! (난 당신을 도우려 하는 겁니다!)
&gt; i&#39;m just kidding (농담이었어요)
Bot: i m sorry . (미안하네요.)
&gt; where are you from? (어디서 오셨어요?)
Bot: san francisco . (샌프란시스코요.)
&gt; it&#39;s time for me to leave (전 이제 가봐야겠네요)
Bot: i know . (알겠습니다.)
&gt; goodbye (안녕히 계세요)
Bot: goodbye . (안녕히 가세요.)
</pre></div>
</div>
<p><strong>이 튜토리얼의 핵심 내용</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">코넬 대학교의 영화 속 대화 말뭉치 데이터셋</a> 을
읽어오고 전처리합니다</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1508.04025">Luong의 어텐션(attention) 메커니즘</a> 을
이용하여 sequence-to-sequence 모델을 구현합니다</p></li>
<li><p>미니배치를 이용하여 인코더와 디코더를 함께 학습합니다</p></li>
<li><p>탐욕적 탐색 기법(greedy-search)을 사용하는 디코더 모듈을 구현합니다</p></li>
<li><p>학습한 챗봇과 대화를 나눠 봅니다</p></li>
</ul>
<p><strong>감사의 글</strong></p>
<p>이 튜토리얼은 다음 자료의 도움을 받아 작성하였습니다.</p>
<ol class="arabic simple">
<li><p>Yuan-Kuei Wu의 pytorch-chatbot 구현체:
<a class="reference external" href="https://github.com/ywk991112/pytorch-chatbot">https://github.com/ywk991112/pytorch-chatbot</a></p></li>
<li><p>Sean Robertson의 practical-pytorch seq2seq-translation 예제:
<a class="reference external" href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation">https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</a></p></li>
<li><p>FloydHub의 코넬 대학교의 영화 말뭉치 데이터 전처리 코드:
<a class="reference external" href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus">https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</a></p></li>
</ol>
<div class="section" id="id3">
<h2>준비 단계<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>시작에 앞서, <a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">여기</a> 에서
ZIP 파일 형태의 데이터를 내려받고, 현재 디렉토리 아래에 <code class="docutils literal notranslate"><span class="pre">data/</span></code> 라는
디렉토리를 만들어서 내려받은 데이터를 옮겨두시기 바랍니다.</p>
<p>그 다음에는, 몇 가지 필요한 도구들을 import 하겠습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="k">import</span> <span class="n">script</span><span class="p">,</span> <span class="n">trace</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="k">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="k">import</span> <span class="nb">open</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h2>데이터 읽기 &amp; 전처리하기<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>다음 단계는 데이터 파일의 형식을 재조정한 후, 우리가 작업하기 편한
구조로 읽어들이는 것입니다.</p>
<p><a class="reference external" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">코넬 대학교의 영화 속 대화 말뭉치 데이터셋</a> 은
영화 속 등장 인물의 대화가 풍부하게 포함된 데이터셋입니다.</p>
<ul class="simple">
<li><p>영화 속 등장 인물 10,292 쌍이 대화를 220,579번 주고받습니다</p></li>
<li><p>영화 617개의 등장 인물 9,035명이 나옵니다</p></li>
<li><p>총 발화(utterance) 수는 304,713번입니다</p></li>
</ul>
<p>이 데이터셋은 규모도 크고 내용도 다양하며, 격식체와 비격식체, 여러
시간대, 여러 감정 상태 등이 두루 포함되어 있습니다. 우리의 바람은
이러한 다양성으로 인해 모델이 견고해지는, 즉 모델이 여러 종류의 입력
및 질의에 잘 대응할 수 있게 되는 것입니다.</p>
<p>우선은 원본 데이터 파일을 몇 줄 살펴보면서 형식이 어떻게 되어있는지
살펴 보겠습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_name</span> <span class="o">=</span> <span class="s2">&quot;cornell movie-dialogs corpus&quot;</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">printLines</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">datafile</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">datafile</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[:</span><span class="n">n</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

<span class="n">printLines</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_lines.txt&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>b&#39;L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n&#39;
b&#39;L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n&#39;
b&#39;L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n&#39;
b&#39;L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n&#39;
b&quot;L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let&#39;s go.\n&quot;
b&#39;L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n&#39;
b&quot;L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you&#39;re gonna need to learn how to lie.\n&quot;
b&#39;L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n&#39;
b&#39;L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\&#39;m kidding.  You know how sometimes you just become this &quot;persona&quot;?  And you don\&#39;t know how to quit?\n&#39;
b&#39;L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n&#39;
</pre></div>
</div>
<div class="section" id="id5">
<h3>원하는 형식의 데이터 파일로 만들기<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>편의를 위해 데이터의 형식을 원하는 형태로 만들려고 합니다. 각 줄에
<em>질의 문장</em> 과 <em>응답 문장</em> 의 쌍이 탭으로 구분되어 있게끔 하는 것입니다.</p>
<p>다음의 함수를 통해 <em>movie_lines.txt</em> 원본 데이터 파일을 파싱하려
합니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loadLines</span></code> 는 파일에 포함된 대사를 변환하여 항목(대사 ID <code class="docutils literal notranslate"><span class="pre">lineID</span></code>,
인물 ID <code class="docutils literal notranslate"><span class="pre">characterID</span></code>, 영화 ID <code class="docutils literal notranslate"><span class="pre">movieID</span></code>, 인물 <code class="docutils literal notranslate"><span class="pre">character</span></code>, 대사
내용 <code class="docutils literal notranslate"><span class="pre">text</span></code>)에 대한 사전 형태로 변환합니다</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loadConversations</span></code> 는 <code class="docutils literal notranslate"><span class="pre">loadLines</span></code> 를 통해 읽어들인
대사(<code class="docutils literal notranslate"><span class="pre">lines</span></code>)의 항목(<code class="docutils literal notranslate"><span class="pre">fields</span></code>)를 <em>movie_conversations.txt</em> 에 나와
있는 내용에 맞춰 대화 형태로 묶습니다</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">extractSentencePairs</span></code> 는 대화(<code class="docutils literal notranslate"><span class="pre">conversations</span></code>)에서 문장 쌍을
추출합니다</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 파일에 포함된 대사를 쪼개서 항목에 대한 사전(``dict``) 형태로 변환합니다</span>
<span class="k">def</span> <span class="nf">loadLines</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">fields</span><span class="p">):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;iso-8859-1&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; +++$+++ &quot;</span><span class="p">)</span>
            <span class="c1"># 항목을 추출합니다</span>
            <span class="n">lineObj</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fields</span><span class="p">):</span>
                <span class="n">lineObj</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">lines</span><span class="p">[</span><span class="n">lineObj</span><span class="p">[</span><span class="s1">&#39;lineID&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">lineObj</span>
    <span class="k">return</span> <span class="n">lines</span>


<span class="c1"># 대사의 항목을 *movie_conversations.txt* 를 참고하여 대화 형태로 묶습니다</span>
<span class="k">def</span> <span class="nf">loadConversations</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">fields</span><span class="p">):</span>
    <span class="n">conversations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;iso-8859-1&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; +++$+++ &quot;</span><span class="p">)</span>
            <span class="c1"># 항목을 추출합니다</span>
            <span class="n">convObj</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fields</span><span class="p">):</span>
                <span class="n">convObj</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># 문자열을 리스트로 변환합니다(convObj[&quot;utteranceIDs&quot;] == &quot;[&#39;L598485&#39;, &#39;L598486&#39;, ...]&quot;)</span>
            <span class="n">utterance_id_pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;L[0-9]+&#39;</span><span class="p">)</span>
            <span class="n">lineIds</span> <span class="o">=</span> <span class="n">utterance_id_pattern</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;utteranceIDs&quot;</span><span class="p">])</span>
            <span class="c1"># 대사를 재구성합니다</span>
            <span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">lineId</span> <span class="ow">in</span> <span class="n">lineIds</span><span class="p">:</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="n">lineId</span><span class="p">])</span>
            <span class="n">conversations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convObj</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conversations</span>


<span class="c1"># conversations에서 문장 쌍을 추출합니다</span>
<span class="k">def</span> <span class="nf">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
    <span class="n">qa_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">conversation</span> <span class="ow">in</span> <span class="n">conversations</span><span class="p">:</span>
        <span class="c1"># 대화를 이루는 각 대사에 대해 반복문을 수행합니다</span>
        <span class="c1"># 대화의 마지막 대사는 (그에 대한 응답이 없으므로) 무시합니다</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">inputLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">targetLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="c1"># 잘못된 샘플은 제거합니다(리스트가 하나라도 비어 있는 경우)</span>
            <span class="k">if</span> <span class="n">inputLine</span> <span class="ow">and</span> <span class="n">targetLine</span><span class="p">:</span>
                <span class="n">qa_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">inputLine</span><span class="p">,</span> <span class="n">targetLine</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">qa_pairs</span>
</pre></div>
</div>
<p>이제 이 함수들을 호출하여 새로운 파일인 <em>formatted_movie_lines.txt</em> 를
만듭니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 새 파일에 대한 경로를 정의합니다</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;formatted_movie_lines.txt&quot;</span><span class="p">)</span>

<span class="n">delimiter</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>
<span class="c1"># 구분자에 대해 unescape 함수를 호출합니다</span>
<span class="n">delimiter</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">codecs</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">delimiter</span><span class="p">,</span> <span class="s2">&quot;unicode_escape&quot;</span><span class="p">))</span>

<span class="c1"># 대사 사전(dict), 대화 리스트(list), 그리고 각 항목의 이름을 초기화합니다</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">conversations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MOVIE_LINES_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lineID&quot;</span><span class="p">,</span> <span class="s2">&quot;characterID&quot;</span><span class="p">,</span> <span class="s2">&quot;movieID&quot;</span><span class="p">,</span> <span class="s2">&quot;character&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">MOVIE_CONVERSATIONS_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;character1ID&quot;</span><span class="p">,</span> <span class="s2">&quot;character2ID&quot;</span><span class="p">,</span> <span class="s2">&quot;movieID&quot;</span><span class="p">,</span> <span class="s2">&quot;utteranceIDs&quot;</span><span class="p">]</span>

<span class="c1"># 대사(lines)를 읽어들여 대화(conversations)로 재구성합니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Processing corpus...&quot;</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">loadLines</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_lines.txt&quot;</span><span class="p">),</span> <span class="n">MOVIE_LINES_FIELDS</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Loading conversations...&quot;</span><span class="p">)</span>
<span class="n">conversations</span> <span class="o">=</span> <span class="n">loadConversations</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_conversations.txt&quot;</span><span class="p">),</span>
                                  <span class="n">lines</span><span class="p">,</span> <span class="n">MOVIE_CONVERSATIONS_FIELDS</span><span class="p">)</span>

<span class="c1"># 결과를 새로운 csv 파일로 저장합니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Writing newly formatted file...&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outputfile</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">outputfile</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="n">delimiter</span><span class="p">,</span> <span class="n">lineterminator</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

<span class="c1"># 몇 줄을 예제 삼아 출력해 봅니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample lines from file:&quot;</span><span class="p">)</span>
<span class="n">printLines</span><span class="p">(</span><span class="n">datafile</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Processing corpus...

Loading conversations...

Writing newly formatted file...

Sample lines from file:
b&quot;Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we&#39;d start with pronunciation, if that&#39;s okay with you.\n&quot;
b&quot;Well, I thought we&#39;d start with pronunciation, if that&#39;s okay with you.\tNot the hacking and gagging and spitting part.  Please.\n&quot;
b&quot;Not the hacking and gagging and spitting part.  Please.\tOkay... then how &#39;bout we try out some French cuisine.  Saturday?  Night?\n&quot;
b&quot;You&#39;re asking me out.  That&#39;s so cute. What&#39;s your name again?\tForget it.\n&quot;
b&quot;No, no, it&#39;s my fault -- we didn&#39;t have a proper introduction ---\tCameron.\n&quot;
b&quot;Cameron.\tThe thing is, Cameron -- I&#39;m at the mercy of a particularly hideous breed of loser.  My sister.  I can&#39;t date until she does.\n&quot;
b&quot;The thing is, Cameron -- I&#39;m at the mercy of a particularly hideous breed of loser.  My sister.  I can&#39;t date until she does.\tSeems like she could get a date easy enough...\n&quot;
b&#39;Why?\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n&#39;
b&quot;Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\tThat&#39;s a shame.\n&quot;
b&#39;Gosh, if only we could find Kat a boyfriend...\tLet me see what I can do.\n&#39;
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3>데이터 읽고 정리하기<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>다음에 해야 할 일은 어휘집을 만들고, 질의/응답 문장 쌍을 메모리로
읽어들이는 것입니다.</p>
<p>우리가 다루는 대상은 일련의 <strong>단어</strong> 들이며, 따라서 이들을 이산 공간 상의
수치(discrete numerical space)로 자연스럽게 대응시키기 어렵다는 점에
유의하시기 바랍니다. 따라서 우리는 데이터셋 안에 들어 있는 단어를 인덱스
값으로 변환하는 매핑을 따로 만들어야 합니다.</p>
<p>이를 위해 우리는 <code class="docutils literal notranslate"><span class="pre">Voc</span></code> 라는 클래스를 만들어 단어에서 인덱스로의
매핑, 인덱스에서 단어로의 역 매핑, 각 단어의 등장 횟수, 전체 단어 수
등을 관리하려 합니다. 이 클래스는 어휘집에 새로운 단어를 추가하는
메서드(<code class="docutils literal notranslate"><span class="pre">addWord</span></code>), 문장에 등장하는 모든 단어를 추가하는
메서드(<code class="docutils literal notranslate"><span class="pre">addSentence</span></code>), 그리고 자주 등장하지 않는 단어를 정리하는
메서드(<code class="docutils literal notranslate"><span class="pre">trim</span></code>)를 제공합니다. 단어를 정리하는 내용에 대해서는 뒤에서
좀 더 자세히 살펴보겠습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 기본 단어 토큰 값</span>
<span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 짧은 문장을 채울(패딩, PADding) 때 사용할 제로 토큰</span>
<span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 문장의 시작(SOS, Start Of Sentence)을 나타내는 토큰</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 문장의 끝(EOS, End Of Sentence)을 나태는 토큰</span>

<span class="k">class</span> <span class="nc">Voc</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">&quot;PAD&quot;</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">&quot;SOS&quot;</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">&quot;EOS&quot;</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># SOS, EOS, PAD를 센 것</span>

    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># 등장 횟수가 기준 이하인 단어를 정리합니다</span>
    <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">keep_words</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="n">keep_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;keep_words </span><span class="si">{}</span><span class="s1"> / </span><span class="si">{}</span><span class="s1"> = </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>
        <span class="p">))</span>

        <span class="c1"># 사전을 다시 초기화힙니다</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">&quot;PAD&quot;</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">&quot;SOS&quot;</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">&quot;EOS&quot;</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># 기본 토큰을 센 것</span>

        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keep_words</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 어휘집과 질의/응답 문장 쌍을 재구성하려 합니다. 그러한 데이터를
사용하려면 그 전에 약간의 전처리 작업을 수행해야 합니다.</p>
<p>우선, <code class="docutils literal notranslate"><span class="pre">unicodeToAscii</span></code> 를 이용하여 유니코드 문자열을 아스키로 변환해야
합니다. 다음에는 모든 글자를 소문자로 변환하고, 알파벳도 아니고 기본적인
문장 부호도 아닌 글자는 제거합니다(정규화, <code class="docutils literal notranslate"><span class="pre">normalizeString</span></code>).
마지막으로는 학습할 때의 편의성을 위해서, 길이가 일정 기준을 초과하는,
즉 <code class="docutils literal notranslate"><span class="pre">MAX_LENGTH</span></code> 보다 긴 문장을 제거합니다(<code class="docutils literal notranslate"><span class="pre">filterPairs</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 고려할 문장의 최대 길이</span>

<span class="c1"># 유니코드 문자열을 아스키로 변환합니다</span>
<span class="c1"># https://stackoverflow.com/a/518232/2809427 참고</span>
<span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">&#39;Mn&#39;</span>
    <span class="p">)</span>

<span class="c1"># 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다</span>
<span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([.!?])&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; \1&quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[^a-zA-Z.!?]+&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="c1"># 질의/응답 쌍을 읽어서 voc 객체를 반환합니다</span>
<span class="k">def</span> <span class="nf">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reading lines...&quot;</span><span class="p">)</span>
    <span class="c1"># 파일을 읽고, 쪼개어 lines에 저장합니다</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># 각 줄을 쪼개어 pairs에 저장하고 정규화합니다</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="n">voc</span> <span class="o">=</span> <span class="n">Voc</span><span class="p">(</span><span class="n">corpus_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>

<span class="c1"># 문장의 쌍 &#39;p&#39;에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다</span>
<span class="k">def</span> <span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c1"># EOS 토큰을 위해 입력 시퀀스의 마지막 단어를 보존해야 합니다</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span>

<span class="c1"># 조건식 filterPair에 따라 pairs를 필터링합니다</span>
<span class="k">def</span> <span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>

<span class="c1"># 앞에서 정의한 함수를 이용하여 만든 voc 객체와 리스트 pairs를 반환합니다</span>
<span class="k">def</span> <span class="nf">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start preparing training data ...&quot;</span><span class="p">)</span>
    <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Read </span><span class="si">{!s}</span><span class="s2"> sentence pairs&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trimmed to </span><span class="si">{!s}</span><span class="s2"> sentence pairs&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Counting words...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Counted words:&quot;</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>


<span class="c1"># voc와 pairs를 읽고 재구성합니다</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;save&quot;</span><span class="p">)</span>
<span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
<span class="c1"># 검증을 위해 pairs의 일부 내용을 출력해 봅니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">pairs:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Start preparing training data ...
Reading lines...
Read 221282 sentence pairs
Trimmed to 64271 sentence pairs
Counting words...
Counted words: 18008

pairs:
[&#39;there .&#39;, &#39;where ?&#39;]
[&#39;you have my word . as a gentleman&#39;, &#39;you re sweet .&#39;]
[&#39;hi .&#39;, &#39;looks like things worked out tonight huh ?&#39;]
[&#39;you know chastity ?&#39;, &#39;i believe we share an art instructor&#39;]
[&#39;have fun tonight ?&#39;, &#39;tons&#39;]
[&#39;well no . . .&#39;, &#39;then that s all you had to say .&#39;]
[&#39;then that s all you had to say .&#39;, &#39;but&#39;]
[&#39;but&#39;, &#39;you always been this selfish ?&#39;]
[&#39;do you listen to this crap ?&#39;, &#39;what crap ?&#39;]
[&#39;what good stuff ?&#39;, &#39;the real you .&#39;]
</pre></div>
</div>
<p>학습 단계가 빨리 수렴하도록 하는 또 다른 전략은 자주 쓰이지 않는 단어를
어휘집에서 제거하는 것입니다. 피처 공간의 크기를 줄이면 모델이
학습을 통해 근사하려는 함수의 난이도를 낮추는 효과도 있습니다. 우리는
이를 두 단계로 나눠 진행하려 합니다.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">voc.trim</span></code> 함수를 이용하여 <code class="docutils literal notranslate"><span class="pre">MIN_COUNT</span></code> 라는 기준 이하의 단어를
제거합니다.</p></li>
<li><p>제거하기로 한 단어를 포함하는 경우를 pairs에서 제외합니다</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MIN_COUNT</span> <span class="o">=</span> <span class="mi">3</span>    <span class="c1"># 제외할 단어의 기준이 되는 등장 횟수</span>

<span class="k">def</span> <span class="nf">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">):</span>
    <span class="c1"># MIN_COUNT 미만으로 사용된 단어는 voc에서 제외합니다</span>
    <span class="n">voc</span><span class="o">.</span><span class="n">trim</span><span class="p">(</span><span class="n">MIN_COUNT</span><span class="p">)</span>
    <span class="c1"># 제외할 단어가 포함된 경우를 pairs에서도 제외합니다</span>
    <span class="n">keep_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">keep_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">keep_output</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># 입력 문장을 검사합니다</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_input</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>
        <span class="c1"># 출력 문장을 검사합니다</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_output</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">break</span>

        <span class="c1"># 입출력 문장에 제외하기로 한 단어를 포함하지 않는 경우만을 남겨둡니다</span>
        <span class="k">if</span> <span class="n">keep_input</span> <span class="ow">and</span> <span class="n">keep_output</span><span class="p">:</span>
            <span class="n">keep_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trimmed from </span><span class="si">{}</span><span class="s2"> pairs to </span><span class="si">{}</span><span class="s2">, </span><span class="si">{:.4f}</span><span class="s2"> of total&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">keep_pairs</span>


<span class="c1"># voc와 pairs를 정돈합니다</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>keep_words 7823 / 18005 = 0.4345
Trimmed from 64271 pairs to 53165, 0.8272 of total
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h2>모델을 위한 데이터 준비하기<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>상당한 노력을 기울여 데이터를 전처리하고, 잘 정리하여 어휘집 객체와
문장 쌍의 리스트 형태로 만들어두긴 했지만, 결국 우리가 만들 모델에서
사용하는 입력은 수치 값으로 이루어진 torch 텐서입니다. 처리한 데이터를
모델에 맞는 형태로 준비하는 방법의 하나가 <a class="reference external" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">seq2seq 변환 튜토리얼</a>
에 나와 있습니다. 이 튜토리얼에서는 배치 크기로 1을 사용하며, 이는 즉
문장에 등장하는 단어를 어휘집에서의 인덱스로 변환하여 모델에 제공하기만
하면 된다는 의미입니다.</p>
<p>그래도 여러분이 학습 속도나 GPU 병렬 처리 용량을 향상하고 싶다면
미니배치를 이용하여 학습해야 할 것입니다.</p>
<p>미니배치를 사용한다는 것은 배치에 포함된 문장 길이가 달라질 수 있다는
점에 유의해야 한다는 것을 뜻합니다. 같은 배치 안에서 크기가 다른
문장을 처리하기 위해서는 배치용 입력 텐서의 모양을 <em>(max_length,
batch_size)</em> 로 맞춰야 합니다. 이때 <em>max_length</em> 보다 짧은 문장에
대해서는 <em>EOS 토큰</em> 뒤에 제로 토큰을 덧붙이면 됩니다.</p>
<p>영어로 된 문장을 텐서로 변환하기 위해 단순히 그에 대응하는 인덱스를
사용하고(<code class="docutils literal notranslate"><span class="pre">indexesFromSentence</span></code>) 제로 토큰을 패딩한다고 해봅시다.
그러면 텐서의 모양이 <em>(batch_size, max_length)</em> 이 되고, 첫 번째 차원에
대해 인덱싱을 수행하면 모든 시간대별 문장이 전부 반환될 것입니다.
그러나 우리는 배치를 시간에 따라, 그리고 배치에 포함된 모든 문장에
대해 인덱싱할 수도 있어야 합니다. 따라서 우리는 입력 배치의 모양을
뒤집어서 <em>(max_length, batch_size)</em> 형태로 만들 것입니다. 그러고 난
후에 첫 번째 차원에 대해 인덱싱하면 배치에 포함된 모든 문장을 시간에
대해 인덱싱한 결과를 반환하게 됩니다. 우리는 이 뒤집기 작업을
<code class="docutils literal notranslate"><span class="pre">zeroPadding</span></code> 함수를 이용하여 묵시적으로 수행할 것입니다.</p>
<div class="figure align-center">
<img alt="batches" src="../_images/seq2seq_batches.png" />
</div>
<p><code class="docutils literal notranslate"><span class="pre">inputVar</span></code> 함수는 문장을 텐서로 변환하는, 그리고 궁극적으로는 제로
패딩하여 올바른 모양으로 맞춘 텐서를 만드는 작업을 수행합니다. 이
함수는 각 배치에 포함된 시퀀스의 길이(<code class="docutils literal notranslate"><span class="pre">lengths</span></code>)로 구성된 텐서도 같이
반환합니다. 그리고 우리는 이를 나중에 디코더로 넘겨줄 것입니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">outputVar</span></code> 함수는 <code class="docutils literal notranslate"><span class="pre">inputVar</span></code> 와 비슷한 작업을 수행하지만, <code class="docutils literal notranslate"><span class="pre">lengths</span></code>
텐서를 반환하는 대신에 이진 마스크로 구성된 텐서와 목표 문장의 최대
길이를 같이 반환합니다. 이진 마스크 텐서는 출력에 해당하는 목표 텐서와
그 모양이 같지만, 패딩 토큰(<em>PAD_token</em>)에 해당하는 경우에는 값이 0이며
나머지 경우의 값은 1입니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">batch2TrainData</span></code> 는 단순히 여러 쌍을 입력으로 받아서, 앞서 설명한
함수를 이용하여 입력 및 목표 텐서를 구하여 반환합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_token</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">zeroPadding</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">fillvalue</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">binaryMatrix</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
        <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">PAD_token</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1"># 입력 시퀀스 텐서에 패딩한 결과와 lengths를 반환합니다</span>
<span class="k">def</span> <span class="nf">inputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">lengths</span>

<span class="c1"># 패딩한 목표 시퀀스 텐서, 패딩 마스크, 그리고 최대 목표 길이를 반환합니다</span>
<span class="k">def</span> <span class="nf">outputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">max_target_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">binaryMatrix</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>

<span class="c1"># 입력 배치를 이루는 쌍에 대한 모든 아이템을 반환합니다</span>
<span class="k">def</span> <span class="nf">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pair_batch</span><span class="p">):</span>
    <span class="n">pair_batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pair_batch</span><span class="p">:</span>
        <span class="n">input_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">inputVar</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">outputVar</span><span class="p">(</span><span class="n">output_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>


<span class="c1"># 검증용 예시</span>
<span class="n">small_batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">small_batch_size</span><span class="p">)])</span>
<span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">batches</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input_variable:&quot;</span><span class="p">,</span> <span class="n">input_variable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lengths:&quot;</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;target_variable:&quot;</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mask:&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max_target_len:&quot;</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>input_variable: tensor([[ 124,  401,   33, 7304, 1574],
        [   6,  177,    6,  215,  631],
        [7773,    4,   50,   12,  174],
        [1200,   76,  148,  735,    4],
        [  51,  132,    7,   56,    2],
        [ 280,  380,   74,   19,    0],
        [  56,   31,    6,    4,    0],
        [   7,    4,    2,    2,    0],
        [   6,    2,    0,    0,    0],
        [   2,    0,    0,    0,    0]])
lengths: tensor([10,  9,  8,  8,  5])
target_variable: tensor([[7773,   34,   25,  101,  124],
        [  37,    4,  112,   37,  125],
        [  12,    4,  113,  159,    4],
        [4801,    4,   40, 7199,    2],
        [   4,  571,  359,    4,    0],
        [   2,    4,  147,    2,    0],
        [   0,    2,    7,    0,    0],
        [   0,    0,  106,    0,    0],
        [   0,    0,    4,    0,    0],
        [   0,    0,    2,    0,    0]])
mask: tensor([[1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 1, 0],
        [1, 1, 1, 1, 0],
        [0, 1, 1, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 1, 0, 0]], dtype=torch.uint8)
max_target_len: 10
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h2>모델 정의하기<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<div class="section" id="seq2seq">
<h3>Seq2Seq 모델<a class="headerlink" href="#seq2seq" title="Permalink to this headline">¶</a></h3>
<p>우리 챗봇의 두뇌에 해당하는 모델은 sequence-to-sequence (seq2seq)
모델입니다. seq2seq 모델의 목표는 가변 길이 시퀀스를 입력으로 받고,
크기가 고정된 모델을 이용하여, 가변 길이 시퀀스를 출력으로 반환하는
것입니다.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1409.3215">Sutskever 등</a> 은 두 개의 독립된
순환 신경망을 같이 이용하여 이러한 목적을 달성할 수 있음을 발견했습니다.
RNN 하나는 <strong>인코더</strong> 로, 가변 길이 입력 시퀀스를 고정된 길이의 문맥
벡터(context vector)로 인코딩합니다. 이론상 문맥 벡터(RNN의 마지막
은닉 레이어)는 봇에게 입력으로 주어지는 질의 문장에 대한 의미론적 정보를
담고 있을 것입니다. 두 번째 RNN은 <strong>디코더</strong> 입니다. 디코더는 단어 하나와
문맥 벡터를 입력으로 받고, 시퀀스의 다음 단어가 무엇일지를 추론하여
반환하며, 다음 단계에서 사용할 은닉 상태도 같이 반환합니다.</p>
<div class="figure align-center">
<img alt="model" src="../_images/seq2seq_ts.png" />
</div>
<p>그림 출처:
<a class="reference external" href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p>
</div>
<div class="section" id="id9">
<h3>인코더<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>인코더 RNN은 입력 시퀀스를 토큰 단위로(예를 들어, 단어 단위로) 한번에
하나씩 살펴보며 진행합니다. 그리고 각 단계마다 “출력” 벡터와 “은닉
상태” 벡터를 반환합니다. 은닉 상태 벡터는 다음 단계를 진행할 때 같이
사용되며, 출력 벡터는 차례대로 기록됩니다. 인코더는 시퀀스의 각 지점에
대해 파악한 문맥을 고차원 공간에 있는 점들의 집합으로 변환합니다.
나중에 디코더는 이를 이용하여 주어진 문제에 대해 의미 있는 출력을
구할 것입니다.</p>
<p>인코더의 핵심 부분에는 다중 레이어 게이트 순환 유닛(multi-layered Gated
Recurrent Unit)이 있습니다. 이는 <a class="reference external" href="https://arxiv.org/pdf/1406.1078v3.pdf">Cho 등</a>
이 2014년에 고안한 것입니다. 우리는 GRU를 양방향으로 변환한 형태를
사용할 것이며, 이는 본질적으로 두 개의 독립된 RNN이 존재한다는
의미입니다. 하나는 입력 시퀀스를 원래 시퀀스에서의 순서로 처리하며,
다른 하나는 입력 시퀀스를 역순으로 처리합니다. 단계마다 각 네트워크의
출력을 합산합니다. 양방향 GRU를 사용하면 과거와 미래의 문맥을 함께
인코딩할 수 있다는 장점이 있습니다.</p>
<p>양방향 RNN:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/RNN-bidirectional.png"><img alt="rnn_bidir" src="../_images/RNN-bidirectional.png" style="width: 70%;" /></a>
</div>
<p>그림 출처: <a class="reference external" href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a></p>
<p><code class="docutils literal notranslate"><span class="pre">embedding</span></code> 레이어가 단어 인덱스를 임의 크기의 피처 공간으로
인코딩하는 데 사용되었음에 유의하기 바랍니다. 우리의 모델에서는 이
레이어가 각 단어를 크기가 <em>hidden_size</em> 인 피처 공간으로 매핑할
것입니다. 학습을 거치면 서로 뜻이 유사한 단어는 의미적으로 유사하게
인코딩될 것입니다.</p>
<p>마지막으로, RNN 모듈에 패딩된 배치를 보내려면 RNN과 연결된 부분에서
패킹 및 언패킹하는 작업을 수행해야 합니다. 각각은
<code class="docutils literal notranslate"><span class="pre">nn.utils.rnn.pack_padded_sequence</span></code> 와
<code class="docutils literal notranslate"><span class="pre">nn.utils.rnn.pad_packed_sequence</span></code> 를 통해 수행할 수 있습니다.</p>
<p><strong>계산 그래프:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>단어 인덱스를 임베딩으로 변환합니다.</p></li>
<li><p>RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다.</p></li>
<li><p>GRU로 포워드 패스를 수행합니다.</p></li>
<li><p>패딩을 언패킹합니다.</p></li>
<li><p>양방향 GRU의 출력을 합산합니다.</p></li>
<li><p>출력과 마지막 은닉 상태를 반환합니다.</p></li>
</ol>
</div></blockquote>
<p><strong>입력:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_seq</span></code>: 입력 시퀀스 배치. shape=<em>(max_length,
batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_lengths</span></code>: 배치에 포함된 각 문장의 길이로 구성된 리스트.
shape=<em>(batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: 은닉 상태. shape=<em>(n_layers x num_directions,
batch_size, hidden_size)</em></p></li>
</ul>
<p><strong>출력:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">outputs</span></code>: GRU의 마지막 은닉 레이어에 대한 출력 피처 값(양방향
(출력을 합산한 것). shape=<em>(max_length, batch_size, hidden_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: GRU의 최종 은닉 상태. shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>

        <span class="c1"># GRU를 초기화합니다. input_size와 hidden_size 패러미터는 둘 다 &#39;hidden_size&#39;로</span>
        <span class="c1"># 둡니다. 이는 우리 입력의 크기가 hideen_size 만큼의 피처를 갖는 단어 임베딩이기</span>
        <span class="c1"># 때문입니다.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                          <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 단어 인덱스를 임베딩으로 변환합니다</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="c1"># RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다</span>
        <span class="n">packed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">)</span>
        <span class="c1"># GRU로 포워드 패스를 수행합니다</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># 패딩을 언패킹합니다</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># 양방향 GRU의 출력을 합산합니다</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:]</span>
        <span class="c1"># 출력과 마지막 은닉 상태를 반환합니다</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
<div class="section" id="id10">
<h3>디코더<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>디코더 RNN은 토큰 단위로 응답 문장을 생성하는 역할을 수행합니다. 이때
인코더의 문백 벡터를 사용하며, 내부 은닉 상태에 따라 시퀀스의 다음
단어를 생성하게 됩니다. 디코더는 <em>EOS_token</em>, 즉 문장의 끝을 나타내는
토큰을 출력할 때까지 계속 단어를 생성합니다. 원래의 seq2seq 디코더에는
알려진 문제점이 있습니다. 만약 우리가 입력 시퀀스의 의미를 인코딩할
때 문맥 벡터에만 전적으로 의존한다면, 그 과정 중에 정보 손실이 일어날
가능성이 높다는 것입니다. 이는 특히 입력 시퀀스의 길이가 길 때 그러하며,
이 때문에 디코더의 기능이 크게 제한될 수 있습니다.</p>
<p>이를 해결하기 위한 방편으로, <a class="reference external" href="https://arxiv.org/abs/1409.0473">Bahdanau 등</a> 은 ‘어텐션 메커니즘’을
고안했습니다. 이는 디코더가 매 단계에 대해 고정된 문맥을 계속 사용하는
것이 아니라, 입력 시퀀스의 특정 부분에 집중하게 하는 방식입니다.</p>
<p>높은 차원에서 이야기 하자면, 어텐션은 디코더의 현재 은닉 상태와 인코더의
출력을 바탕으로 계산됩니다. 출력되는 어텐션 가중치는 입력 시퀀스와
동일한 모양을 가집니다. 따라서 이를 인코더의 출력과 곱할 수 있고, 그
결과로 얻게 되는 가중치 합은 인코더의 출력에서 어느 부분에 집중해야
할지를 알려줍니다. <a class="reference external" href="https://github.com/spro">Sean Robertson</a>
의 그림에 이러한 내용이 잘 설명되어 있습니다.</p>
<div class="figure align-center">
<img alt="attn2" src="../_images/attn2.png" />
</div>
<p><a class="reference external" href="https://arxiv.org/abs/1508.04025">Luong 등</a> 은 Bahdanau의 기초 연구를
더욱 발전시킨 ‘전역(global) 어텐션’을 제안했습니다. ‘전역 어텐션’의
핵심적인 차이점은 인코더의 은닉 상태를 모두 고려한다는 점입니다. 이는
Bahdanau 등의 ‘지역(local) 어텐션’ 방식이 현재 시점에 대한 인코더의
은닉 상태만을 고려한다는 점과 다른 부분입니다. ‘전역 어텐션’의 또 다른
차이점은 어텐션에 대한 가중치, 혹은 에너지를 계산할 때 현재 시점에 대한
디코더의 은닉 상태만을 사용한다는 점입니다. Bahdanau 등은 어텐션을
계산할 때 디코더의 이전 단계 상태에 대한 정보를 활용합니다. 또한 Luong 등의
방법에서는 인코더의 출력과 디코더의 출력에 대한 어텐션 에너지를 계산하는
방법을 제공하며, 이를 ‘점수 함수(score function)’라 부릅니다.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/scores.png"><img alt="scores" src="../_images/scores.png" style="width: 60%;" /></a>
</div>
<p>이때 <span class="math notranslate nohighlight">\(h_t\)</span> 는 목표 디코더의 현재 상태를, <span class="math notranslate nohighlight">\(\bar{h}_s\)</span> 는 인코더의
모든 상태를 뜻합니다.</p>
<p>종합해 보면, 전역 어텐션 메커니즘을 다음 그림과 같이 요약할 수 있을
것입니다. 우리가 ‘어텐션 레이어’를 <code class="docutils literal notranslate"><span class="pre">Attn</span></code> 라는 독립적인 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 로
구현할 것임에 유의하기 바랍니다. 이 모듈의 출력은 모양이 <em>(batch_size, 1,
max_length)</em> 인 정규화된 softmax 가중치 텐서입니다.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/global_attn.png"><img alt="global_attn" src="../_images/global_attn.png" style="width: 60%;" /></a>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Luong 어텐션 레이어</span>
<span class="k">class</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;dot&#39;</span><span class="p">,</span> <span class="s1">&#39;general&#39;</span><span class="p">,</span> <span class="s1">&#39;concat&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;is not an appropriate attention method.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">dot_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">general_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">concat_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Attention 가중치(에너지)를 제안된 방법에 따라 계산합니다</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">general_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;dot&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="c1"># max_length와 batch_size의 차원을 뒤집습니다</span>
        <span class="n">attn_energies</span> <span class="o">=</span> <span class="n">attn_energies</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="c1"># 정규화된 softmax 확률 점수를 반환합니다 (차원을 늘려서)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_energies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>이처럼 어텐션 서브모듈을 정의하고 나면 실제 디코더 모델을 구현할 수
있게 됩니다. 디코더에 대해서는 매 시간마다 배치를 하나씩 수동으로
제공하려 합니다. 이는 임베딩된 단어 텐서와 GRU 출력의 모양이 둘 다
<em>(1, batch_size, hidden_size)</em> 라는 의미입니다.</p>
<p><strong>계산 그래프:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>현재의 입력 단어에 대한 임베딩을 구합니다.</p></li>
<li><p>무방향 GRU로 포워드 패스를 수행합니다.</p></li>
<li><p>(2)에서 구한 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다.</p></li>
<li><p>인코더 출력에 어텐션을 곱하여 새로운 “가중치 합” 문백 벡터를 구합니다.</p></li>
<li><p>Luong의 논문에 나온 식 5를 이용하여 가중치 문백 벡터와 GRU 출력을 결합합니다.</p></li>
<li><p>Luong의 논문에 나온 식 6을 이용하여(softmax 없이) 다음 단어를 예측합니다.</p></li>
<li><p>출력과 마지막 은닉 상태를 반환합니다.</p></li>
</ol>
</div></blockquote>
<p><strong>입력:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_step</span></code>: 입력 시퀀스 배치에 대한 한 단위 시간(한 단어).
shape=<em>(1, batch_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">last_hidden</span></code>: GRU의 마지막 은닉 레이어. shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">encoder_outputs</span></code>: 인코더 모델의 출력. shape=<em>(max_length,
batch_size, hidden_size)</em></p></li>
</ul>
<p><strong>출력:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">output</span></code>: 각 단어가 디코딩된 시퀀스에서 다음 단어로 사용되었을
때 적합할 확률을 나타내는 정규화된 softmax 텐서.
shape=<em>(batch_size, voc.num_words)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden</span></code>: GRU의 마지막 은닉 상태. shape=<em>(n_layers x
num_directions, batch_size, hidden_size)</em></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LuongAttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 참조를 보존해 둡니다</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># 레이어를 정의합니다</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attn</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_step</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다</span>
        <span class="c1"># 현재의 입력 단어에 대한 임베딩을 구합니다</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_step</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="c1"># 무방향 GRU로 포워드 패스를 수행합니다</span>
        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">)</span>
        <span class="c1"># 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="c1"># 인코더 출력에 어텐션을 곱하여 새로운 &quot;가중치 합&quot; 문백 벡터를 구합니다</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Luong의 논문에 나온 식 5를 이용하여 가중치 문백 벡터와 GRU 출력을 결합합니다</span>
        <span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_input</span><span class="p">))</span>
        <span class="c1"># Luong의 논문에 나온 식 6을 이용하여 다음 단어를 예측합니다</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 출력과 마지막 은닉 상태를 반환합니다</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id11">
<h2>학습 프로시저 정의하기<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<div class="section" id="masked-loss">
<h3>Masked loss<a class="headerlink" href="#masked-loss" title="Permalink to this headline">¶</a></h3>
<p>우리는 패딩된 시퀀스 배치를 다루기 때문에 손실을 계산할 때 단순히 텐서의
모든 원소를 고려할 수는 없습니다. 우리는 <code class="docutils literal notranslate"><span class="pre">maskNLLLoss</span></code> 를 정의하여
디코더의 출력 텐서, 목표 텐서, 이진 마스크 텐서를 바탕으로 손실을 계산하려
합니다. 이 손실 함수에서는 마스크 텐서의 <em>1</em> 에 대응하는 원소에 대한 음의
로그 우도 값의 평균을 계산합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">maskNLLLoss</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">nTotal</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">crossEntropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">crossEntropy</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">nTotal</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id12">
<h3>한 번의 학습 단계<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code> 함수에 학습을 한 단계(입력 배치 한 개에 대한) 진행하는 알고리즘이
나와 있습니다.</p>
<p>우리는 수렴이 잘 되도록 몇 가지 영리한 전략을 사용해보려 합니다.</p>
<ul class="simple">
<li><p>첫 번째 전략은 <strong>teacher forcing</strong> 을 사용하는 것입니다. 이는
<code class="docutils literal notranslate"><span class="pre">teacher_forcing_ratio</span></code> 로 정의된 확률에 따라, 디코더의 이번 단계
예측값 대신에 현재의 목표 단어를 디코더의 다음 입력 값으로 활용하는
것입니다. 이 기법은 디코더의 보조 바퀴처럼 작용하여 효율적으로 학습될 수
있게 도와 줍니다. 하지만 teacher forcing 기법은 추론 과정에서 모델이
불안정 해지도록 할 수도 있는데, 이는 디코더가 학습 과정에서 자신의 출력
시퀀스를 직접 만들어 볼 기회를 충분히 제공받지 못할 수 있기 때문입니다.
따라서 우리는 <code class="docutils literal notranslate"><span class="pre">teacher_forcing_ratio</span></code> 를 어떻게 설정해 두었는지에
주의를 기울여야 하며, 수렴이 빨리 되었다고 속아 넘어가서는 안 됩니다.</p></li>
<li><p>우리가 구현한 두 번째 전략은 <strong>gradient clipping</strong> 입니다. 이는 소위
‘그라디언트 폭발’ 문제를 해결하기 위해 널리 사용되는 기법입니다. 핵심은
그라디언트를 클리핑 하거나 임계값을 둠으로써, 그라디언트가 지수
함수적으로 증가하거나 오버플로를 일으키는(NaN) 경우를 막고, 비용 함수의
급격한 경사를 피하겠다는 것입니다.</p></li>
</ul>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/grad_clip.png"><img alt="grad_clip" src="../_images/grad_clip.png" style="width: 60%;" /></a>
</div>
<p>그림 출처: Goodfellow 등 저. <em>Deep Learning</em>. 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p><strong>작업 절차:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>전체 입력 배치에 대하여 인코더로 포워드 패스를 수행합니다.</p></li>
<li><p>디코더의 입력을 SOS_token로, 은닉 상태를 인코더의 마지막 은닉 상태로 초기화합니다.</p></li>
<li><p>입력 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다.</p></li>
<li><p>Teacher forcing을 사용하는 경우, 디코더의 다음 입력을 현재의 목표로 둡니다. 그렇지 않으면 디코더의 다음 입력을 현재 디코더의 출력으로 둡니다.</p></li>
<li><p>손실을 계산하고 누적합니다.</p></li>
<li><p>역전파를 수행합니다.</p></li>
<li><p>그라디언트를 클리핑 합니다.</p></li>
<li><p>인코더 및 디코더 모델의 패러미터를 갱신합니다.</p></li>
</ol>
</div></blockquote>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>PyTorch의 RNN 모듈(<code class="docutils literal notranslate"><span class="pre">RNN</span></code>, <code class="docutils literal notranslate"><span class="pre">LSTM</span></code>, <code class="docutils literal notranslate"><span class="pre">GRU</span></code>)은 전체 입력 시퀀스(또는
시퀀스의 배치)를 단순히 넣어주기만 하면 다른 비순환 레이어처럼 사용할 수
있습니다. 우리는 <code class="docutils literal notranslate"><span class="pre">encoder</span></code> 에서 <code class="docutils literal notranslate"><span class="pre">GRU</span></code> 레이어를 이런 식으로 사용합니다.
그 안이 실제로 어떻게 되어 있는지를 살펴보면, 매 시간 단계마다 은닉 상태를
계산하는 반복 프로세스가 존재합니다. 또 다른 방법은, 이 모듈을 매번 한 단위
시간만큼 수행할 수도 있습니다. 그 경우에는 우리가 <code class="docutils literal notranslate"><span class="pre">decoder</span></code> 모델을 다룰
때처럼, 학습 과정에서 수동으로 시퀀스에 대해 반복 작업을 수행해 주어야
합니다. 이 모듈에 대해 모델의 개념을 확실히 갖고만 있다면, 순차 모델을
구현하는 것도 매우 단순할 것입니다.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span>
          <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>

    <span class="c1"># 제로 그라디언트</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># device 옵션을 설정합니다</span>
    <span class="n">input_variable</span> <span class="o">=</span> <span class="n">input_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_variable</span> <span class="o">=</span> <span class="n">target_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 변수를 초기화합니다</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">print_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_totals</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># 인코더로 포워드 패스를 수행합니다</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="c1"># 초기 디코더 입력을 생성합니다(각 문장을 SOS 도큰으로 시작합니다)</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">SOS_token</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로 둡니다</span>
    <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>

    <span class="c1"># 이번 반복에서 teacher forcing을 사용할지를 결정합니다</span>
    <span class="n">use_teacher_forcing</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">teacher_forcing_ratio</span> <span class="k">else</span> <span class="kc">False</span>

    <span class="c1"># 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다</span>
    <span class="k">if</span> <span class="n">use_teacher_forcing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># 손실을 계산하고 누적합니다</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">topi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># 손실을 계산하고 누적합니다</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>

    <span class="c1"># 역전파를 수행합니다</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 그라디언트 클리핑: 그라디언트를 제자리에서 수정합니다</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>

    <span class="c1"># 모델의 가중치를 수정합니다</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">print_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_totals</span>
</pre></div>
</div>
</div>
<div class="section" id="id13">
<h3>학습 단계<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>이제 마지막으로 전체 학습 프로시저와 데이터를 하나로 엮을 때가
되었습니다. <code class="docutils literal notranslate"><span class="pre">trainIters</span></code> 함수는 주어진 모델, optimizer, 데이터 등을
토대로 학습을 <code class="docutils literal notranslate"><span class="pre">n_iterations</span></code> 번의 단계만큼 진행하는 역할을 담당합니다.
이 함수는 자기 자신을 살 설명하고 있는 편인데, 무거운 작업을 <code class="docutils literal notranslate"><span class="pre">train</span></code>
함수에 옮겨 놓았기 때문입니다.</p>
<p>한 가지 주의할 점은 우리가 모델을 저장하려 할 때, 인코더와 디코더의
state_dicts (패러미터), optimizer의 state_dicts, 손실, 진행 단계 수
등을 tarball로 만들어 저장한다는 점입니다. 모델을 이러한 방식으로
저장하면 checkpoint에 대해 아주 높은 수준의 유연성을 확보할 수 있게
됩니다. Checkpoint를 불러오고 나면, 우리는 모델 패러미터를 이용하여
예측을 진행할 수도 있고, 이전에 멈췄던 부분부터 학습을 계속  진행할
수도 있게 됩니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">):</span>

    <span class="c1"># 각 단계에 대한 배치를 읽어옵니다</span>
    <span class="n">training_batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span>
                      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">)]</span>

    <span class="c1"># 초기화</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Initializing ...&#39;</span><span class="p">)</span>
    <span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
        <span class="n">start_iteration</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># 학습 루프</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iteration</span><span class="p">,</span> <span class="n">n_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">training_batch</span> <span class="o">=</span> <span class="n">training_batches</span><span class="p">[</span><span class="n">iteration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># 배치에서 각 필드를 읽어옵니다</span>
        <span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">training_batch</span>

        <span class="c1"># 배치에 대해 학습을 한 단계 진행합니다</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span>
                     <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">)</span>
        <span class="n">print_loss</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="c1"># 경과를 출력합니다</span>
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration: </span><span class="si">{}</span><span class="s2">; Percent complete: </span><span class="si">{:.1f}</span><span class="s2">%; Average loss: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">/</span> <span class="n">n_iteration</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>
            <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Checkpoint를 저장합니다</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">-</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
                <span class="s1">&#39;iteration&#39;</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
                <span class="s1">&#39;en&#39;</span><span class="p">:</span> <span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;de&#39;</span><span class="p">:</span> <span class="n">decoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;en_opt&#39;</span><span class="p">:</span> <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;de_opt&#39;</span><span class="p">:</span> <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                <span class="s1">&#39;voc_dict&#39;</span><span class="p">:</span> <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
                <span class="s1">&#39;embedding&#39;</span><span class="p">:</span> <span class="n">embedding</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="p">},</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">.tar&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39;checkpoint&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id14">
<h2>평가 정의하기<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>모델을 학습시키고 나면 직접 봇과 대화를 나눠보고 싶어질 것입니다. 그러려면
먼저 모델이 인코딩된 입력을 어떻게 디코딩할지를 정의해줘야 합니다.</p>
<div class="section" id="id15">
<h3>탐욕적 디코딩<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>탐욕적 디코딩(Greedy decoding)은 우리가 학습 단계에서 teacher forcing을
적용하지 않았을 때 사용한 디코딩 방법입니다. 달리 말하면, 각 단계에 대해
단순히 <code class="docutils literal notranslate"><span class="pre">decoder_output</span></code> 에서 가장 높은 softmax값을 갖는 단어를 선택하는
방식입니다. 이 디코딩 방법은 한 번의 단계에 대해서는 최적입니다.</p>
<p>우리는 탐욕적 디코딩 연산을 수행할 수 있도록 <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code>
클래스를 만들었습니다. 수행 과정에서 이 클래스의 인스턴스는 모양이
<em>(input_seq length, 1)</em> 인 입력 시퀀스(<code class="docutils literal notranslate"><span class="pre">input_seq</span></code>), 조종할 입력
길이(<code class="docutils literal notranslate"><span class="pre">input_length</span></code>) 텐서, 그리고 응답 문장 길이의 제한을 나타내는
<code class="docutils literal notranslate"><span class="pre">max_length</span></code> 를 입력으로 받습니다. 입력 시퀀서는 다음과 같은 계산 그래프에
의해 평가됩니다.</p>
<p><strong>계산 그래프:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>인코더 모델로 입력을 포워드 패스합니다.</p></li>
<li><p>인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다.</p></li>
<li><p>디코더의 첫 번째 입력을 SOS_token으로 초기화합니다.</p></li>
<li><p>디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다.</p></li>
<li><dl class="simple">
<dt>반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다.</dt><dd><ol class="loweralpha simple">
<li><p>디코더로의 포워드 패스를 수행합니다.</p></li>
<li><p>가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다.</p></li>
<li><p>토큰과 점수를 기록합니다.</p></li>
<li><p>현재의 토큰을 디코더의 다음 입력으로 준비시킵니다.</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>단어 토큰과 점수를 모아서 반환합니다.</p></li>
</ol>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GreedySearchDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GreedySearchDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="c1"># 인코더 모델로 입력을 포워드 패스합니다</span>
        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
        <span class="c1"># 인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>
        <span class="c1"># 디코더의 첫 번째 입력을 SOS_token으로 초기화합니다</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="o">*</span> <span class="n">SOS_token</span>
        <span class="c1"># 디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># 반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="c1"># 디코더로의 포워드 패스를 수행합니다</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
            <span class="c1"># 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다</span>
            <span class="n">decoder_scores</span><span class="p">,</span> <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># 토큰과 점수를 기록합니다</span>
            <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_scores</span><span class="p">,</span> <span class="n">decoder_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 단어 토큰과 점수를 모아서 반환합니다</span>
        <span class="k">return</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_scores</span>
</pre></div>
</div>
</div>
<div class="section" id="id16">
<h3>내 텍스트 평가하기<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>이제 디코딩 모델을 정의했으니, 문자열로 된 입력 시퀀스를 평가하는 함수를
작성해볼 수 있을 것입니다. <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> 함수에 입력 시퀀스를 낮은
레벨에서 어떻게 처리할지가 나와 있습니다. 우리는 먼저 문장을
<em>batch_size==1</em> 이고 단어 인덱스로 구성된 입력 배치 형태로 만듭니다.
이를 위해 문장의 각 단어를 그에 대응하는 인덱스로 변환하고, 차원을
뒤집어서 모델에 맞는 입력 형태로 변환합니다. 우리는 입력 시퀀스의 길이를
저장하고 있는 <code class="docutils literal notranslate"><span class="pre">lengths</span></code> 텐서도 만듭니다. 이 경우에는 <code class="docutils literal notranslate"><span class="pre">lengths</span></code> 가
스칼라 값이 되는데, 우리는 한 번에 한 문장만 평가하기 때문입니다(batch_size==1).
다음으로는 <code class="docutils literal notranslate"><span class="pre">GreedySearchDecoder</span></code> 의 객체(<code class="docutils literal notranslate"><span class="pre">searcher</span></code>)를 이용하여
응답 문장 텐서를 디코딩합니다. 마지막으로, 응답 인덱스를 단어로 변환하고
디코딩된 단어의 리스트를 반환합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">evaluateInput</span></code> 은 우리의 챗봇에 대한 인터페이스 역할을 수행합니다.
이를 호출하면 입력 텍스트 필드가 생성되는데, 거기에 우리의 질의 문장을
입력해볼 수 있습니다. 입력 문장을 타이핑하고 <em>엔터</em> 를 누르면, 입력한
텍스트가 학습 데이터와 같은 방식으로 정규화되고, 최종적으로는 <code class="docutils literal notranslate"><span class="pre">evaluate</span></code>
함수에 입력으로 제공되어 디코딩된 출력 문장을 구하게 됩니다. 우리는
이러한 과정을 계속 반복하며, 이를 통해 ‘q’나 ‘quit’를 입력하기 전까지는
계속 채팅할 수 있습니다.</p>
<p>마지막으로, 만약 어휘집에 포함되어 있지 않은 단어를 포함하고 있는 문장이
입력되더라도 이를 예의 바르게 처리합니다. 즉 에러 메시지를 출력하고
사용자에게 새로운 문장을 입력해달라고 요구청합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="c1">### 입력 시퀀스를 배치 형태로 만듭니다</span>
    <span class="c1"># 단어 -&gt; 인덱스</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)]</span>
    <span class="c1"># lengths 텐서를 만듭니다</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="c1"># 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># 적절한 디바이스를 사용합니다</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># searcher를 이용하여 문장을 디코딩합니다</span>
    <span class="n">tokens</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># 인덱스 -&gt; 단어</span>
    <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decoded_words</span>


<span class="k">def</span> <span class="nf">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># 입력 문장을 받아옵니다</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;&gt; &#39;</span><span class="p">)</span>
            <span class="c1"># 종료 조건인지 검사합니다</span>
            <span class="k">if</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">&#39;q&#39;</span> <span class="ow">or</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">&#39;quit&#39;</span><span class="p">:</span> <span class="k">break</span>
            <span class="c1"># 문장을 정규화합니다</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># 문장을 평가합니다</span>
            <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># 응답 문장을 형식에 맞춰 출력합니다</span>
            <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;EOS&#39;</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">)]</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Bot:&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: Encountered unknown word.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id17">
<h2>모델 수행하기<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<p>마지막으로, 우리의 모델을 수행해 볼 시간입니다!</p>
<p>우리가 챗봇 모델을 학습할 때든 테스트할 때든, 우리는 각각의 인코더 및
디코더 모델을 초기화해줘야 합니다. 다음 블록에서는 우리가 원하는대로
설정을 맞추고, 처음부터 시작할지, 아니면 checkpoint를 불러올지 정하고,
모델을 빌드하고 초기화합니다. 성능을 최적화하기 위해서는 모델 설정을
여러가지로 바꿔 보면서 테스트해보기 바랍니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 모델을 설정합니다</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;cb_model&#39;</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s1">&#39;dot&#39;</span>
<span class="c1">#attn_model = &#39;general&#39;</span>
<span class="c1">#attn_model = &#39;concat&#39;</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">encoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">decoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># 불러올 checkpoint를 설정합니다. 처음부터 시작할 때는 None으로 둡니다.</span>
<span class="n">loadFilename</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">checkpoint_iter</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="c1">#loadFilename = os.path.join(save_dir, model_name, corpus_name,</span>
<span class="c1">#                            &#39;{}-{}_{}&#39;.format(encoder_n_layers, decoder_n_layers, hidden_size),</span>
<span class="c1">#                            &#39;{}_checkpoint.tar&#39;.format(checkpoint_iter))</span>


<span class="c1"># loadFilename이 제공되는 경우에는 모델을 불러옵니다</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="c1"># 모델을 학습할 때와 같은 기기에서 불러오는 경우</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loadFilename</span><span class="p">)</span>
    <span class="c1"># GPU에서 학습한 모델을 CPU로 불러오는 경우</span>
    <span class="c1">#checkpoint = torch.load(loadFilename, map_location=torch.device(&#39;cpu&#39;))</span>
    <span class="n">encoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">]</span>
    <span class="n">decoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;de&#39;</span><span class="p">]</span>
    <span class="n">encoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;en_opt&#39;</span><span class="p">]</span>
    <span class="n">decoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;de_opt&#39;</span><span class="p">]</span>
    <span class="n">embedding_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;embedding&#39;</span><span class="p">]</span>
    <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;voc_dict&#39;</span><span class="p">]</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Building encoder and decoder ...&#39;</span><span class="p">)</span>
<span class="c1"># 단어 임베딩을 초기화합니다</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">embedding</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">embedding_sd</span><span class="p">)</span>
<span class="c1"># 인코더 및 디코더 모델을 초기화합니다</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_sd</span><span class="p">)</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_sd</span><span class="p">)</span>
<span class="c1"># 적절한 디바이스를 사용합니다</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Models built and ready to go!&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Building encoder and decoder ...
Models built and ready to go!
</pre></div>
</div>
<div class="section" id="id18">
<h3>학습 수행하기<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>모델을 학습해보고 싶다면 다음 블록을 수행하면 됩니다.</p>
<p>먼저 학습 패러미터를 설정하고, optimizer를 초기화한 뒤, 마지막으로 <code class="docutils literal notranslate"><span class="pre">trainIters</span></code>
함수를 호출하여 학습 단계를 진행합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 학습 및 최적화 설정</span>
<span class="n">clip</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">teacher_forcing_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">decoder_learning_ratio</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Dropout 레이어를 학습 모드로 둡니다</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Optimizer를 초기화합니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Building optimizers ...&#39;</span><span class="p">)</span>
<span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">decoder_learning_ratio</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_optimizer_sd</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_optimizer_sd</span><span class="p">)</span>

<span class="c1"># cuda가 있다면 cuda를 설정합니다</span>
<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># 학습 단계를 수행합니다</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting Training!&quot;</span><span class="p">)</span>
<span class="n">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span>
           <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
           <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Building optimizers ...
Starting Training!
Initializing ...
Training...
Iteration: 1; Percent complete: 0.0%; Average loss: 8.9708
Iteration: 2; Percent complete: 0.1%; Average loss: 8.8403
Iteration: 3; Percent complete: 0.1%; Average loss: 8.6946
Iteration: 4; Percent complete: 0.1%; Average loss: 8.3880
Iteration: 5; Percent complete: 0.1%; Average loss: 8.0884
Iteration: 6; Percent complete: 0.1%; Average loss: 7.4863
Iteration: 7; Percent complete: 0.2%; Average loss: 6.9958
Iteration: 8; Percent complete: 0.2%; Average loss: 6.7815
Iteration: 9; Percent complete: 0.2%; Average loss: 6.8976
Iteration: 10; Percent complete: 0.2%; Average loss: 6.8477
Iteration: 11; Percent complete: 0.3%; Average loss: 6.4180
Iteration: 12; Percent complete: 0.3%; Average loss: 5.8966
Iteration: 13; Percent complete: 0.3%; Average loss: 5.5685
Iteration: 14; Percent complete: 0.4%; Average loss: 5.4767
Iteration: 15; Percent complete: 0.4%; Average loss: 5.7457
Iteration: 16; Percent complete: 0.4%; Average loss: 5.4045
Iteration: 17; Percent complete: 0.4%; Average loss: 5.3223
Iteration: 18; Percent complete: 0.4%; Average loss: 5.1455
Iteration: 19; Percent complete: 0.5%; Average loss: 5.2017
Iteration: 20; Percent complete: 0.5%; Average loss: 4.8577
Iteration: 21; Percent complete: 0.5%; Average loss: 4.9279
Iteration: 22; Percent complete: 0.5%; Average loss: 4.9788
Iteration: 23; Percent complete: 0.6%; Average loss: 4.8758
Iteration: 24; Percent complete: 0.6%; Average loss: 4.8669
Iteration: 25; Percent complete: 0.6%; Average loss: 4.6804
Iteration: 26; Percent complete: 0.7%; Average loss: 4.6636
Iteration: 27; Percent complete: 0.7%; Average loss: 4.9362
Iteration: 28; Percent complete: 0.7%; Average loss: 4.8784
Iteration: 29; Percent complete: 0.7%; Average loss: 4.7558
Iteration: 30; Percent complete: 0.8%; Average loss: 4.6668
Iteration: 31; Percent complete: 0.8%; Average loss: 4.8234
Iteration: 32; Percent complete: 0.8%; Average loss: 5.2034
Iteration: 33; Percent complete: 0.8%; Average loss: 4.8756
Iteration: 34; Percent complete: 0.9%; Average loss: 4.9047
Iteration: 35; Percent complete: 0.9%; Average loss: 4.7229
Iteration: 36; Percent complete: 0.9%; Average loss: 4.6396
Iteration: 37; Percent complete: 0.9%; Average loss: 4.5257
Iteration: 38; Percent complete: 0.9%; Average loss: 4.7224
Iteration: 39; Percent complete: 1.0%; Average loss: 4.6872
Iteration: 40; Percent complete: 1.0%; Average loss: 4.8360
Iteration: 41; Percent complete: 1.0%; Average loss: 4.7123
Iteration: 42; Percent complete: 1.1%; Average loss: 4.9312
Iteration: 43; Percent complete: 1.1%; Average loss: 4.6581
Iteration: 44; Percent complete: 1.1%; Average loss: 4.6335
Iteration: 45; Percent complete: 1.1%; Average loss: 4.6524
Iteration: 46; Percent complete: 1.1%; Average loss: 4.6073
Iteration: 47; Percent complete: 1.2%; Average loss: 4.5233
Iteration: 48; Percent complete: 1.2%; Average loss: 4.6350
Iteration: 49; Percent complete: 1.2%; Average loss: 4.4990
Iteration: 50; Percent complete: 1.2%; Average loss: 4.5377
Iteration: 51; Percent complete: 1.3%; Average loss: 4.6468
Iteration: 52; Percent complete: 1.3%; Average loss: 4.7080
Iteration: 53; Percent complete: 1.3%; Average loss: 4.5277
Iteration: 54; Percent complete: 1.4%; Average loss: 4.7776
Iteration: 55; Percent complete: 1.4%; Average loss: 4.5371
Iteration: 56; Percent complete: 1.4%; Average loss: 4.3414
Iteration: 57; Percent complete: 1.4%; Average loss: 4.6025
Iteration: 58; Percent complete: 1.5%; Average loss: 4.7977
Iteration: 59; Percent complete: 1.5%; Average loss: 4.5146
Iteration: 60; Percent complete: 1.5%; Average loss: 4.6386
Iteration: 61; Percent complete: 1.5%; Average loss: 4.5040
Iteration: 62; Percent complete: 1.6%; Average loss: 4.6425
Iteration: 63; Percent complete: 1.6%; Average loss: 4.6127
Iteration: 64; Percent complete: 1.6%; Average loss: 4.4516
Iteration: 65; Percent complete: 1.6%; Average loss: 4.6772
Iteration: 66; Percent complete: 1.7%; Average loss: 4.6376
Iteration: 67; Percent complete: 1.7%; Average loss: 4.4957
Iteration: 68; Percent complete: 1.7%; Average loss: 4.4566
Iteration: 69; Percent complete: 1.7%; Average loss: 4.2983
Iteration: 70; Percent complete: 1.8%; Average loss: 4.4659
Iteration: 71; Percent complete: 1.8%; Average loss: 4.4535
Iteration: 72; Percent complete: 1.8%; Average loss: 4.4993
Iteration: 73; Percent complete: 1.8%; Average loss: 4.5407
Iteration: 74; Percent complete: 1.8%; Average loss: 4.6925
Iteration: 75; Percent complete: 1.9%; Average loss: 4.4913
Iteration: 76; Percent complete: 1.9%; Average loss: 4.3785
Iteration: 77; Percent complete: 1.9%; Average loss: 4.6116
Iteration: 78; Percent complete: 1.9%; Average loss: 4.4776
Iteration: 79; Percent complete: 2.0%; Average loss: 4.7877
Iteration: 80; Percent complete: 2.0%; Average loss: 4.7234
Iteration: 81; Percent complete: 2.0%; Average loss: 4.1763
Iteration: 82; Percent complete: 2.1%; Average loss: 4.4704
Iteration: 83; Percent complete: 2.1%; Average loss: 4.5362
Iteration: 84; Percent complete: 2.1%; Average loss: 4.5116
Iteration: 85; Percent complete: 2.1%; Average loss: 4.4163
Iteration: 86; Percent complete: 2.1%; Average loss: 4.3802
Iteration: 87; Percent complete: 2.2%; Average loss: 4.4093
Iteration: 88; Percent complete: 2.2%; Average loss: 4.3776
Iteration: 89; Percent complete: 2.2%; Average loss: 4.5559
Iteration: 90; Percent complete: 2.2%; Average loss: 4.3777
Iteration: 91; Percent complete: 2.3%; Average loss: 4.1819
Iteration: 92; Percent complete: 2.3%; Average loss: 4.0873
Iteration: 93; Percent complete: 2.3%; Average loss: 4.3758
Iteration: 94; Percent complete: 2.4%; Average loss: 4.1678
Iteration: 95; Percent complete: 2.4%; Average loss: 4.1302
Iteration: 96; Percent complete: 2.4%; Average loss: 4.4141
Iteration: 97; Percent complete: 2.4%; Average loss: 4.3121
Iteration: 98; Percent complete: 2.5%; Average loss: 4.3845
Iteration: 99; Percent complete: 2.5%; Average loss: 4.3235
Iteration: 100; Percent complete: 2.5%; Average loss: 4.3553
Iteration: 101; Percent complete: 2.5%; Average loss: 4.2070
Iteration: 102; Percent complete: 2.5%; Average loss: 4.4206
Iteration: 103; Percent complete: 2.6%; Average loss: 4.2460
Iteration: 104; Percent complete: 2.6%; Average loss: 4.4176
Iteration: 105; Percent complete: 2.6%; Average loss: 4.2437
Iteration: 106; Percent complete: 2.6%; Average loss: 4.3766
Iteration: 107; Percent complete: 2.7%; Average loss: 4.2291
Iteration: 108; Percent complete: 2.7%; Average loss: 4.4596
Iteration: 109; Percent complete: 2.7%; Average loss: 4.4927
Iteration: 110; Percent complete: 2.8%; Average loss: 4.4274
Iteration: 111; Percent complete: 2.8%; Average loss: 4.3089
Iteration: 112; Percent complete: 2.8%; Average loss: 4.1298
Iteration: 113; Percent complete: 2.8%; Average loss: 4.2670
Iteration: 114; Percent complete: 2.9%; Average loss: 4.6080
Iteration: 115; Percent complete: 2.9%; Average loss: 4.4209
Iteration: 116; Percent complete: 2.9%; Average loss: 4.2770
Iteration: 117; Percent complete: 2.9%; Average loss: 4.4726
Iteration: 118; Percent complete: 2.9%; Average loss: 4.2269
Iteration: 119; Percent complete: 3.0%; Average loss: 4.6540
Iteration: 120; Percent complete: 3.0%; Average loss: 4.3868
Iteration: 121; Percent complete: 3.0%; Average loss: 4.2066
Iteration: 122; Percent complete: 3.0%; Average loss: 4.3099
Iteration: 123; Percent complete: 3.1%; Average loss: 3.9908
Iteration: 124; Percent complete: 3.1%; Average loss: 4.2614
Iteration: 125; Percent complete: 3.1%; Average loss: 4.3858
Iteration: 126; Percent complete: 3.1%; Average loss: 4.3401
Iteration: 127; Percent complete: 3.2%; Average loss: 4.2203
Iteration: 128; Percent complete: 3.2%; Average loss: 4.1521
Iteration: 129; Percent complete: 3.2%; Average loss: 4.2143
Iteration: 130; Percent complete: 3.2%; Average loss: 4.3594
Iteration: 131; Percent complete: 3.3%; Average loss: 4.2838
Iteration: 132; Percent complete: 3.3%; Average loss: 4.2534
Iteration: 133; Percent complete: 3.3%; Average loss: 4.4141
Iteration: 134; Percent complete: 3.4%; Average loss: 4.1108
Iteration: 135; Percent complete: 3.4%; Average loss: 4.3941
Iteration: 136; Percent complete: 3.4%; Average loss: 4.2821
Iteration: 137; Percent complete: 3.4%; Average loss: 4.2953
Iteration: 138; Percent complete: 3.5%; Average loss: 4.3751
Iteration: 139; Percent complete: 3.5%; Average loss: 4.5178
Iteration: 140; Percent complete: 3.5%; Average loss: 4.0683
Iteration: 141; Percent complete: 3.5%; Average loss: 4.2605
Iteration: 142; Percent complete: 3.5%; Average loss: 4.2920
Iteration: 143; Percent complete: 3.6%; Average loss: 3.9052
Iteration: 144; Percent complete: 3.6%; Average loss: 4.0755
Iteration: 145; Percent complete: 3.6%; Average loss: 4.2734
Iteration: 146; Percent complete: 3.6%; Average loss: 4.1397
Iteration: 147; Percent complete: 3.7%; Average loss: 4.0804
Iteration: 148; Percent complete: 3.7%; Average loss: 4.0364
Iteration: 149; Percent complete: 3.7%; Average loss: 4.4056
Iteration: 150; Percent complete: 3.8%; Average loss: 4.2191
Iteration: 151; Percent complete: 3.8%; Average loss: 4.2066
Iteration: 152; Percent complete: 3.8%; Average loss: 4.3789
Iteration: 153; Percent complete: 3.8%; Average loss: 3.9339
Iteration: 154; Percent complete: 3.9%; Average loss: 4.2397
Iteration: 155; Percent complete: 3.9%; Average loss: 4.1038
Iteration: 156; Percent complete: 3.9%; Average loss: 4.0594
Iteration: 157; Percent complete: 3.9%; Average loss: 4.2168
Iteration: 158; Percent complete: 4.0%; Average loss: 4.2275
Iteration: 159; Percent complete: 4.0%; Average loss: 4.2411
Iteration: 160; Percent complete: 4.0%; Average loss: 4.3014
Iteration: 161; Percent complete: 4.0%; Average loss: 4.3327
Iteration: 162; Percent complete: 4.0%; Average loss: 4.3695
Iteration: 163; Percent complete: 4.1%; Average loss: 4.0560
Iteration: 164; Percent complete: 4.1%; Average loss: 4.1770
Iteration: 165; Percent complete: 4.1%; Average loss: 3.8956
Iteration: 166; Percent complete: 4.2%; Average loss: 4.0064
Iteration: 167; Percent complete: 4.2%; Average loss: 4.0793
Iteration: 168; Percent complete: 4.2%; Average loss: 4.0561
Iteration: 169; Percent complete: 4.2%; Average loss: 3.9781
Iteration: 170; Percent complete: 4.2%; Average loss: 3.9492
Iteration: 171; Percent complete: 4.3%; Average loss: 4.0514
Iteration: 172; Percent complete: 4.3%; Average loss: 4.0700
Iteration: 173; Percent complete: 4.3%; Average loss: 4.1024
Iteration: 174; Percent complete: 4.3%; Average loss: 4.2975
Iteration: 175; Percent complete: 4.4%; Average loss: 4.2410
Iteration: 176; Percent complete: 4.4%; Average loss: 4.3840
Iteration: 177; Percent complete: 4.4%; Average loss: 4.1402
Iteration: 178; Percent complete: 4.5%; Average loss: 4.0217
Iteration: 179; Percent complete: 4.5%; Average loss: 4.3414
Iteration: 180; Percent complete: 4.5%; Average loss: 3.9880
Iteration: 181; Percent complete: 4.5%; Average loss: 4.2015
Iteration: 182; Percent complete: 4.5%; Average loss: 4.0772
Iteration: 183; Percent complete: 4.6%; Average loss: 3.8861
Iteration: 184; Percent complete: 4.6%; Average loss: 4.2767
Iteration: 185; Percent complete: 4.6%; Average loss: 4.0194
Iteration: 186; Percent complete: 4.7%; Average loss: 3.9277
Iteration: 187; Percent complete: 4.7%; Average loss: 4.2186
Iteration: 188; Percent complete: 4.7%; Average loss: 3.8042
Iteration: 189; Percent complete: 4.7%; Average loss: 3.9961
Iteration: 190; Percent complete: 4.8%; Average loss: 4.1070
Iteration: 191; Percent complete: 4.8%; Average loss: 4.0864
Iteration: 192; Percent complete: 4.8%; Average loss: 3.8912
Iteration: 193; Percent complete: 4.8%; Average loss: 4.4608
Iteration: 194; Percent complete: 4.9%; Average loss: 4.2040
Iteration: 195; Percent complete: 4.9%; Average loss: 3.8111
Iteration: 196; Percent complete: 4.9%; Average loss: 4.1497
Iteration: 197; Percent complete: 4.9%; Average loss: 4.2127
Iteration: 198; Percent complete: 5.0%; Average loss: 4.3278
Iteration: 199; Percent complete: 5.0%; Average loss: 3.8839
Iteration: 200; Percent complete: 5.0%; Average loss: 3.7620
Iteration: 201; Percent complete: 5.0%; Average loss: 3.9327
Iteration: 202; Percent complete: 5.1%; Average loss: 4.2118
Iteration: 203; Percent complete: 5.1%; Average loss: 4.1172
Iteration: 204; Percent complete: 5.1%; Average loss: 4.1232
Iteration: 205; Percent complete: 5.1%; Average loss: 3.8951
Iteration: 206; Percent complete: 5.1%; Average loss: 4.2503
Iteration: 207; Percent complete: 5.2%; Average loss: 3.9660
Iteration: 208; Percent complete: 5.2%; Average loss: 4.0551
Iteration: 209; Percent complete: 5.2%; Average loss: 4.0939
Iteration: 210; Percent complete: 5.2%; Average loss: 4.0018
Iteration: 211; Percent complete: 5.3%; Average loss: 4.1827
Iteration: 212; Percent complete: 5.3%; Average loss: 4.0296
Iteration: 213; Percent complete: 5.3%; Average loss: 3.9588
Iteration: 214; Percent complete: 5.3%; Average loss: 4.2279
Iteration: 215; Percent complete: 5.4%; Average loss: 4.0487
Iteration: 216; Percent complete: 5.4%; Average loss: 3.9734
Iteration: 217; Percent complete: 5.4%; Average loss: 3.8417
Iteration: 218; Percent complete: 5.5%; Average loss: 3.8809
Iteration: 219; Percent complete: 5.5%; Average loss: 4.0768
Iteration: 220; Percent complete: 5.5%; Average loss: 3.9959
Iteration: 221; Percent complete: 5.5%; Average loss: 4.1274
Iteration: 222; Percent complete: 5.5%; Average loss: 3.9083
Iteration: 223; Percent complete: 5.6%; Average loss: 3.6864
Iteration: 224; Percent complete: 5.6%; Average loss: 4.0271
Iteration: 225; Percent complete: 5.6%; Average loss: 3.8641
Iteration: 226; Percent complete: 5.7%; Average loss: 3.9547
Iteration: 227; Percent complete: 5.7%; Average loss: 4.0166
Iteration: 228; Percent complete: 5.7%; Average loss: 4.1438
Iteration: 229; Percent complete: 5.7%; Average loss: 4.1116
Iteration: 230; Percent complete: 5.8%; Average loss: 4.0175
Iteration: 231; Percent complete: 5.8%; Average loss: 4.2920
Iteration: 232; Percent complete: 5.8%; Average loss: 4.0645
Iteration: 233; Percent complete: 5.8%; Average loss: 3.7954
Iteration: 234; Percent complete: 5.9%; Average loss: 4.0073
Iteration: 235; Percent complete: 5.9%; Average loss: 4.1474
Iteration: 236; Percent complete: 5.9%; Average loss: 4.0504
Iteration: 237; Percent complete: 5.9%; Average loss: 4.0581
Iteration: 238; Percent complete: 5.9%; Average loss: 4.2039
Iteration: 239; Percent complete: 6.0%; Average loss: 4.1531
Iteration: 240; Percent complete: 6.0%; Average loss: 4.2721
Iteration: 241; Percent complete: 6.0%; Average loss: 3.6714
Iteration: 242; Percent complete: 6.0%; Average loss: 4.0335
Iteration: 243; Percent complete: 6.1%; Average loss: 3.8752
Iteration: 244; Percent complete: 6.1%; Average loss: 3.9017
Iteration: 245; Percent complete: 6.1%; Average loss: 3.9139
Iteration: 246; Percent complete: 6.2%; Average loss: 3.7217
Iteration: 247; Percent complete: 6.2%; Average loss: 3.7023
Iteration: 248; Percent complete: 6.2%; Average loss: 4.1358
Iteration: 249; Percent complete: 6.2%; Average loss: 4.0772
Iteration: 250; Percent complete: 6.2%; Average loss: 4.0425
Iteration: 251; Percent complete: 6.3%; Average loss: 3.9186
Iteration: 252; Percent complete: 6.3%; Average loss: 4.3097
Iteration: 253; Percent complete: 6.3%; Average loss: 4.0556
Iteration: 254; Percent complete: 6.3%; Average loss: 3.8985
Iteration: 255; Percent complete: 6.4%; Average loss: 3.9799
Iteration: 256; Percent complete: 6.4%; Average loss: 4.1424
Iteration: 257; Percent complete: 6.4%; Average loss: 3.9666
Iteration: 258; Percent complete: 6.5%; Average loss: 3.6854
Iteration: 259; Percent complete: 6.5%; Average loss: 3.7523
Iteration: 260; Percent complete: 6.5%; Average loss: 4.1942
Iteration: 261; Percent complete: 6.5%; Average loss: 4.1555
Iteration: 262; Percent complete: 6.6%; Average loss: 4.0876
Iteration: 263; Percent complete: 6.6%; Average loss: 4.0691
Iteration: 264; Percent complete: 6.6%; Average loss: 4.1809
Iteration: 265; Percent complete: 6.6%; Average loss: 4.1310
Iteration: 266; Percent complete: 6.7%; Average loss: 4.0696
Iteration: 267; Percent complete: 6.7%; Average loss: 4.1119
Iteration: 268; Percent complete: 6.7%; Average loss: 4.1710
Iteration: 269; Percent complete: 6.7%; Average loss: 3.9053
Iteration: 270; Percent complete: 6.8%; Average loss: 3.7342
Iteration: 271; Percent complete: 6.8%; Average loss: 3.6321
Iteration: 272; Percent complete: 6.8%; Average loss: 4.0387
Iteration: 273; Percent complete: 6.8%; Average loss: 4.1067
Iteration: 274; Percent complete: 6.9%; Average loss: 3.8863
Iteration: 275; Percent complete: 6.9%; Average loss: 4.1265
Iteration: 276; Percent complete: 6.9%; Average loss: 3.9716
Iteration: 277; Percent complete: 6.9%; Average loss: 3.7784
Iteration: 278; Percent complete: 7.0%; Average loss: 3.7799
Iteration: 279; Percent complete: 7.0%; Average loss: 3.8327
Iteration: 280; Percent complete: 7.0%; Average loss: 3.9779
Iteration: 281; Percent complete: 7.0%; Average loss: 3.7546
Iteration: 282; Percent complete: 7.0%; Average loss: 3.6955
Iteration: 283; Percent complete: 7.1%; Average loss: 3.7805
Iteration: 284; Percent complete: 7.1%; Average loss: 4.1907
Iteration: 285; Percent complete: 7.1%; Average loss: 4.1854
Iteration: 286; Percent complete: 7.1%; Average loss: 3.7925
Iteration: 287; Percent complete: 7.2%; Average loss: 3.8029
Iteration: 288; Percent complete: 7.2%; Average loss: 3.9065
Iteration: 289; Percent complete: 7.2%; Average loss: 3.7952
Iteration: 290; Percent complete: 7.2%; Average loss: 3.7639
Iteration: 291; Percent complete: 7.3%; Average loss: 4.1821
Iteration: 292; Percent complete: 7.3%; Average loss: 4.0123
Iteration: 293; Percent complete: 7.3%; Average loss: 3.8019
Iteration: 294; Percent complete: 7.3%; Average loss: 4.0496
Iteration: 295; Percent complete: 7.4%; Average loss: 3.7498
Iteration: 296; Percent complete: 7.4%; Average loss: 3.5891
Iteration: 297; Percent complete: 7.4%; Average loss: 3.7761
Iteration: 298; Percent complete: 7.4%; Average loss: 3.8885
Iteration: 299; Percent complete: 7.5%; Average loss: 3.8998
Iteration: 300; Percent complete: 7.5%; Average loss: 3.5944
Iteration: 301; Percent complete: 7.5%; Average loss: 3.8512
Iteration: 302; Percent complete: 7.5%; Average loss: 3.8508
Iteration: 303; Percent complete: 7.6%; Average loss: 3.6034
Iteration: 304; Percent complete: 7.6%; Average loss: 4.0359
Iteration: 305; Percent complete: 7.6%; Average loss: 3.9190
Iteration: 306; Percent complete: 7.6%; Average loss: 3.9872
Iteration: 307; Percent complete: 7.7%; Average loss: 4.1190
Iteration: 308; Percent complete: 7.7%; Average loss: 3.9301
Iteration: 309; Percent complete: 7.7%; Average loss: 3.7372
Iteration: 310; Percent complete: 7.8%; Average loss: 3.6829
Iteration: 311; Percent complete: 7.8%; Average loss: 4.1411
Iteration: 312; Percent complete: 7.8%; Average loss: 4.1205
Iteration: 313; Percent complete: 7.8%; Average loss: 3.8663
Iteration: 314; Percent complete: 7.8%; Average loss: 3.8840
Iteration: 315; Percent complete: 7.9%; Average loss: 3.9021
Iteration: 316; Percent complete: 7.9%; Average loss: 4.0300
Iteration: 317; Percent complete: 7.9%; Average loss: 3.5764
Iteration: 318; Percent complete: 8.0%; Average loss: 4.1867
Iteration: 319; Percent complete: 8.0%; Average loss: 3.8519
Iteration: 320; Percent complete: 8.0%; Average loss: 3.9370
Iteration: 321; Percent complete: 8.0%; Average loss: 3.6789
Iteration: 322; Percent complete: 8.1%; Average loss: 3.8827
Iteration: 323; Percent complete: 8.1%; Average loss: 3.8542
Iteration: 324; Percent complete: 8.1%; Average loss: 4.0032
Iteration: 325; Percent complete: 8.1%; Average loss: 3.8164
Iteration: 326; Percent complete: 8.2%; Average loss: 3.8637
Iteration: 327; Percent complete: 8.2%; Average loss: 3.5916
Iteration: 328; Percent complete: 8.2%; Average loss: 3.7643
Iteration: 329; Percent complete: 8.2%; Average loss: 3.8203
Iteration: 330; Percent complete: 8.2%; Average loss: 3.7982
Iteration: 331; Percent complete: 8.3%; Average loss: 3.9937
Iteration: 332; Percent complete: 8.3%; Average loss: 3.8901
Iteration: 333; Percent complete: 8.3%; Average loss: 3.5710
Iteration: 334; Percent complete: 8.3%; Average loss: 3.9224
Iteration: 335; Percent complete: 8.4%; Average loss: 3.9094
Iteration: 336; Percent complete: 8.4%; Average loss: 4.0130
Iteration: 337; Percent complete: 8.4%; Average loss: 3.7738
Iteration: 338; Percent complete: 8.5%; Average loss: 3.5254
Iteration: 339; Percent complete: 8.5%; Average loss: 3.8460
Iteration: 340; Percent complete: 8.5%; Average loss: 4.0452
Iteration: 341; Percent complete: 8.5%; Average loss: 3.9702
Iteration: 342; Percent complete: 8.6%; Average loss: 3.8501
Iteration: 343; Percent complete: 8.6%; Average loss: 3.9461
Iteration: 344; Percent complete: 8.6%; Average loss: 3.9238
Iteration: 345; Percent complete: 8.6%; Average loss: 3.7040
Iteration: 346; Percent complete: 8.6%; Average loss: 3.8080
Iteration: 347; Percent complete: 8.7%; Average loss: 3.8315
Iteration: 348; Percent complete: 8.7%; Average loss: 3.9838
Iteration: 349; Percent complete: 8.7%; Average loss: 3.9795
Iteration: 350; Percent complete: 8.8%; Average loss: 4.0843
Iteration: 351; Percent complete: 8.8%; Average loss: 3.9248
Iteration: 352; Percent complete: 8.8%; Average loss: 3.9290
Iteration: 353; Percent complete: 8.8%; Average loss: 3.9382
Iteration: 354; Percent complete: 8.8%; Average loss: 3.6967
Iteration: 355; Percent complete: 8.9%; Average loss: 3.7205
Iteration: 356; Percent complete: 8.9%; Average loss: 3.7111
Iteration: 357; Percent complete: 8.9%; Average loss: 3.9285
Iteration: 358; Percent complete: 8.9%; Average loss: 3.7452
Iteration: 359; Percent complete: 9.0%; Average loss: 3.7257
Iteration: 360; Percent complete: 9.0%; Average loss: 3.6536
Iteration: 361; Percent complete: 9.0%; Average loss: 3.8149
Iteration: 362; Percent complete: 9.0%; Average loss: 3.9398
Iteration: 363; Percent complete: 9.1%; Average loss: 3.8566
Iteration: 364; Percent complete: 9.1%; Average loss: 3.7189
Iteration: 365; Percent complete: 9.1%; Average loss: 3.8877
Iteration: 366; Percent complete: 9.2%; Average loss: 3.7158
Iteration: 367; Percent complete: 9.2%; Average loss: 3.8309
Iteration: 368; Percent complete: 9.2%; Average loss: 3.8509
Iteration: 369; Percent complete: 9.2%; Average loss: 3.7319
Iteration: 370; Percent complete: 9.2%; Average loss: 3.7325
Iteration: 371; Percent complete: 9.3%; Average loss: 3.8450
Iteration: 372; Percent complete: 9.3%; Average loss: 3.8536
Iteration: 373; Percent complete: 9.3%; Average loss: 3.7753
Iteration: 374; Percent complete: 9.3%; Average loss: 4.1307
Iteration: 375; Percent complete: 9.4%; Average loss: 3.7170
Iteration: 376; Percent complete: 9.4%; Average loss: 3.8005
Iteration: 377; Percent complete: 9.4%; Average loss: 3.8363
Iteration: 378; Percent complete: 9.4%; Average loss: 3.9310
Iteration: 379; Percent complete: 9.5%; Average loss: 3.7841
Iteration: 380; Percent complete: 9.5%; Average loss: 3.7731
Iteration: 381; Percent complete: 9.5%; Average loss: 3.7250
Iteration: 382; Percent complete: 9.6%; Average loss: 3.9608
Iteration: 383; Percent complete: 9.6%; Average loss: 3.8859
Iteration: 384; Percent complete: 9.6%; Average loss: 3.9835
Iteration: 385; Percent complete: 9.6%; Average loss: 4.0088
Iteration: 386; Percent complete: 9.7%; Average loss: 3.7859
Iteration: 387; Percent complete: 9.7%; Average loss: 3.6878
Iteration: 388; Percent complete: 9.7%; Average loss: 3.7701
Iteration: 389; Percent complete: 9.7%; Average loss: 3.7807
Iteration: 390; Percent complete: 9.8%; Average loss: 3.5649
Iteration: 391; Percent complete: 9.8%; Average loss: 3.9786
Iteration: 392; Percent complete: 9.8%; Average loss: 3.9322
Iteration: 393; Percent complete: 9.8%; Average loss: 3.6200
Iteration: 394; Percent complete: 9.8%; Average loss: 3.7071
Iteration: 395; Percent complete: 9.9%; Average loss: 3.9700
Iteration: 396; Percent complete: 9.9%; Average loss: 3.9340
Iteration: 397; Percent complete: 9.9%; Average loss: 3.6973
Iteration: 398; Percent complete: 10.0%; Average loss: 3.7643
Iteration: 399; Percent complete: 10.0%; Average loss: 3.7398
Iteration: 400; Percent complete: 10.0%; Average loss: 3.6722
Iteration: 401; Percent complete: 10.0%; Average loss: 4.0725
Iteration: 402; Percent complete: 10.1%; Average loss: 4.5208
Iteration: 403; Percent complete: 10.1%; Average loss: 3.8056
Iteration: 404; Percent complete: 10.1%; Average loss: 4.0200
Iteration: 405; Percent complete: 10.1%; Average loss: 3.8778
Iteration: 406; Percent complete: 10.2%; Average loss: 4.1849
Iteration: 407; Percent complete: 10.2%; Average loss: 3.7099
Iteration: 408; Percent complete: 10.2%; Average loss: 3.5961
Iteration: 409; Percent complete: 10.2%; Average loss: 3.8026
Iteration: 410; Percent complete: 10.2%; Average loss: 3.7619
Iteration: 411; Percent complete: 10.3%; Average loss: 3.6208
Iteration: 412; Percent complete: 10.3%; Average loss: 3.7872
Iteration: 413; Percent complete: 10.3%; Average loss: 3.8628
Iteration: 414; Percent complete: 10.3%; Average loss: 3.9061
Iteration: 415; Percent complete: 10.4%; Average loss: 3.9426
Iteration: 416; Percent complete: 10.4%; Average loss: 3.7155
Iteration: 417; Percent complete: 10.4%; Average loss: 3.6924
Iteration: 418; Percent complete: 10.4%; Average loss: 3.9717
Iteration: 419; Percent complete: 10.5%; Average loss: 3.8160
Iteration: 420; Percent complete: 10.5%; Average loss: 3.8704
Iteration: 421; Percent complete: 10.5%; Average loss: 3.7577
Iteration: 422; Percent complete: 10.5%; Average loss: 3.6778
Iteration: 423; Percent complete: 10.6%; Average loss: 3.8709
Iteration: 424; Percent complete: 10.6%; Average loss: 3.8934
Iteration: 425; Percent complete: 10.6%; Average loss: 3.7519
Iteration: 426; Percent complete: 10.7%; Average loss: 3.4634
Iteration: 427; Percent complete: 10.7%; Average loss: 3.6829
Iteration: 428; Percent complete: 10.7%; Average loss: 3.5181
Iteration: 429; Percent complete: 10.7%; Average loss: 4.0757
Iteration: 430; Percent complete: 10.8%; Average loss: 3.6832
Iteration: 431; Percent complete: 10.8%; Average loss: 3.9460
Iteration: 432; Percent complete: 10.8%; Average loss: 3.9164
Iteration: 433; Percent complete: 10.8%; Average loss: 3.6932
Iteration: 434; Percent complete: 10.8%; Average loss: 3.7918
Iteration: 435; Percent complete: 10.9%; Average loss: 4.1048
Iteration: 436; Percent complete: 10.9%; Average loss: 3.5953
Iteration: 437; Percent complete: 10.9%; Average loss: 3.9972
Iteration: 438; Percent complete: 10.9%; Average loss: 3.8378
Iteration: 439; Percent complete: 11.0%; Average loss: 3.7084
Iteration: 440; Percent complete: 11.0%; Average loss: 3.9118
Iteration: 441; Percent complete: 11.0%; Average loss: 3.6079
Iteration: 442; Percent complete: 11.1%; Average loss: 3.5799
Iteration: 443; Percent complete: 11.1%; Average loss: 3.8671
Iteration: 444; Percent complete: 11.1%; Average loss: 3.7834
Iteration: 445; Percent complete: 11.1%; Average loss: 3.7690
Iteration: 446; Percent complete: 11.2%; Average loss: 3.6988
Iteration: 447; Percent complete: 11.2%; Average loss: 3.7646
Iteration: 448; Percent complete: 11.2%; Average loss: 3.8437
Iteration: 449; Percent complete: 11.2%; Average loss: 3.8342
Iteration: 450; Percent complete: 11.2%; Average loss: 4.0000
Iteration: 451; Percent complete: 11.3%; Average loss: 3.6813
Iteration: 452; Percent complete: 11.3%; Average loss: 3.8736
Iteration: 453; Percent complete: 11.3%; Average loss: 3.8471
Iteration: 454; Percent complete: 11.3%; Average loss: 3.8403
Iteration: 455; Percent complete: 11.4%; Average loss: 3.6537
Iteration: 456; Percent complete: 11.4%; Average loss: 3.8054
Iteration: 457; Percent complete: 11.4%; Average loss: 3.9282
Iteration: 458; Percent complete: 11.5%; Average loss: 3.9647
Iteration: 459; Percent complete: 11.5%; Average loss: 3.9317
Iteration: 460; Percent complete: 11.5%; Average loss: 3.6906
Iteration: 461; Percent complete: 11.5%; Average loss: 3.7646
Iteration: 462; Percent complete: 11.6%; Average loss: 3.5635
Iteration: 463; Percent complete: 11.6%; Average loss: 3.6501
Iteration: 464; Percent complete: 11.6%; Average loss: 3.8331
Iteration: 465; Percent complete: 11.6%; Average loss: 3.6354
Iteration: 466; Percent complete: 11.7%; Average loss: 3.7872
Iteration: 467; Percent complete: 11.7%; Average loss: 3.7677
Iteration: 468; Percent complete: 11.7%; Average loss: 3.8080
Iteration: 469; Percent complete: 11.7%; Average loss: 3.5741
Iteration: 470; Percent complete: 11.8%; Average loss: 3.6645
Iteration: 471; Percent complete: 11.8%; Average loss: 4.0274
Iteration: 472; Percent complete: 11.8%; Average loss: 3.6676
Iteration: 473; Percent complete: 11.8%; Average loss: 3.4729
Iteration: 474; Percent complete: 11.8%; Average loss: 3.5348
Iteration: 475; Percent complete: 11.9%; Average loss: 3.7442
Iteration: 476; Percent complete: 11.9%; Average loss: 3.6868
Iteration: 477; Percent complete: 11.9%; Average loss: 3.8223
Iteration: 478; Percent complete: 11.9%; Average loss: 3.5547
Iteration: 479; Percent complete: 12.0%; Average loss: 3.6144
Iteration: 480; Percent complete: 12.0%; Average loss: 3.8737
Iteration: 481; Percent complete: 12.0%; Average loss: 3.8205
Iteration: 482; Percent complete: 12.0%; Average loss: 3.6242
Iteration: 483; Percent complete: 12.1%; Average loss: 3.9903
Iteration: 484; Percent complete: 12.1%; Average loss: 3.6266
Iteration: 485; Percent complete: 12.1%; Average loss: 3.7898
Iteration: 486; Percent complete: 12.2%; Average loss: 3.8197
Iteration: 487; Percent complete: 12.2%; Average loss: 3.4811
Iteration: 488; Percent complete: 12.2%; Average loss: 3.7553
Iteration: 489; Percent complete: 12.2%; Average loss: 3.5799
Iteration: 490; Percent complete: 12.2%; Average loss: 3.7773
Iteration: 491; Percent complete: 12.3%; Average loss: 3.5693
Iteration: 492; Percent complete: 12.3%; Average loss: 3.8693
Iteration: 493; Percent complete: 12.3%; Average loss: 4.0777
Iteration: 494; Percent complete: 12.3%; Average loss: 3.8287
Iteration: 495; Percent complete: 12.4%; Average loss: 3.4930
Iteration: 496; Percent complete: 12.4%; Average loss: 3.5108
Iteration: 497; Percent complete: 12.4%; Average loss: 3.7908
Iteration: 498; Percent complete: 12.4%; Average loss: 3.6334
Iteration: 499; Percent complete: 12.5%; Average loss: 3.6121
Iteration: 500; Percent complete: 12.5%; Average loss: 3.9153
Iteration: 501; Percent complete: 12.5%; Average loss: 3.4436
Iteration: 502; Percent complete: 12.6%; Average loss: 3.6279
Iteration: 503; Percent complete: 12.6%; Average loss: 3.7370
Iteration: 504; Percent complete: 12.6%; Average loss: 3.4317
Iteration: 505; Percent complete: 12.6%; Average loss: 3.7068
Iteration: 506; Percent complete: 12.7%; Average loss: 3.7175
Iteration: 507; Percent complete: 12.7%; Average loss: 3.5811
Iteration: 508; Percent complete: 12.7%; Average loss: 3.7230
Iteration: 509; Percent complete: 12.7%; Average loss: 3.5589
Iteration: 510; Percent complete: 12.8%; Average loss: 3.9324
Iteration: 511; Percent complete: 12.8%; Average loss: 3.8061
Iteration: 512; Percent complete: 12.8%; Average loss: 3.8984
Iteration: 513; Percent complete: 12.8%; Average loss: 3.5134
Iteration: 514; Percent complete: 12.8%; Average loss: 3.6695
Iteration: 515; Percent complete: 12.9%; Average loss: 3.7814
Iteration: 516; Percent complete: 12.9%; Average loss: 3.7406
Iteration: 517; Percent complete: 12.9%; Average loss: 3.7750
Iteration: 518; Percent complete: 13.0%; Average loss: 3.8638
Iteration: 519; Percent complete: 13.0%; Average loss: 3.6251
Iteration: 520; Percent complete: 13.0%; Average loss: 3.6691
Iteration: 521; Percent complete: 13.0%; Average loss: 3.6104
Iteration: 522; Percent complete: 13.1%; Average loss: 3.8071
Iteration: 523; Percent complete: 13.1%; Average loss: 3.8242
Iteration: 524; Percent complete: 13.1%; Average loss: 3.9226
Iteration: 525; Percent complete: 13.1%; Average loss: 3.8062
Iteration: 526; Percent complete: 13.2%; Average loss: 3.6185
Iteration: 527; Percent complete: 13.2%; Average loss: 3.8578
Iteration: 528; Percent complete: 13.2%; Average loss: 3.5228
Iteration: 529; Percent complete: 13.2%; Average loss: 3.9559
Iteration: 530; Percent complete: 13.2%; Average loss: 3.9150
Iteration: 531; Percent complete: 13.3%; Average loss: 3.8387
Iteration: 532; Percent complete: 13.3%; Average loss: 3.6793
Iteration: 533; Percent complete: 13.3%; Average loss: 3.6438
Iteration: 534; Percent complete: 13.4%; Average loss: 3.8545
Iteration: 535; Percent complete: 13.4%; Average loss: 3.8904
Iteration: 536; Percent complete: 13.4%; Average loss: 3.7735
Iteration: 537; Percent complete: 13.4%; Average loss: 3.9697
Iteration: 538; Percent complete: 13.5%; Average loss: 3.6741
Iteration: 539; Percent complete: 13.5%; Average loss: 3.6578
Iteration: 540; Percent complete: 13.5%; Average loss: 3.6863
Iteration: 541; Percent complete: 13.5%; Average loss: 3.7410
Iteration: 542; Percent complete: 13.6%; Average loss: 3.6528
Iteration: 543; Percent complete: 13.6%; Average loss: 3.4911
Iteration: 544; Percent complete: 13.6%; Average loss: 3.7208
Iteration: 545; Percent complete: 13.6%; Average loss: 3.9635
Iteration: 546; Percent complete: 13.7%; Average loss: 3.7256
Iteration: 547; Percent complete: 13.7%; Average loss: 3.7487
Iteration: 548; Percent complete: 13.7%; Average loss: 3.6041
Iteration: 549; Percent complete: 13.7%; Average loss: 3.5516
Iteration: 550; Percent complete: 13.8%; Average loss: 3.7326
Iteration: 551; Percent complete: 13.8%; Average loss: 3.6444
Iteration: 552; Percent complete: 13.8%; Average loss: 3.6505
Iteration: 553; Percent complete: 13.8%; Average loss: 3.7835
Iteration: 554; Percent complete: 13.9%; Average loss: 3.5703
Iteration: 555; Percent complete: 13.9%; Average loss: 3.8452
Iteration: 556; Percent complete: 13.9%; Average loss: 3.4396
Iteration: 557; Percent complete: 13.9%; Average loss: 3.9528
Iteration: 558; Percent complete: 14.0%; Average loss: 3.8302
Iteration: 559; Percent complete: 14.0%; Average loss: 3.7089
Iteration: 560; Percent complete: 14.0%; Average loss: 3.9027
Iteration: 561; Percent complete: 14.0%; Average loss: 3.8303
Iteration: 562; Percent complete: 14.1%; Average loss: 3.8481
Iteration: 563; Percent complete: 14.1%; Average loss: 3.6504
Iteration: 564; Percent complete: 14.1%; Average loss: 3.5020
Iteration: 565; Percent complete: 14.1%; Average loss: 3.6662
Iteration: 566; Percent complete: 14.1%; Average loss: 3.6605
Iteration: 567; Percent complete: 14.2%; Average loss: 3.6969
Iteration: 568; Percent complete: 14.2%; Average loss: 3.6141
Iteration: 569; Percent complete: 14.2%; Average loss: 3.7778
Iteration: 570; Percent complete: 14.2%; Average loss: 3.5275
Iteration: 571; Percent complete: 14.3%; Average loss: 3.4330
Iteration: 572; Percent complete: 14.3%; Average loss: 3.5727
Iteration: 573; Percent complete: 14.3%; Average loss: 3.7951
Iteration: 574; Percent complete: 14.3%; Average loss: 3.4685
Iteration: 575; Percent complete: 14.4%; Average loss: 3.5158
Iteration: 576; Percent complete: 14.4%; Average loss: 3.5712
Iteration: 577; Percent complete: 14.4%; Average loss: 3.5686
Iteration: 578; Percent complete: 14.4%; Average loss: 3.6671
Iteration: 579; Percent complete: 14.5%; Average loss: 3.8080
Iteration: 580; Percent complete: 14.5%; Average loss: 3.5969
Iteration: 581; Percent complete: 14.5%; Average loss: 3.7889
Iteration: 582; Percent complete: 14.5%; Average loss: 3.6790
Iteration: 583; Percent complete: 14.6%; Average loss: 3.8357
Iteration: 584; Percent complete: 14.6%; Average loss: 3.6708
Iteration: 585; Percent complete: 14.6%; Average loss: 3.4313
Iteration: 586; Percent complete: 14.6%; Average loss: 3.6976
Iteration: 587; Percent complete: 14.7%; Average loss: 3.4836
Iteration: 588; Percent complete: 14.7%; Average loss: 3.7425
Iteration: 589; Percent complete: 14.7%; Average loss: 3.8017
Iteration: 590; Percent complete: 14.8%; Average loss: 3.6605
Iteration: 591; Percent complete: 14.8%; Average loss: 3.8721
Iteration: 592; Percent complete: 14.8%; Average loss: 3.4599
Iteration: 593; Percent complete: 14.8%; Average loss: 3.6236
Iteration: 594; Percent complete: 14.8%; Average loss: 3.7519
Iteration: 595; Percent complete: 14.9%; Average loss: 3.8252
Iteration: 596; Percent complete: 14.9%; Average loss: 3.3145
Iteration: 597; Percent complete: 14.9%; Average loss: 3.6364
Iteration: 598; Percent complete: 14.9%; Average loss: 3.6481
Iteration: 599; Percent complete: 15.0%; Average loss: 3.6433
Iteration: 600; Percent complete: 15.0%; Average loss: 3.6686
Iteration: 601; Percent complete: 15.0%; Average loss: 3.8736
Iteration: 602; Percent complete: 15.0%; Average loss: 3.7647
Iteration: 603; Percent complete: 15.1%; Average loss: 3.7494
Iteration: 604; Percent complete: 15.1%; Average loss: 3.5645
Iteration: 605; Percent complete: 15.1%; Average loss: 3.7842
Iteration: 606; Percent complete: 15.2%; Average loss: 3.6859
Iteration: 607; Percent complete: 15.2%; Average loss: 3.7276
Iteration: 608; Percent complete: 15.2%; Average loss: 3.7738
Iteration: 609; Percent complete: 15.2%; Average loss: 3.7070
Iteration: 610; Percent complete: 15.2%; Average loss: 3.7248
Iteration: 611; Percent complete: 15.3%; Average loss: 3.6264
Iteration: 612; Percent complete: 15.3%; Average loss: 3.6124
Iteration: 613; Percent complete: 15.3%; Average loss: 3.3380
Iteration: 614; Percent complete: 15.3%; Average loss: 3.6820
Iteration: 615; Percent complete: 15.4%; Average loss: 3.7655
Iteration: 616; Percent complete: 15.4%; Average loss: 3.5370
Iteration: 617; Percent complete: 15.4%; Average loss: 4.0736
Iteration: 618; Percent complete: 15.4%; Average loss: 3.6622
Iteration: 619; Percent complete: 15.5%; Average loss: 3.3354
Iteration: 620; Percent complete: 15.5%; Average loss: 3.3423
Iteration: 621; Percent complete: 15.5%; Average loss: 3.7680
Iteration: 622; Percent complete: 15.6%; Average loss: 3.5240
Iteration: 623; Percent complete: 15.6%; Average loss: 3.5344
Iteration: 624; Percent complete: 15.6%; Average loss: 3.9477
Iteration: 625; Percent complete: 15.6%; Average loss: 3.8006
Iteration: 626; Percent complete: 15.7%; Average loss: 3.5647
Iteration: 627; Percent complete: 15.7%; Average loss: 3.6684
Iteration: 628; Percent complete: 15.7%; Average loss: 3.4625
Iteration: 629; Percent complete: 15.7%; Average loss: 3.7408
Iteration: 630; Percent complete: 15.8%; Average loss: 3.7113
Iteration: 631; Percent complete: 15.8%; Average loss: 3.7108
Iteration: 632; Percent complete: 15.8%; Average loss: 3.7929
Iteration: 633; Percent complete: 15.8%; Average loss: 3.7441
Iteration: 634; Percent complete: 15.8%; Average loss: 3.6059
Iteration: 635; Percent complete: 15.9%; Average loss: 3.5314
Iteration: 636; Percent complete: 15.9%; Average loss: 3.5644
Iteration: 637; Percent complete: 15.9%; Average loss: 3.5376
Iteration: 638; Percent complete: 16.0%; Average loss: 3.9920
Iteration: 639; Percent complete: 16.0%; Average loss: 3.6038
Iteration: 640; Percent complete: 16.0%; Average loss: 3.4331
Iteration: 641; Percent complete: 16.0%; Average loss: 3.2810
Iteration: 642; Percent complete: 16.1%; Average loss: 3.8620
Iteration: 643; Percent complete: 16.1%; Average loss: 3.2829
Iteration: 644; Percent complete: 16.1%; Average loss: 3.9104
Iteration: 645; Percent complete: 16.1%; Average loss: 3.7314
Iteration: 646; Percent complete: 16.2%; Average loss: 3.6528
Iteration: 647; Percent complete: 16.2%; Average loss: 3.6931
Iteration: 648; Percent complete: 16.2%; Average loss: 3.5287
Iteration: 649; Percent complete: 16.2%; Average loss: 3.5333
Iteration: 650; Percent complete: 16.2%; Average loss: 3.5903
Iteration: 651; Percent complete: 16.3%; Average loss: 3.7355
Iteration: 652; Percent complete: 16.3%; Average loss: 3.6469
Iteration: 653; Percent complete: 16.3%; Average loss: 3.6178
Iteration: 654; Percent complete: 16.4%; Average loss: 3.6366
Iteration: 655; Percent complete: 16.4%; Average loss: 3.4181
Iteration: 656; Percent complete: 16.4%; Average loss: 3.2830
Iteration: 657; Percent complete: 16.4%; Average loss: 3.8263
Iteration: 658; Percent complete: 16.4%; Average loss: 3.7156
Iteration: 659; Percent complete: 16.5%; Average loss: 3.5839
Iteration: 660; Percent complete: 16.5%; Average loss: 3.6994
Iteration: 661; Percent complete: 16.5%; Average loss: 3.6490
Iteration: 662; Percent complete: 16.6%; Average loss: 3.8071
Iteration: 663; Percent complete: 16.6%; Average loss: 3.8757
Iteration: 664; Percent complete: 16.6%; Average loss: 3.8152
Iteration: 665; Percent complete: 16.6%; Average loss: 3.7146
Iteration: 666; Percent complete: 16.7%; Average loss: 3.6877
Iteration: 667; Percent complete: 16.7%; Average loss: 3.6475
Iteration: 668; Percent complete: 16.7%; Average loss: 3.5612
Iteration: 669; Percent complete: 16.7%; Average loss: 3.5221
Iteration: 670; Percent complete: 16.8%; Average loss: 3.7373
Iteration: 671; Percent complete: 16.8%; Average loss: 3.6845
Iteration: 672; Percent complete: 16.8%; Average loss: 3.4995
Iteration: 673; Percent complete: 16.8%; Average loss: 3.6777
Iteration: 674; Percent complete: 16.9%; Average loss: 3.4969
Iteration: 675; Percent complete: 16.9%; Average loss: 3.4158
Iteration: 676; Percent complete: 16.9%; Average loss: 3.7758
Iteration: 677; Percent complete: 16.9%; Average loss: 3.4244
Iteration: 678; Percent complete: 17.0%; Average loss: 3.4592
Iteration: 679; Percent complete: 17.0%; Average loss: 3.4869
Iteration: 680; Percent complete: 17.0%; Average loss: 3.6078
Iteration: 681; Percent complete: 17.0%; Average loss: 3.8138
Iteration: 682; Percent complete: 17.1%; Average loss: 3.6431
Iteration: 683; Percent complete: 17.1%; Average loss: 3.5461
Iteration: 684; Percent complete: 17.1%; Average loss: 3.6855
Iteration: 685; Percent complete: 17.1%; Average loss: 3.7921
Iteration: 686; Percent complete: 17.2%; Average loss: 3.8628
Iteration: 687; Percent complete: 17.2%; Average loss: 3.6130
Iteration: 688; Percent complete: 17.2%; Average loss: 3.7110
Iteration: 689; Percent complete: 17.2%; Average loss: 3.7512
Iteration: 690; Percent complete: 17.2%; Average loss: 3.6246
Iteration: 691; Percent complete: 17.3%; Average loss: 3.5947
Iteration: 692; Percent complete: 17.3%; Average loss: 3.6217
Iteration: 693; Percent complete: 17.3%; Average loss: 3.8250
Iteration: 694; Percent complete: 17.3%; Average loss: 3.7288
Iteration: 695; Percent complete: 17.4%; Average loss: 3.5812
Iteration: 696; Percent complete: 17.4%; Average loss: 3.8040
Iteration: 697; Percent complete: 17.4%; Average loss: 3.6890
Iteration: 698; Percent complete: 17.4%; Average loss: 3.8200
Iteration: 699; Percent complete: 17.5%; Average loss: 3.6782
Iteration: 700; Percent complete: 17.5%; Average loss: 3.7414
Iteration: 701; Percent complete: 17.5%; Average loss: 3.5280
Iteration: 702; Percent complete: 17.5%; Average loss: 3.4075
Iteration: 703; Percent complete: 17.6%; Average loss: 3.8202
Iteration: 704; Percent complete: 17.6%; Average loss: 3.5276
Iteration: 705; Percent complete: 17.6%; Average loss: 3.5041
Iteration: 706; Percent complete: 17.6%; Average loss: 3.4609
Iteration: 707; Percent complete: 17.7%; Average loss: 3.6640
Iteration: 708; Percent complete: 17.7%; Average loss: 3.7812
Iteration: 709; Percent complete: 17.7%; Average loss: 3.6380
Iteration: 710; Percent complete: 17.8%; Average loss: 3.5660
Iteration: 711; Percent complete: 17.8%; Average loss: 3.6742
Iteration: 712; Percent complete: 17.8%; Average loss: 3.2774
Iteration: 713; Percent complete: 17.8%; Average loss: 3.5669
Iteration: 714; Percent complete: 17.8%; Average loss: 3.7424
Iteration: 715; Percent complete: 17.9%; Average loss: 3.4483
Iteration: 716; Percent complete: 17.9%; Average loss: 3.5185
Iteration: 717; Percent complete: 17.9%; Average loss: 3.6966
Iteration: 718; Percent complete: 17.9%; Average loss: 3.3510
Iteration: 719; Percent complete: 18.0%; Average loss: 3.4939
Iteration: 720; Percent complete: 18.0%; Average loss: 3.8079
Iteration: 721; Percent complete: 18.0%; Average loss: 3.4837
Iteration: 722; Percent complete: 18.1%; Average loss: 3.8113
Iteration: 723; Percent complete: 18.1%; Average loss: 3.6610
Iteration: 724; Percent complete: 18.1%; Average loss: 3.7633
Iteration: 725; Percent complete: 18.1%; Average loss: 3.6938
Iteration: 726; Percent complete: 18.1%; Average loss: 3.7760
Iteration: 727; Percent complete: 18.2%; Average loss: 3.6966
Iteration: 728; Percent complete: 18.2%; Average loss: 3.8239
Iteration: 729; Percent complete: 18.2%; Average loss: 3.4281
Iteration: 730; Percent complete: 18.2%; Average loss: 3.5789
Iteration: 731; Percent complete: 18.3%; Average loss: 3.9749
Iteration: 732; Percent complete: 18.3%; Average loss: 3.6820
Iteration: 733; Percent complete: 18.3%; Average loss: 3.5889
Iteration: 734; Percent complete: 18.4%; Average loss: 3.7568
Iteration: 735; Percent complete: 18.4%; Average loss: 3.8443
Iteration: 736; Percent complete: 18.4%; Average loss: 3.3322
Iteration: 737; Percent complete: 18.4%; Average loss: 3.5718
Iteration: 738; Percent complete: 18.4%; Average loss: 3.6479
Iteration: 739; Percent complete: 18.5%; Average loss: 3.5974
Iteration: 740; Percent complete: 18.5%; Average loss: 3.4557
Iteration: 741; Percent complete: 18.5%; Average loss: 3.4746
Iteration: 742; Percent complete: 18.6%; Average loss: 3.4736
Iteration: 743; Percent complete: 18.6%; Average loss: 3.6697
Iteration: 744; Percent complete: 18.6%; Average loss: 3.9113
Iteration: 745; Percent complete: 18.6%; Average loss: 3.3120
Iteration: 746; Percent complete: 18.6%; Average loss: 3.4215
Iteration: 747; Percent complete: 18.7%; Average loss: 3.5362
Iteration: 748; Percent complete: 18.7%; Average loss: 3.4820
Iteration: 749; Percent complete: 18.7%; Average loss: 3.3916
Iteration: 750; Percent complete: 18.8%; Average loss: 3.5663
Iteration: 751; Percent complete: 18.8%; Average loss: 3.6184
Iteration: 752; Percent complete: 18.8%; Average loss: 3.4162
Iteration: 753; Percent complete: 18.8%; Average loss: 3.5808
Iteration: 754; Percent complete: 18.9%; Average loss: 3.4878
Iteration: 755; Percent complete: 18.9%; Average loss: 3.5750
Iteration: 756; Percent complete: 18.9%; Average loss: 3.5858
Iteration: 757; Percent complete: 18.9%; Average loss: 3.3503
Iteration: 758; Percent complete: 18.9%; Average loss: 3.5637
Iteration: 759; Percent complete: 19.0%; Average loss: 3.5012
Iteration: 760; Percent complete: 19.0%; Average loss: 3.4616
Iteration: 761; Percent complete: 19.0%; Average loss: 3.4184
Iteration: 762; Percent complete: 19.1%; Average loss: 3.3894
Iteration: 763; Percent complete: 19.1%; Average loss: 3.5544
Iteration: 764; Percent complete: 19.1%; Average loss: 3.6130
Iteration: 765; Percent complete: 19.1%; Average loss: 3.7408
Iteration: 766; Percent complete: 19.1%; Average loss: 3.4075
Iteration: 767; Percent complete: 19.2%; Average loss: 3.4580
Iteration: 768; Percent complete: 19.2%; Average loss: 3.3865
Iteration: 769; Percent complete: 19.2%; Average loss: 3.7218
Iteration: 770; Percent complete: 19.2%; Average loss: 3.3325
Iteration: 771; Percent complete: 19.3%; Average loss: 3.7409
Iteration: 772; Percent complete: 19.3%; Average loss: 3.8814
Iteration: 773; Percent complete: 19.3%; Average loss: 3.5635
Iteration: 774; Percent complete: 19.4%; Average loss: 3.5208
Iteration: 775; Percent complete: 19.4%; Average loss: 3.7011
Iteration: 776; Percent complete: 19.4%; Average loss: 3.5951
Iteration: 777; Percent complete: 19.4%; Average loss: 3.7284
Iteration: 778; Percent complete: 19.4%; Average loss: 3.4036
Iteration: 779; Percent complete: 19.5%; Average loss: 3.3531
Iteration: 780; Percent complete: 19.5%; Average loss: 3.3716
Iteration: 781; Percent complete: 19.5%; Average loss: 3.5423
Iteration: 782; Percent complete: 19.6%; Average loss: 3.7060
Iteration: 783; Percent complete: 19.6%; Average loss: 3.3231
Iteration: 784; Percent complete: 19.6%; Average loss: 3.7885
Iteration: 785; Percent complete: 19.6%; Average loss: 3.7657
Iteration: 786; Percent complete: 19.7%; Average loss: 3.6482
Iteration: 787; Percent complete: 19.7%; Average loss: 3.3603
Iteration: 788; Percent complete: 19.7%; Average loss: 3.7585
Iteration: 789; Percent complete: 19.7%; Average loss: 3.6518
Iteration: 790; Percent complete: 19.8%; Average loss: 3.6393
Iteration: 791; Percent complete: 19.8%; Average loss: 3.3641
Iteration: 792; Percent complete: 19.8%; Average loss: 3.5469
Iteration: 793; Percent complete: 19.8%; Average loss: 3.5708
Iteration: 794; Percent complete: 19.9%; Average loss: 3.5654
Iteration: 795; Percent complete: 19.9%; Average loss: 3.2238
Iteration: 796; Percent complete: 19.9%; Average loss: 3.4811
Iteration: 797; Percent complete: 19.9%; Average loss: 3.4907
Iteration: 798; Percent complete: 20.0%; Average loss: 3.6566
Iteration: 799; Percent complete: 20.0%; Average loss: 3.4671
Iteration: 800; Percent complete: 20.0%; Average loss: 3.7440
Iteration: 801; Percent complete: 20.0%; Average loss: 3.4700
Iteration: 802; Percent complete: 20.1%; Average loss: 3.3651
Iteration: 803; Percent complete: 20.1%; Average loss: 3.4488
Iteration: 804; Percent complete: 20.1%; Average loss: 3.2562
Iteration: 805; Percent complete: 20.1%; Average loss: 3.4984
Iteration: 806; Percent complete: 20.2%; Average loss: 3.5315
Iteration: 807; Percent complete: 20.2%; Average loss: 3.5134
Iteration: 808; Percent complete: 20.2%; Average loss: 3.4459
Iteration: 809; Percent complete: 20.2%; Average loss: 3.5119
Iteration: 810; Percent complete: 20.2%; Average loss: 3.6393
Iteration: 811; Percent complete: 20.3%; Average loss: 3.4319
Iteration: 812; Percent complete: 20.3%; Average loss: 3.3044
Iteration: 813; Percent complete: 20.3%; Average loss: 3.3421
Iteration: 814; Percent complete: 20.3%; Average loss: 3.7237
Iteration: 815; Percent complete: 20.4%; Average loss: 3.5608
Iteration: 816; Percent complete: 20.4%; Average loss: 3.6324
Iteration: 817; Percent complete: 20.4%; Average loss: 3.5276
Iteration: 818; Percent complete: 20.4%; Average loss: 3.6388
Iteration: 819; Percent complete: 20.5%; Average loss: 3.5814
Iteration: 820; Percent complete: 20.5%; Average loss: 3.4083
Iteration: 821; Percent complete: 20.5%; Average loss: 3.4901
Iteration: 822; Percent complete: 20.5%; Average loss: 3.4339
Iteration: 823; Percent complete: 20.6%; Average loss: 3.3572
Iteration: 824; Percent complete: 20.6%; Average loss: 3.4910
Iteration: 825; Percent complete: 20.6%; Average loss: 3.2438
Iteration: 826; Percent complete: 20.6%; Average loss: 3.4777
Iteration: 827; Percent complete: 20.7%; Average loss: 3.6497
Iteration: 828; Percent complete: 20.7%; Average loss: 3.5738
Iteration: 829; Percent complete: 20.7%; Average loss: 3.4322
Iteration: 830; Percent complete: 20.8%; Average loss: 3.4552
Iteration: 831; Percent complete: 20.8%; Average loss: 3.5915
Iteration: 832; Percent complete: 20.8%; Average loss: 3.5394
Iteration: 833; Percent complete: 20.8%; Average loss: 3.5336
Iteration: 834; Percent complete: 20.8%; Average loss: 3.7080
Iteration: 835; Percent complete: 20.9%; Average loss: 3.5068
Iteration: 836; Percent complete: 20.9%; Average loss: 3.4761
Iteration: 837; Percent complete: 20.9%; Average loss: 3.5458
Iteration: 838; Percent complete: 20.9%; Average loss: 3.5008
Iteration: 839; Percent complete: 21.0%; Average loss: 3.5185
Iteration: 840; Percent complete: 21.0%; Average loss: 3.6866
Iteration: 841; Percent complete: 21.0%; Average loss: 3.3319
Iteration: 842; Percent complete: 21.1%; Average loss: 3.6050
Iteration: 843; Percent complete: 21.1%; Average loss: 3.4917
Iteration: 844; Percent complete: 21.1%; Average loss: 3.6877
Iteration: 845; Percent complete: 21.1%; Average loss: 3.7344
Iteration: 846; Percent complete: 21.1%; Average loss: 3.4670
Iteration: 847; Percent complete: 21.2%; Average loss: 3.6714
Iteration: 848; Percent complete: 21.2%; Average loss: 3.6882
Iteration: 849; Percent complete: 21.2%; Average loss: 3.5510
Iteration: 850; Percent complete: 21.2%; Average loss: 3.6648
Iteration: 851; Percent complete: 21.3%; Average loss: 3.5291
Iteration: 852; Percent complete: 21.3%; Average loss: 3.1438
Iteration: 853; Percent complete: 21.3%; Average loss: 3.4342
Iteration: 854; Percent complete: 21.3%; Average loss: 3.3928
Iteration: 855; Percent complete: 21.4%; Average loss: 3.6647
Iteration: 856; Percent complete: 21.4%; Average loss: 3.5930
Iteration: 857; Percent complete: 21.4%; Average loss: 3.4091
Iteration: 858; Percent complete: 21.4%; Average loss: 3.7079
Iteration: 859; Percent complete: 21.5%; Average loss: 3.4535
Iteration: 860; Percent complete: 21.5%; Average loss: 3.4583
Iteration: 861; Percent complete: 21.5%; Average loss: 3.4391
Iteration: 862; Percent complete: 21.6%; Average loss: 3.5034
Iteration: 863; Percent complete: 21.6%; Average loss: 3.3331
Iteration: 864; Percent complete: 21.6%; Average loss: 3.3136
Iteration: 865; Percent complete: 21.6%; Average loss: 3.1383
Iteration: 866; Percent complete: 21.6%; Average loss: 3.4251
Iteration: 867; Percent complete: 21.7%; Average loss: 3.5096
Iteration: 868; Percent complete: 21.7%; Average loss: 3.6254
Iteration: 869; Percent complete: 21.7%; Average loss: 3.6617
Iteration: 870; Percent complete: 21.8%; Average loss: 3.6293
Iteration: 871; Percent complete: 21.8%; Average loss: 3.2384
Iteration: 872; Percent complete: 21.8%; Average loss: 3.4223
Iteration: 873; Percent complete: 21.8%; Average loss: 3.4944
Iteration: 874; Percent complete: 21.9%; Average loss: 3.5052
Iteration: 875; Percent complete: 21.9%; Average loss: 3.3166
Iteration: 876; Percent complete: 21.9%; Average loss: 3.6171
Iteration: 877; Percent complete: 21.9%; Average loss: 3.3861
Iteration: 878; Percent complete: 21.9%; Average loss: 3.7429
Iteration: 879; Percent complete: 22.0%; Average loss: 3.2814
Iteration: 880; Percent complete: 22.0%; Average loss: 3.6019
Iteration: 881; Percent complete: 22.0%; Average loss: 3.4726
Iteration: 882; Percent complete: 22.1%; Average loss: 3.6168
Iteration: 883; Percent complete: 22.1%; Average loss: 3.7220
Iteration: 884; Percent complete: 22.1%; Average loss: 3.5211
Iteration: 885; Percent complete: 22.1%; Average loss: 3.3467
Iteration: 886; Percent complete: 22.1%; Average loss: 3.6311
Iteration: 887; Percent complete: 22.2%; Average loss: 3.5039
Iteration: 888; Percent complete: 22.2%; Average loss: 3.3894
Iteration: 889; Percent complete: 22.2%; Average loss: 3.4104
Iteration: 890; Percent complete: 22.2%; Average loss: 3.4961
Iteration: 891; Percent complete: 22.3%; Average loss: 3.5186
Iteration: 892; Percent complete: 22.3%; Average loss: 3.5282
Iteration: 893; Percent complete: 22.3%; Average loss: 3.6540
Iteration: 894; Percent complete: 22.4%; Average loss: 3.5416
Iteration: 895; Percent complete: 22.4%; Average loss: 3.6564
Iteration: 896; Percent complete: 22.4%; Average loss: 3.3858
Iteration: 897; Percent complete: 22.4%; Average loss: 3.6001
Iteration: 898; Percent complete: 22.4%; Average loss: 3.5394
Iteration: 899; Percent complete: 22.5%; Average loss: 3.7671
Iteration: 900; Percent complete: 22.5%; Average loss: 3.3887
Iteration: 901; Percent complete: 22.5%; Average loss: 3.3430
Iteration: 902; Percent complete: 22.6%; Average loss: 3.4491
Iteration: 903; Percent complete: 22.6%; Average loss: 3.1959
Iteration: 904; Percent complete: 22.6%; Average loss: 3.5778
Iteration: 905; Percent complete: 22.6%; Average loss: 3.3521
Iteration: 906; Percent complete: 22.7%; Average loss: 3.3574
Iteration: 907; Percent complete: 22.7%; Average loss: 3.2397
Iteration: 908; Percent complete: 22.7%; Average loss: 3.6039
Iteration: 909; Percent complete: 22.7%; Average loss: 3.3838
Iteration: 910; Percent complete: 22.8%; Average loss: 3.6730
Iteration: 911; Percent complete: 22.8%; Average loss: 3.5385
Iteration: 912; Percent complete: 22.8%; Average loss: 3.6214
Iteration: 913; Percent complete: 22.8%; Average loss: 3.4901
Iteration: 914; Percent complete: 22.9%; Average loss: 3.4910
Iteration: 915; Percent complete: 22.9%; Average loss: 3.4899
Iteration: 916; Percent complete: 22.9%; Average loss: 3.3767
Iteration: 917; Percent complete: 22.9%; Average loss: 3.3090
Iteration: 918; Percent complete: 22.9%; Average loss: 3.3808
Iteration: 919; Percent complete: 23.0%; Average loss: 3.5429
Iteration: 920; Percent complete: 23.0%; Average loss: 3.6689
Iteration: 921; Percent complete: 23.0%; Average loss: 3.2063
Iteration: 922; Percent complete: 23.1%; Average loss: 3.5153
Iteration: 923; Percent complete: 23.1%; Average loss: 3.5208
Iteration: 924; Percent complete: 23.1%; Average loss: 3.1956
Iteration: 925; Percent complete: 23.1%; Average loss: 3.7227
Iteration: 926; Percent complete: 23.2%; Average loss: 3.6143
Iteration: 927; Percent complete: 23.2%; Average loss: 3.5289
Iteration: 928; Percent complete: 23.2%; Average loss: 3.3331
Iteration: 929; Percent complete: 23.2%; Average loss: 3.5102
Iteration: 930; Percent complete: 23.2%; Average loss: 3.5352
Iteration: 931; Percent complete: 23.3%; Average loss: 3.5918
Iteration: 932; Percent complete: 23.3%; Average loss: 3.4900
Iteration: 933; Percent complete: 23.3%; Average loss: 3.4136
Iteration: 934; Percent complete: 23.4%; Average loss: 3.4023
Iteration: 935; Percent complete: 23.4%; Average loss: 3.6388
Iteration: 936; Percent complete: 23.4%; Average loss: 3.3527
Iteration: 937; Percent complete: 23.4%; Average loss: 3.4732
Iteration: 938; Percent complete: 23.4%; Average loss: 3.4355
Iteration: 939; Percent complete: 23.5%; Average loss: 3.2789
Iteration: 940; Percent complete: 23.5%; Average loss: 3.5334
Iteration: 941; Percent complete: 23.5%; Average loss: 3.3786
Iteration: 942; Percent complete: 23.5%; Average loss: 3.5768
Iteration: 943; Percent complete: 23.6%; Average loss: 3.5030
Iteration: 944; Percent complete: 23.6%; Average loss: 3.3656
Iteration: 945; Percent complete: 23.6%; Average loss: 3.4632
Iteration: 946; Percent complete: 23.6%; Average loss: 3.5034
Iteration: 947; Percent complete: 23.7%; Average loss: 3.5127
Iteration: 948; Percent complete: 23.7%; Average loss: 3.4346
Iteration: 949; Percent complete: 23.7%; Average loss: 3.8302
Iteration: 950; Percent complete: 23.8%; Average loss: 3.5284
Iteration: 951; Percent complete: 23.8%; Average loss: 3.4615
Iteration: 952; Percent complete: 23.8%; Average loss: 3.5178
Iteration: 953; Percent complete: 23.8%; Average loss: 3.4584
Iteration: 954; Percent complete: 23.8%; Average loss: 3.3771
Iteration: 955; Percent complete: 23.9%; Average loss: 3.5177
Iteration: 956; Percent complete: 23.9%; Average loss: 3.4797
Iteration: 957; Percent complete: 23.9%; Average loss: 3.7820
Iteration: 958; Percent complete: 23.9%; Average loss: 3.5752
Iteration: 959; Percent complete: 24.0%; Average loss: 3.5888
Iteration: 960; Percent complete: 24.0%; Average loss: 3.4124
Iteration: 961; Percent complete: 24.0%; Average loss: 3.3932
Iteration: 962; Percent complete: 24.1%; Average loss: 3.5512
Iteration: 963; Percent complete: 24.1%; Average loss: 3.2528
Iteration: 964; Percent complete: 24.1%; Average loss: 3.4632
Iteration: 965; Percent complete: 24.1%; Average loss: 3.3705
Iteration: 966; Percent complete: 24.1%; Average loss: 3.2898
Iteration: 967; Percent complete: 24.2%; Average loss: 3.4526
Iteration: 968; Percent complete: 24.2%; Average loss: 3.4422
Iteration: 969; Percent complete: 24.2%; Average loss: 3.6612
Iteration: 970; Percent complete: 24.2%; Average loss: 3.4477
Iteration: 971; Percent complete: 24.3%; Average loss: 3.5500
Iteration: 972; Percent complete: 24.3%; Average loss: 3.5610
Iteration: 973; Percent complete: 24.3%; Average loss: 3.3778
Iteration: 974; Percent complete: 24.3%; Average loss: 3.5175
Iteration: 975; Percent complete: 24.4%; Average loss: 3.3166
Iteration: 976; Percent complete: 24.4%; Average loss: 3.4711
Iteration: 977; Percent complete: 24.4%; Average loss: 3.2917
Iteration: 978; Percent complete: 24.4%; Average loss: 3.7381
Iteration: 979; Percent complete: 24.5%; Average loss: 3.4652
Iteration: 980; Percent complete: 24.5%; Average loss: 3.5540
Iteration: 981; Percent complete: 24.5%; Average loss: 3.6482
Iteration: 982; Percent complete: 24.6%; Average loss: 3.3453
Iteration: 983; Percent complete: 24.6%; Average loss: 3.1273
Iteration: 984; Percent complete: 24.6%; Average loss: 3.4152
Iteration: 985; Percent complete: 24.6%; Average loss: 3.3603
Iteration: 986; Percent complete: 24.6%; Average loss: 3.3866
Iteration: 987; Percent complete: 24.7%; Average loss: 3.5234
Iteration: 988; Percent complete: 24.7%; Average loss: 3.4884
Iteration: 989; Percent complete: 24.7%; Average loss: 3.6097
Iteration: 990; Percent complete: 24.8%; Average loss: 3.3921
Iteration: 991; Percent complete: 24.8%; Average loss: 3.3624
Iteration: 992; Percent complete: 24.8%; Average loss: 3.2174
Iteration: 993; Percent complete: 24.8%; Average loss: 3.7253
Iteration: 994; Percent complete: 24.9%; Average loss: 3.5456
Iteration: 995; Percent complete: 24.9%; Average loss: 3.5093
Iteration: 996; Percent complete: 24.9%; Average loss: 3.5287
Iteration: 997; Percent complete: 24.9%; Average loss: 3.3532
Iteration: 998; Percent complete: 24.9%; Average loss: 3.3433
Iteration: 999; Percent complete: 25.0%; Average loss: 3.8132
Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6049
Iteration: 1001; Percent complete: 25.0%; Average loss: 3.2377
Iteration: 1002; Percent complete: 25.1%; Average loss: 3.3309
Iteration: 1003; Percent complete: 25.1%; Average loss: 3.4505
Iteration: 1004; Percent complete: 25.1%; Average loss: 3.4291
Iteration: 1005; Percent complete: 25.1%; Average loss: 3.3406
Iteration: 1006; Percent complete: 25.1%; Average loss: 3.2515
Iteration: 1007; Percent complete: 25.2%; Average loss: 3.3511
Iteration: 1008; Percent complete: 25.2%; Average loss: 3.6176
Iteration: 1009; Percent complete: 25.2%; Average loss: 3.3502
Iteration: 1010; Percent complete: 25.2%; Average loss: 3.3441
Iteration: 1011; Percent complete: 25.3%; Average loss: 3.5564
Iteration: 1012; Percent complete: 25.3%; Average loss: 3.4844
Iteration: 1013; Percent complete: 25.3%; Average loss: 3.5733
Iteration: 1014; Percent complete: 25.4%; Average loss: 3.3780
Iteration: 1015; Percent complete: 25.4%; Average loss: 3.5642
Iteration: 1016; Percent complete: 25.4%; Average loss: 3.3642
Iteration: 1017; Percent complete: 25.4%; Average loss: 3.5608
Iteration: 1018; Percent complete: 25.4%; Average loss: 3.3337
Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5158
Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5092
Iteration: 1021; Percent complete: 25.5%; Average loss: 3.3529
Iteration: 1022; Percent complete: 25.6%; Average loss: 3.2653
Iteration: 1023; Percent complete: 25.6%; Average loss: 3.5221
Iteration: 1024; Percent complete: 25.6%; Average loss: 3.3276
Iteration: 1025; Percent complete: 25.6%; Average loss: 3.5689
Iteration: 1026; Percent complete: 25.7%; Average loss: 3.2397
Iteration: 1027; Percent complete: 25.7%; Average loss: 3.3457
Iteration: 1028; Percent complete: 25.7%; Average loss: 3.2805
Iteration: 1029; Percent complete: 25.7%; Average loss: 3.2355
Iteration: 1030; Percent complete: 25.8%; Average loss: 3.6688
Iteration: 1031; Percent complete: 25.8%; Average loss: 3.3739
Iteration: 1032; Percent complete: 25.8%; Average loss: 3.2820
Iteration: 1033; Percent complete: 25.8%; Average loss: 3.1011
Iteration: 1034; Percent complete: 25.9%; Average loss: 3.2641
Iteration: 1035; Percent complete: 25.9%; Average loss: 3.3374
Iteration: 1036; Percent complete: 25.9%; Average loss: 3.2848
Iteration: 1037; Percent complete: 25.9%; Average loss: 3.7669
Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3439
Iteration: 1039; Percent complete: 26.0%; Average loss: 3.5405
Iteration: 1040; Percent complete: 26.0%; Average loss: 3.4347
Iteration: 1041; Percent complete: 26.0%; Average loss: 3.4530
Iteration: 1042; Percent complete: 26.1%; Average loss: 3.3259
Iteration: 1043; Percent complete: 26.1%; Average loss: 3.3375
Iteration: 1044; Percent complete: 26.1%; Average loss: 3.3377
Iteration: 1045; Percent complete: 26.1%; Average loss: 3.8312
Iteration: 1046; Percent complete: 26.2%; Average loss: 3.5522
Iteration: 1047; Percent complete: 26.2%; Average loss: 3.3713
Iteration: 1048; Percent complete: 26.2%; Average loss: 3.6288
Iteration: 1049; Percent complete: 26.2%; Average loss: 3.4556
Iteration: 1050; Percent complete: 26.2%; Average loss: 3.5935
Iteration: 1051; Percent complete: 26.3%; Average loss: 3.4662
Iteration: 1052; Percent complete: 26.3%; Average loss: 3.5939
Iteration: 1053; Percent complete: 26.3%; Average loss: 3.1667
Iteration: 1054; Percent complete: 26.4%; Average loss: 3.4483
Iteration: 1055; Percent complete: 26.4%; Average loss: 3.3770
Iteration: 1056; Percent complete: 26.4%; Average loss: 3.3404
Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4190
Iteration: 1058; Percent complete: 26.5%; Average loss: 3.5383
Iteration: 1059; Percent complete: 26.5%; Average loss: 3.6703
Iteration: 1060; Percent complete: 26.5%; Average loss: 3.7519
Iteration: 1061; Percent complete: 26.5%; Average loss: 3.3947
Iteration: 1062; Percent complete: 26.6%; Average loss: 3.5274
Iteration: 1063; Percent complete: 26.6%; Average loss: 3.4761
Iteration: 1064; Percent complete: 26.6%; Average loss: 3.3251
Iteration: 1065; Percent complete: 26.6%; Average loss: 3.4555
Iteration: 1066; Percent complete: 26.7%; Average loss: 3.4445
Iteration: 1067; Percent complete: 26.7%; Average loss: 3.4854
Iteration: 1068; Percent complete: 26.7%; Average loss: 3.2378
Iteration: 1069; Percent complete: 26.7%; Average loss: 3.1726
Iteration: 1070; Percent complete: 26.8%; Average loss: 3.1291
Iteration: 1071; Percent complete: 26.8%; Average loss: 3.4731
Iteration: 1072; Percent complete: 26.8%; Average loss: 3.4999
Iteration: 1073; Percent complete: 26.8%; Average loss: 3.3358
Iteration: 1074; Percent complete: 26.9%; Average loss: 3.0844
Iteration: 1075; Percent complete: 26.9%; Average loss: 3.2671
Iteration: 1076; Percent complete: 26.9%; Average loss: 3.2922
Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4439
Iteration: 1078; Percent complete: 27.0%; Average loss: 3.3207
Iteration: 1079; Percent complete: 27.0%; Average loss: 3.3227
Iteration: 1080; Percent complete: 27.0%; Average loss: 3.4749
Iteration: 1081; Percent complete: 27.0%; Average loss: 3.4828
Iteration: 1082; Percent complete: 27.1%; Average loss: 3.2052
Iteration: 1083; Percent complete: 27.1%; Average loss: 3.6793
Iteration: 1084; Percent complete: 27.1%; Average loss: 3.2326
Iteration: 1085; Percent complete: 27.1%; Average loss: 3.5649
Iteration: 1086; Percent complete: 27.2%; Average loss: 3.3562
Iteration: 1087; Percent complete: 27.2%; Average loss: 3.5591
Iteration: 1088; Percent complete: 27.2%; Average loss: 3.2790
Iteration: 1089; Percent complete: 27.2%; Average loss: 2.9346
Iteration: 1090; Percent complete: 27.3%; Average loss: 3.0973
Iteration: 1091; Percent complete: 27.3%; Average loss: 3.4547
Iteration: 1092; Percent complete: 27.3%; Average loss: 3.2541
Iteration: 1093; Percent complete: 27.3%; Average loss: 3.5703
Iteration: 1094; Percent complete: 27.4%; Average loss: 3.6954
Iteration: 1095; Percent complete: 27.4%; Average loss: 3.2843
Iteration: 1096; Percent complete: 27.4%; Average loss: 3.2895
Iteration: 1097; Percent complete: 27.4%; Average loss: 3.2615
Iteration: 1098; Percent complete: 27.5%; Average loss: 3.6121
Iteration: 1099; Percent complete: 27.5%; Average loss: 3.5832
Iteration: 1100; Percent complete: 27.5%; Average loss: 3.5831
Iteration: 1101; Percent complete: 27.5%; Average loss: 3.5763
Iteration: 1102; Percent complete: 27.6%; Average loss: 3.2913
Iteration: 1103; Percent complete: 27.6%; Average loss: 3.3169
Iteration: 1104; Percent complete: 27.6%; Average loss: 3.2771
Iteration: 1105; Percent complete: 27.6%; Average loss: 3.5499
Iteration: 1106; Percent complete: 27.7%; Average loss: 3.4617
Iteration: 1107; Percent complete: 27.7%; Average loss: 3.4246
Iteration: 1108; Percent complete: 27.7%; Average loss: 3.3612
Iteration: 1109; Percent complete: 27.7%; Average loss: 3.4773
Iteration: 1110; Percent complete: 27.8%; Average loss: 3.2100
Iteration: 1111; Percent complete: 27.8%; Average loss: 3.4858
Iteration: 1112; Percent complete: 27.8%; Average loss: 3.4524
Iteration: 1113; Percent complete: 27.8%; Average loss: 3.3005
Iteration: 1114; Percent complete: 27.9%; Average loss: 3.7459
Iteration: 1115; Percent complete: 27.9%; Average loss: 3.2463
Iteration: 1116; Percent complete: 27.9%; Average loss: 3.8085
Iteration: 1117; Percent complete: 27.9%; Average loss: 3.4433
Iteration: 1118; Percent complete: 28.0%; Average loss: 3.5369
Iteration: 1119; Percent complete: 28.0%; Average loss: 3.0773
Iteration: 1120; Percent complete: 28.0%; Average loss: 3.3679
Iteration: 1121; Percent complete: 28.0%; Average loss: 3.4815
Iteration: 1122; Percent complete: 28.1%; Average loss: 3.4697
Iteration: 1123; Percent complete: 28.1%; Average loss: 3.0437
Iteration: 1124; Percent complete: 28.1%; Average loss: 3.2746
Iteration: 1125; Percent complete: 28.1%; Average loss: 3.6587
Iteration: 1126; Percent complete: 28.1%; Average loss: 3.6509
Iteration: 1127; Percent complete: 28.2%; Average loss: 3.4486
Iteration: 1128; Percent complete: 28.2%; Average loss: 3.2793
Iteration: 1129; Percent complete: 28.2%; Average loss: 3.4900
Iteration: 1130; Percent complete: 28.2%; Average loss: 3.4665
Iteration: 1131; Percent complete: 28.3%; Average loss: 3.5799
Iteration: 1132; Percent complete: 28.3%; Average loss: 3.4119
Iteration: 1133; Percent complete: 28.3%; Average loss: 3.4672
Iteration: 1134; Percent complete: 28.3%; Average loss: 3.2536
Iteration: 1135; Percent complete: 28.4%; Average loss: 3.3453
Iteration: 1136; Percent complete: 28.4%; Average loss: 3.2728
Iteration: 1137; Percent complete: 28.4%; Average loss: 3.6913
Iteration: 1138; Percent complete: 28.4%; Average loss: 3.6033
Iteration: 1139; Percent complete: 28.5%; Average loss: 3.3174
Iteration: 1140; Percent complete: 28.5%; Average loss: 3.3042
Iteration: 1141; Percent complete: 28.5%; Average loss: 3.3199
Iteration: 1142; Percent complete: 28.5%; Average loss: 3.4815
Iteration: 1143; Percent complete: 28.6%; Average loss: 3.4857
Iteration: 1144; Percent complete: 28.6%; Average loss: 3.3204
Iteration: 1145; Percent complete: 28.6%; Average loss: 3.2626
Iteration: 1146; Percent complete: 28.6%; Average loss: 3.4749
Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5519
Iteration: 1148; Percent complete: 28.7%; Average loss: 3.1961
Iteration: 1149; Percent complete: 28.7%; Average loss: 3.1578
Iteration: 1150; Percent complete: 28.7%; Average loss: 3.5499
Iteration: 1151; Percent complete: 28.8%; Average loss: 3.3525
Iteration: 1152; Percent complete: 28.8%; Average loss: 3.4950
Iteration: 1153; Percent complete: 28.8%; Average loss: 3.4516
Iteration: 1154; Percent complete: 28.8%; Average loss: 3.5171
Iteration: 1155; Percent complete: 28.9%; Average loss: 3.2717
Iteration: 1156; Percent complete: 28.9%; Average loss: 3.4333
Iteration: 1157; Percent complete: 28.9%; Average loss: 3.4334
Iteration: 1158; Percent complete: 28.9%; Average loss: 3.6439
Iteration: 1159; Percent complete: 29.0%; Average loss: 3.4150
Iteration: 1160; Percent complete: 29.0%; Average loss: 3.1459
Iteration: 1161; Percent complete: 29.0%; Average loss: 3.4474
Iteration: 1162; Percent complete: 29.0%; Average loss: 3.2497
Iteration: 1163; Percent complete: 29.1%; Average loss: 3.2392
Iteration: 1164; Percent complete: 29.1%; Average loss: 3.6149
Iteration: 1165; Percent complete: 29.1%; Average loss: 3.2963
Iteration: 1166; Percent complete: 29.1%; Average loss: 3.7605
Iteration: 1167; Percent complete: 29.2%; Average loss: 3.4158
Iteration: 1168; Percent complete: 29.2%; Average loss: 3.0694
Iteration: 1169; Percent complete: 29.2%; Average loss: 3.6584
Iteration: 1170; Percent complete: 29.2%; Average loss: 3.5549
Iteration: 1171; Percent complete: 29.3%; Average loss: 3.5361
Iteration: 1172; Percent complete: 29.3%; Average loss: 3.3399
Iteration: 1173; Percent complete: 29.3%; Average loss: 3.1923
Iteration: 1174; Percent complete: 29.3%; Average loss: 3.3501
Iteration: 1175; Percent complete: 29.4%; Average loss: 3.7984
Iteration: 1176; Percent complete: 29.4%; Average loss: 3.4588
Iteration: 1177; Percent complete: 29.4%; Average loss: 3.3859
Iteration: 1178; Percent complete: 29.4%; Average loss: 3.5820
Iteration: 1179; Percent complete: 29.5%; Average loss: 3.3805
Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5714
Iteration: 1181; Percent complete: 29.5%; Average loss: 3.4809
Iteration: 1182; Percent complete: 29.5%; Average loss: 3.2970
Iteration: 1183; Percent complete: 29.6%; Average loss: 3.3038
Iteration: 1184; Percent complete: 29.6%; Average loss: 3.4060
Iteration: 1185; Percent complete: 29.6%; Average loss: 3.3831
Iteration: 1186; Percent complete: 29.6%; Average loss: 3.5131
Iteration: 1187; Percent complete: 29.7%; Average loss: 3.4140
Iteration: 1188; Percent complete: 29.7%; Average loss: 3.4067
Iteration: 1189; Percent complete: 29.7%; Average loss: 3.5303
Iteration: 1190; Percent complete: 29.8%; Average loss: 3.4257
Iteration: 1191; Percent complete: 29.8%; Average loss: 3.2921
Iteration: 1192; Percent complete: 29.8%; Average loss: 3.6455
Iteration: 1193; Percent complete: 29.8%; Average loss: 3.2887
Iteration: 1194; Percent complete: 29.8%; Average loss: 3.5003
Iteration: 1195; Percent complete: 29.9%; Average loss: 3.5490
Iteration: 1196; Percent complete: 29.9%; Average loss: 3.3464
Iteration: 1197; Percent complete: 29.9%; Average loss: 3.3596
Iteration: 1198; Percent complete: 29.9%; Average loss: 3.6644
Iteration: 1199; Percent complete: 30.0%; Average loss: 3.3420
Iteration: 1200; Percent complete: 30.0%; Average loss: 3.5720
Iteration: 1201; Percent complete: 30.0%; Average loss: 3.3972
Iteration: 1202; Percent complete: 30.0%; Average loss: 3.3529
Iteration: 1203; Percent complete: 30.1%; Average loss: 3.4675
Iteration: 1204; Percent complete: 30.1%; Average loss: 3.3804
Iteration: 1205; Percent complete: 30.1%; Average loss: 3.5284
Iteration: 1206; Percent complete: 30.1%; Average loss: 3.5150
Iteration: 1207; Percent complete: 30.2%; Average loss: 3.7949
Iteration: 1208; Percent complete: 30.2%; Average loss: 3.1980
Iteration: 1209; Percent complete: 30.2%; Average loss: 3.4438
Iteration: 1210; Percent complete: 30.2%; Average loss: 3.3559
Iteration: 1211; Percent complete: 30.3%; Average loss: 3.4772
Iteration: 1212; Percent complete: 30.3%; Average loss: 3.3172
Iteration: 1213; Percent complete: 30.3%; Average loss: 3.4039
Iteration: 1214; Percent complete: 30.3%; Average loss: 3.1742
Iteration: 1215; Percent complete: 30.4%; Average loss: 3.4577
Iteration: 1216; Percent complete: 30.4%; Average loss: 3.2600
Iteration: 1217; Percent complete: 30.4%; Average loss: 3.2860
Iteration: 1218; Percent complete: 30.4%; Average loss: 3.2614
Iteration: 1219; Percent complete: 30.5%; Average loss: 3.5675
Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5201
Iteration: 1221; Percent complete: 30.5%; Average loss: 3.3419
Iteration: 1222; Percent complete: 30.6%; Average loss: 3.2895
Iteration: 1223; Percent complete: 30.6%; Average loss: 3.4006
Iteration: 1224; Percent complete: 30.6%; Average loss: 3.1154
Iteration: 1225; Percent complete: 30.6%; Average loss: 3.6311
Iteration: 1226; Percent complete: 30.6%; Average loss: 3.5346
Iteration: 1227; Percent complete: 30.7%; Average loss: 3.5151
Iteration: 1228; Percent complete: 30.7%; Average loss: 3.3344
Iteration: 1229; Percent complete: 30.7%; Average loss: 3.3696
Iteration: 1230; Percent complete: 30.8%; Average loss: 3.2353
Iteration: 1231; Percent complete: 30.8%; Average loss: 3.2533
Iteration: 1232; Percent complete: 30.8%; Average loss: 3.2882
Iteration: 1233; Percent complete: 30.8%; Average loss: 3.3047
Iteration: 1234; Percent complete: 30.9%; Average loss: 3.2896
Iteration: 1235; Percent complete: 30.9%; Average loss: 3.0951
Iteration: 1236; Percent complete: 30.9%; Average loss: 3.1303
Iteration: 1237; Percent complete: 30.9%; Average loss: 3.3703
Iteration: 1238; Percent complete: 30.9%; Average loss: 3.0966
Iteration: 1239; Percent complete: 31.0%; Average loss: 3.2617
Iteration: 1240; Percent complete: 31.0%; Average loss: 3.2274
Iteration: 1241; Percent complete: 31.0%; Average loss: 3.1360
Iteration: 1242; Percent complete: 31.1%; Average loss: 3.4739
Iteration: 1243; Percent complete: 31.1%; Average loss: 3.0550
Iteration: 1244; Percent complete: 31.1%; Average loss: 3.5630
Iteration: 1245; Percent complete: 31.1%; Average loss: 3.5572
Iteration: 1246; Percent complete: 31.1%; Average loss: 3.4113
Iteration: 1247; Percent complete: 31.2%; Average loss: 3.5262
Iteration: 1248; Percent complete: 31.2%; Average loss: 3.5522
Iteration: 1249; Percent complete: 31.2%; Average loss: 3.3500
Iteration: 1250; Percent complete: 31.2%; Average loss: 3.4249
Iteration: 1251; Percent complete: 31.3%; Average loss: 3.7537
Iteration: 1252; Percent complete: 31.3%; Average loss: 3.2723
Iteration: 1253; Percent complete: 31.3%; Average loss: 3.3995
Iteration: 1254; Percent complete: 31.4%; Average loss: 3.4235
Iteration: 1255; Percent complete: 31.4%; Average loss: 3.1935
Iteration: 1256; Percent complete: 31.4%; Average loss: 3.1916
Iteration: 1257; Percent complete: 31.4%; Average loss: 3.6881
Iteration: 1258; Percent complete: 31.4%; Average loss: 3.6187
Iteration: 1259; Percent complete: 31.5%; Average loss: 3.1867
Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2273
Iteration: 1261; Percent complete: 31.5%; Average loss: 3.4657
Iteration: 1262; Percent complete: 31.6%; Average loss: 3.5926
Iteration: 1263; Percent complete: 31.6%; Average loss: 3.3638
Iteration: 1264; Percent complete: 31.6%; Average loss: 3.4849
Iteration: 1265; Percent complete: 31.6%; Average loss: 3.4071
Iteration: 1266; Percent complete: 31.6%; Average loss: 3.3029
Iteration: 1267; Percent complete: 31.7%; Average loss: 3.6416
Iteration: 1268; Percent complete: 31.7%; Average loss: 3.3462
Iteration: 1269; Percent complete: 31.7%; Average loss: 3.3790
Iteration: 1270; Percent complete: 31.8%; Average loss: 3.3173
Iteration: 1271; Percent complete: 31.8%; Average loss: 3.3433
Iteration: 1272; Percent complete: 31.8%; Average loss: 3.1032
Iteration: 1273; Percent complete: 31.8%; Average loss: 3.2069
Iteration: 1274; Percent complete: 31.9%; Average loss: 3.3897
Iteration: 1275; Percent complete: 31.9%; Average loss: 3.3528
Iteration: 1276; Percent complete: 31.9%; Average loss: 3.1592
Iteration: 1277; Percent complete: 31.9%; Average loss: 3.4107
Iteration: 1278; Percent complete: 31.9%; Average loss: 3.3345
Iteration: 1279; Percent complete: 32.0%; Average loss: 3.4849
Iteration: 1280; Percent complete: 32.0%; Average loss: 3.3389
Iteration: 1281; Percent complete: 32.0%; Average loss: 3.2329
Iteration: 1282; Percent complete: 32.0%; Average loss: 3.2768
Iteration: 1283; Percent complete: 32.1%; Average loss: 3.1950
Iteration: 1284; Percent complete: 32.1%; Average loss: 3.3231
Iteration: 1285; Percent complete: 32.1%; Average loss: 3.3642
Iteration: 1286; Percent complete: 32.1%; Average loss: 3.5125
Iteration: 1287; Percent complete: 32.2%; Average loss: 3.3224
Iteration: 1288; Percent complete: 32.2%; Average loss: 3.2997
Iteration: 1289; Percent complete: 32.2%; Average loss: 3.2938
Iteration: 1290; Percent complete: 32.2%; Average loss: 3.0756
Iteration: 1291; Percent complete: 32.3%; Average loss: 3.2232
Iteration: 1292; Percent complete: 32.3%; Average loss: 3.3674
Iteration: 1293; Percent complete: 32.3%; Average loss: 3.4854
Iteration: 1294; Percent complete: 32.4%; Average loss: 3.3969
Iteration: 1295; Percent complete: 32.4%; Average loss: 3.3091
Iteration: 1296; Percent complete: 32.4%; Average loss: 3.3799
Iteration: 1297; Percent complete: 32.4%; Average loss: 3.6428
Iteration: 1298; Percent complete: 32.5%; Average loss: 3.3583
Iteration: 1299; Percent complete: 32.5%; Average loss: 3.4380
Iteration: 1300; Percent complete: 32.5%; Average loss: 3.2401
Iteration: 1301; Percent complete: 32.5%; Average loss: 3.3525
Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2325
Iteration: 1303; Percent complete: 32.6%; Average loss: 3.1527
Iteration: 1304; Percent complete: 32.6%; Average loss: 3.5549
Iteration: 1305; Percent complete: 32.6%; Average loss: 3.3103
Iteration: 1306; Percent complete: 32.6%; Average loss: 3.2956
Iteration: 1307; Percent complete: 32.7%; Average loss: 3.3927
Iteration: 1308; Percent complete: 32.7%; Average loss: 3.6223
Iteration: 1309; Percent complete: 32.7%; Average loss: 2.9849
Iteration: 1310; Percent complete: 32.8%; Average loss: 3.4404
Iteration: 1311; Percent complete: 32.8%; Average loss: 3.3799
Iteration: 1312; Percent complete: 32.8%; Average loss: 3.5093
Iteration: 1313; Percent complete: 32.8%; Average loss: 3.3938
Iteration: 1314; Percent complete: 32.9%; Average loss: 3.6274
Iteration: 1315; Percent complete: 32.9%; Average loss: 3.2583
Iteration: 1316; Percent complete: 32.9%; Average loss: 3.4549
Iteration: 1317; Percent complete: 32.9%; Average loss: 3.3847
Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3042
Iteration: 1319; Percent complete: 33.0%; Average loss: 3.1182
Iteration: 1320; Percent complete: 33.0%; Average loss: 3.4136
Iteration: 1321; Percent complete: 33.0%; Average loss: 3.5304
Iteration: 1322; Percent complete: 33.1%; Average loss: 3.4602
Iteration: 1323; Percent complete: 33.1%; Average loss: 3.2910
Iteration: 1324; Percent complete: 33.1%; Average loss: 3.5633
Iteration: 1325; Percent complete: 33.1%; Average loss: 3.3549
Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3147
Iteration: 1327; Percent complete: 33.2%; Average loss: 3.3866
Iteration: 1328; Percent complete: 33.2%; Average loss: 3.4622
Iteration: 1329; Percent complete: 33.2%; Average loss: 3.4431
Iteration: 1330; Percent complete: 33.2%; Average loss: 3.2541
Iteration: 1331; Percent complete: 33.3%; Average loss: 3.2024
Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3437
Iteration: 1333; Percent complete: 33.3%; Average loss: 3.1998
Iteration: 1334; Percent complete: 33.4%; Average loss: 3.2958
Iteration: 1335; Percent complete: 33.4%; Average loss: 3.3266
Iteration: 1336; Percent complete: 33.4%; Average loss: 3.1899
Iteration: 1337; Percent complete: 33.4%; Average loss: 3.2250
Iteration: 1338; Percent complete: 33.5%; Average loss: 3.3179
Iteration: 1339; Percent complete: 33.5%; Average loss: 3.4458
Iteration: 1340; Percent complete: 33.5%; Average loss: 3.1712
Iteration: 1341; Percent complete: 33.5%; Average loss: 3.3221
Iteration: 1342; Percent complete: 33.6%; Average loss: 3.1865
Iteration: 1343; Percent complete: 33.6%; Average loss: 3.4374
Iteration: 1344; Percent complete: 33.6%; Average loss: 3.1605
Iteration: 1345; Percent complete: 33.6%; Average loss: 3.3531
Iteration: 1346; Percent complete: 33.7%; Average loss: 3.4417
Iteration: 1347; Percent complete: 33.7%; Average loss: 3.5966
Iteration: 1348; Percent complete: 33.7%; Average loss: 3.3354
Iteration: 1349; Percent complete: 33.7%; Average loss: 3.4040
Iteration: 1350; Percent complete: 33.8%; Average loss: 3.5007
Iteration: 1351; Percent complete: 33.8%; Average loss: 3.0964
Iteration: 1352; Percent complete: 33.8%; Average loss: 3.2367
Iteration: 1353; Percent complete: 33.8%; Average loss: 3.2806
Iteration: 1354; Percent complete: 33.9%; Average loss: 3.5054
Iteration: 1355; Percent complete: 33.9%; Average loss: 3.2052
Iteration: 1356; Percent complete: 33.9%; Average loss: 3.5052
Iteration: 1357; Percent complete: 33.9%; Average loss: 3.1777
Iteration: 1358; Percent complete: 34.0%; Average loss: 3.2544
Iteration: 1359; Percent complete: 34.0%; Average loss: 3.2932
Iteration: 1360; Percent complete: 34.0%; Average loss: 3.2529
Iteration: 1361; Percent complete: 34.0%; Average loss: 3.3808
Iteration: 1362; Percent complete: 34.1%; Average loss: 3.4244
Iteration: 1363; Percent complete: 34.1%; Average loss: 3.0862
Iteration: 1364; Percent complete: 34.1%; Average loss: 3.3442
Iteration: 1365; Percent complete: 34.1%; Average loss: 3.4653
Iteration: 1366; Percent complete: 34.2%; Average loss: 3.3395
Iteration: 1367; Percent complete: 34.2%; Average loss: 3.1706
Iteration: 1368; Percent complete: 34.2%; Average loss: 3.3763
Iteration: 1369; Percent complete: 34.2%; Average loss: 3.1672
Iteration: 1370; Percent complete: 34.2%; Average loss: 3.3566
Iteration: 1371; Percent complete: 34.3%; Average loss: 3.5761
Iteration: 1372; Percent complete: 34.3%; Average loss: 3.5473
Iteration: 1373; Percent complete: 34.3%; Average loss: 3.3975
Iteration: 1374; Percent complete: 34.4%; Average loss: 3.3283
Iteration: 1375; Percent complete: 34.4%; Average loss: 3.3654
Iteration: 1376; Percent complete: 34.4%; Average loss: 3.5900
Iteration: 1377; Percent complete: 34.4%; Average loss: 3.4412
Iteration: 1378; Percent complete: 34.4%; Average loss: 3.3947
Iteration: 1379; Percent complete: 34.5%; Average loss: 3.4768
Iteration: 1380; Percent complete: 34.5%; Average loss: 3.0286
Iteration: 1381; Percent complete: 34.5%; Average loss: 3.1254
Iteration: 1382; Percent complete: 34.5%; Average loss: 3.5535
Iteration: 1383; Percent complete: 34.6%; Average loss: 3.3379
Iteration: 1384; Percent complete: 34.6%; Average loss: 3.3012
Iteration: 1385; Percent complete: 34.6%; Average loss: 3.2055
Iteration: 1386; Percent complete: 34.6%; Average loss: 3.1916
Iteration: 1387; Percent complete: 34.7%; Average loss: 3.3110
Iteration: 1388; Percent complete: 34.7%; Average loss: 3.3308
Iteration: 1389; Percent complete: 34.7%; Average loss: 3.2040
Iteration: 1390; Percent complete: 34.8%; Average loss: 3.3232
Iteration: 1391; Percent complete: 34.8%; Average loss: 3.4744
Iteration: 1392; Percent complete: 34.8%; Average loss: 3.4353
Iteration: 1393; Percent complete: 34.8%; Average loss: 3.3055
Iteration: 1394; Percent complete: 34.8%; Average loss: 3.5701
Iteration: 1395; Percent complete: 34.9%; Average loss: 3.2382
Iteration: 1396; Percent complete: 34.9%; Average loss: 3.5682
Iteration: 1397; Percent complete: 34.9%; Average loss: 3.0406
Iteration: 1398; Percent complete: 34.9%; Average loss: 3.5395
Iteration: 1399; Percent complete: 35.0%; Average loss: 3.2916
Iteration: 1400; Percent complete: 35.0%; Average loss: 3.1085
Iteration: 1401; Percent complete: 35.0%; Average loss: 3.2135
Iteration: 1402; Percent complete: 35.0%; Average loss: 3.2470
Iteration: 1403; Percent complete: 35.1%; Average loss: 3.2596
Iteration: 1404; Percent complete: 35.1%; Average loss: 3.2677
Iteration: 1405; Percent complete: 35.1%; Average loss: 3.3903
Iteration: 1406; Percent complete: 35.1%; Average loss: 3.7058
Iteration: 1407; Percent complete: 35.2%; Average loss: 3.1921
Iteration: 1408; Percent complete: 35.2%; Average loss: 3.5110
Iteration: 1409; Percent complete: 35.2%; Average loss: 2.9308
Iteration: 1410; Percent complete: 35.2%; Average loss: 3.2901
Iteration: 1411; Percent complete: 35.3%; Average loss: 3.4329
Iteration: 1412; Percent complete: 35.3%; Average loss: 3.3591
Iteration: 1413; Percent complete: 35.3%; Average loss: 3.3321
Iteration: 1414; Percent complete: 35.4%; Average loss: 3.0782
Iteration: 1415; Percent complete: 35.4%; Average loss: 3.4266
Iteration: 1416; Percent complete: 35.4%; Average loss: 3.1449
Iteration: 1417; Percent complete: 35.4%; Average loss: 3.4942
Iteration: 1418; Percent complete: 35.4%; Average loss: 3.4810
Iteration: 1419; Percent complete: 35.5%; Average loss: 3.1128
Iteration: 1420; Percent complete: 35.5%; Average loss: 3.0795
Iteration: 1421; Percent complete: 35.5%; Average loss: 3.2305
Iteration: 1422; Percent complete: 35.5%; Average loss: 3.4405
Iteration: 1423; Percent complete: 35.6%; Average loss: 3.2534
Iteration: 1424; Percent complete: 35.6%; Average loss: 3.4632
Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2383
Iteration: 1426; Percent complete: 35.6%; Average loss: 3.2550
Iteration: 1427; Percent complete: 35.7%; Average loss: 3.2120
Iteration: 1428; Percent complete: 35.7%; Average loss: 3.1231
Iteration: 1429; Percent complete: 35.7%; Average loss: 3.4463
Iteration: 1430; Percent complete: 35.8%; Average loss: 3.2587
Iteration: 1431; Percent complete: 35.8%; Average loss: 3.2901
Iteration: 1432; Percent complete: 35.8%; Average loss: 3.2019
Iteration: 1433; Percent complete: 35.8%; Average loss: 3.4212
Iteration: 1434; Percent complete: 35.9%; Average loss: 3.5164
Iteration: 1435; Percent complete: 35.9%; Average loss: 3.3305
Iteration: 1436; Percent complete: 35.9%; Average loss: 3.4866
Iteration: 1437; Percent complete: 35.9%; Average loss: 3.2268
Iteration: 1438; Percent complete: 35.9%; Average loss: 3.4633
Iteration: 1439; Percent complete: 36.0%; Average loss: 3.4721
Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2312
Iteration: 1441; Percent complete: 36.0%; Average loss: 3.1839
Iteration: 1442; Percent complete: 36.0%; Average loss: 3.2987
Iteration: 1443; Percent complete: 36.1%; Average loss: 3.5188
Iteration: 1444; Percent complete: 36.1%; Average loss: 3.2911
Iteration: 1445; Percent complete: 36.1%; Average loss: 3.2572
Iteration: 1446; Percent complete: 36.1%; Average loss: 3.5334
Iteration: 1447; Percent complete: 36.2%; Average loss: 3.2204
Iteration: 1448; Percent complete: 36.2%; Average loss: 3.2165
Iteration: 1449; Percent complete: 36.2%; Average loss: 3.3607
Iteration: 1450; Percent complete: 36.2%; Average loss: 3.4091
Iteration: 1451; Percent complete: 36.3%; Average loss: 3.2985
Iteration: 1452; Percent complete: 36.3%; Average loss: 3.2800
Iteration: 1453; Percent complete: 36.3%; Average loss: 3.3136
Iteration: 1454; Percent complete: 36.4%; Average loss: 3.5674
Iteration: 1455; Percent complete: 36.4%; Average loss: 3.2177
Iteration: 1456; Percent complete: 36.4%; Average loss: 3.4292
Iteration: 1457; Percent complete: 36.4%; Average loss: 3.5397
Iteration: 1458; Percent complete: 36.4%; Average loss: 3.2271
Iteration: 1459; Percent complete: 36.5%; Average loss: 3.1182
Iteration: 1460; Percent complete: 36.5%; Average loss: 3.2924
Iteration: 1461; Percent complete: 36.5%; Average loss: 3.3519
Iteration: 1462; Percent complete: 36.5%; Average loss: 3.1869
Iteration: 1463; Percent complete: 36.6%; Average loss: 3.5602
Iteration: 1464; Percent complete: 36.6%; Average loss: 3.3615
Iteration: 1465; Percent complete: 36.6%; Average loss: 3.1926
Iteration: 1466; Percent complete: 36.6%; Average loss: 3.2203
Iteration: 1467; Percent complete: 36.7%; Average loss: 3.2874
Iteration: 1468; Percent complete: 36.7%; Average loss: 3.2606
Iteration: 1469; Percent complete: 36.7%; Average loss: 3.3230
Iteration: 1470; Percent complete: 36.8%; Average loss: 3.2530
Iteration: 1471; Percent complete: 36.8%; Average loss: 3.2038
Iteration: 1472; Percent complete: 36.8%; Average loss: 3.2775
Iteration: 1473; Percent complete: 36.8%; Average loss: 3.3286
Iteration: 1474; Percent complete: 36.9%; Average loss: 3.1765
Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3623
Iteration: 1476; Percent complete: 36.9%; Average loss: 3.1800
Iteration: 1477; Percent complete: 36.9%; Average loss: 3.2065
Iteration: 1478; Percent complete: 37.0%; Average loss: 3.2364
Iteration: 1479; Percent complete: 37.0%; Average loss: 3.3458
Iteration: 1480; Percent complete: 37.0%; Average loss: 3.4044
Iteration: 1481; Percent complete: 37.0%; Average loss: 3.2730
Iteration: 1482; Percent complete: 37.0%; Average loss: 3.0207
Iteration: 1483; Percent complete: 37.1%; Average loss: 3.2196
Iteration: 1484; Percent complete: 37.1%; Average loss: 3.4040
Iteration: 1485; Percent complete: 37.1%; Average loss: 3.0616
Iteration: 1486; Percent complete: 37.1%; Average loss: 3.4709
Iteration: 1487; Percent complete: 37.2%; Average loss: 3.2485
Iteration: 1488; Percent complete: 37.2%; Average loss: 3.2902
Iteration: 1489; Percent complete: 37.2%; Average loss: 3.2458
Iteration: 1490; Percent complete: 37.2%; Average loss: 3.3783
Iteration: 1491; Percent complete: 37.3%; Average loss: 3.5892
Iteration: 1492; Percent complete: 37.3%; Average loss: 2.9111
Iteration: 1493; Percent complete: 37.3%; Average loss: 3.3173
Iteration: 1494; Percent complete: 37.4%; Average loss: 3.0468
Iteration: 1495; Percent complete: 37.4%; Average loss: 3.2962
Iteration: 1496; Percent complete: 37.4%; Average loss: 3.6843
Iteration: 1497; Percent complete: 37.4%; Average loss: 3.2293
Iteration: 1498; Percent complete: 37.5%; Average loss: 3.2589
Iteration: 1499; Percent complete: 37.5%; Average loss: 3.1875
Iteration: 1500; Percent complete: 37.5%; Average loss: 2.9463
Iteration: 1501; Percent complete: 37.5%; Average loss: 3.3571
Iteration: 1502; Percent complete: 37.5%; Average loss: 3.4746
Iteration: 1503; Percent complete: 37.6%; Average loss: 3.5778
Iteration: 1504; Percent complete: 37.6%; Average loss: 3.3613
Iteration: 1505; Percent complete: 37.6%; Average loss: 3.2292
Iteration: 1506; Percent complete: 37.6%; Average loss: 3.1766
Iteration: 1507; Percent complete: 37.7%; Average loss: 3.3404
Iteration: 1508; Percent complete: 37.7%; Average loss: 3.3053
Iteration: 1509; Percent complete: 37.7%; Average loss: 3.2334
Iteration: 1510; Percent complete: 37.8%; Average loss: 3.3677
Iteration: 1511; Percent complete: 37.8%; Average loss: 3.2806
Iteration: 1512; Percent complete: 37.8%; Average loss: 3.3093
Iteration: 1513; Percent complete: 37.8%; Average loss: 3.3513
Iteration: 1514; Percent complete: 37.9%; Average loss: 3.3363
Iteration: 1515; Percent complete: 37.9%; Average loss: 3.1692
Iteration: 1516; Percent complete: 37.9%; Average loss: 3.4816
Iteration: 1517; Percent complete: 37.9%; Average loss: 3.1682
Iteration: 1518; Percent complete: 38.0%; Average loss: 2.8673
Iteration: 1519; Percent complete: 38.0%; Average loss: 3.0814
Iteration: 1520; Percent complete: 38.0%; Average loss: 3.4328
Iteration: 1521; Percent complete: 38.0%; Average loss: 3.2562
Iteration: 1522; Percent complete: 38.0%; Average loss: 3.2154
Iteration: 1523; Percent complete: 38.1%; Average loss: 3.3848
Iteration: 1524; Percent complete: 38.1%; Average loss: 3.3809
Iteration: 1525; Percent complete: 38.1%; Average loss: 3.3576
Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1964
Iteration: 1527; Percent complete: 38.2%; Average loss: 3.3314
Iteration: 1528; Percent complete: 38.2%; Average loss: 2.9808
Iteration: 1529; Percent complete: 38.2%; Average loss: 3.4461
Iteration: 1530; Percent complete: 38.2%; Average loss: 3.0708
Iteration: 1531; Percent complete: 38.3%; Average loss: 3.5730
Iteration: 1532; Percent complete: 38.3%; Average loss: 3.3581
Iteration: 1533; Percent complete: 38.3%; Average loss: 3.0210
Iteration: 1534; Percent complete: 38.4%; Average loss: 3.0789
Iteration: 1535; Percent complete: 38.4%; Average loss: 3.2262
Iteration: 1536; Percent complete: 38.4%; Average loss: 3.5209
Iteration: 1537; Percent complete: 38.4%; Average loss: 3.1507
Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2448
Iteration: 1539; Percent complete: 38.5%; Average loss: 3.4706
Iteration: 1540; Percent complete: 38.5%; Average loss: 3.1771
Iteration: 1541; Percent complete: 38.5%; Average loss: 3.2101
Iteration: 1542; Percent complete: 38.6%; Average loss: 3.0685
Iteration: 1543; Percent complete: 38.6%; Average loss: 3.2323
Iteration: 1544; Percent complete: 38.6%; Average loss: 2.9935
Iteration: 1545; Percent complete: 38.6%; Average loss: 3.2649
Iteration: 1546; Percent complete: 38.6%; Average loss: 3.2217
Iteration: 1547; Percent complete: 38.7%; Average loss: 3.2233
Iteration: 1548; Percent complete: 38.7%; Average loss: 3.2174
Iteration: 1549; Percent complete: 38.7%; Average loss: 3.1303
Iteration: 1550; Percent complete: 38.8%; Average loss: 3.2679
Iteration: 1551; Percent complete: 38.8%; Average loss: 3.3267
Iteration: 1552; Percent complete: 38.8%; Average loss: 3.4150
Iteration: 1553; Percent complete: 38.8%; Average loss: 3.1543
Iteration: 1554; Percent complete: 38.9%; Average loss: 3.1213
Iteration: 1555; Percent complete: 38.9%; Average loss: 3.1995
Iteration: 1556; Percent complete: 38.9%; Average loss: 3.2431
Iteration: 1557; Percent complete: 38.9%; Average loss: 3.2258
Iteration: 1558; Percent complete: 39.0%; Average loss: 3.2449
Iteration: 1559; Percent complete: 39.0%; Average loss: 3.5509
Iteration: 1560; Percent complete: 39.0%; Average loss: 3.2698
Iteration: 1561; Percent complete: 39.0%; Average loss: 3.1473
Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4896
Iteration: 1563; Percent complete: 39.1%; Average loss: 3.4117
Iteration: 1564; Percent complete: 39.1%; Average loss: 2.9899
Iteration: 1565; Percent complete: 39.1%; Average loss: 3.5239
Iteration: 1566; Percent complete: 39.1%; Average loss: 3.0869
Iteration: 1567; Percent complete: 39.2%; Average loss: 3.1429
Iteration: 1568; Percent complete: 39.2%; Average loss: 3.3754
Iteration: 1569; Percent complete: 39.2%; Average loss: 3.3662
Iteration: 1570; Percent complete: 39.2%; Average loss: 3.3077
Iteration: 1571; Percent complete: 39.3%; Average loss: 3.4514
Iteration: 1572; Percent complete: 39.3%; Average loss: 3.2808
Iteration: 1573; Percent complete: 39.3%; Average loss: 3.1932
Iteration: 1574; Percent complete: 39.4%; Average loss: 3.3764
Iteration: 1575; Percent complete: 39.4%; Average loss: 3.1908
Iteration: 1576; Percent complete: 39.4%; Average loss: 3.3395
Iteration: 1577; Percent complete: 39.4%; Average loss: 3.4394
Iteration: 1578; Percent complete: 39.5%; Average loss: 3.2596
Iteration: 1579; Percent complete: 39.5%; Average loss: 3.2489
Iteration: 1580; Percent complete: 39.5%; Average loss: 3.2388
Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4149
Iteration: 1582; Percent complete: 39.6%; Average loss: 3.2926
Iteration: 1583; Percent complete: 39.6%; Average loss: 3.1972
Iteration: 1584; Percent complete: 39.6%; Average loss: 3.3746
Iteration: 1585; Percent complete: 39.6%; Average loss: 3.2610
Iteration: 1586; Percent complete: 39.6%; Average loss: 3.4869
Iteration: 1587; Percent complete: 39.7%; Average loss: 3.0921
Iteration: 1588; Percent complete: 39.7%; Average loss: 3.2655
Iteration: 1589; Percent complete: 39.7%; Average loss: 3.1630
Iteration: 1590; Percent complete: 39.8%; Average loss: 3.3048
Iteration: 1591; Percent complete: 39.8%; Average loss: 3.1648
Iteration: 1592; Percent complete: 39.8%; Average loss: 3.2436
Iteration: 1593; Percent complete: 39.8%; Average loss: 3.2581
Iteration: 1594; Percent complete: 39.9%; Average loss: 3.1924
Iteration: 1595; Percent complete: 39.9%; Average loss: 3.1854
Iteration: 1596; Percent complete: 39.9%; Average loss: 3.1439
Iteration: 1597; Percent complete: 39.9%; Average loss: 3.3676
Iteration: 1598; Percent complete: 40.0%; Average loss: 3.2461
Iteration: 1599; Percent complete: 40.0%; Average loss: 3.1873
Iteration: 1600; Percent complete: 40.0%; Average loss: 3.2748
Iteration: 1601; Percent complete: 40.0%; Average loss: 3.1414
Iteration: 1602; Percent complete: 40.1%; Average loss: 3.2632
Iteration: 1603; Percent complete: 40.1%; Average loss: 3.1891
Iteration: 1604; Percent complete: 40.1%; Average loss: 3.3780
Iteration: 1605; Percent complete: 40.1%; Average loss: 3.3575
Iteration: 1606; Percent complete: 40.2%; Average loss: 3.5319
Iteration: 1607; Percent complete: 40.2%; Average loss: 3.2531
Iteration: 1608; Percent complete: 40.2%; Average loss: 2.9326
Iteration: 1609; Percent complete: 40.2%; Average loss: 3.2006
Iteration: 1610; Percent complete: 40.2%; Average loss: 3.5137
Iteration: 1611; Percent complete: 40.3%; Average loss: 3.2338
Iteration: 1612; Percent complete: 40.3%; Average loss: 3.2409
Iteration: 1613; Percent complete: 40.3%; Average loss: 3.3707
Iteration: 1614; Percent complete: 40.4%; Average loss: 3.3359
Iteration: 1615; Percent complete: 40.4%; Average loss: 3.2865
Iteration: 1616; Percent complete: 40.4%; Average loss: 3.2614
Iteration: 1617; Percent complete: 40.4%; Average loss: 3.1038
Iteration: 1618; Percent complete: 40.5%; Average loss: 3.1271
Iteration: 1619; Percent complete: 40.5%; Average loss: 3.2394
Iteration: 1620; Percent complete: 40.5%; Average loss: 3.3251
Iteration: 1621; Percent complete: 40.5%; Average loss: 3.3626
Iteration: 1622; Percent complete: 40.6%; Average loss: 3.1165
Iteration: 1623; Percent complete: 40.6%; Average loss: 3.4004
Iteration: 1624; Percent complete: 40.6%; Average loss: 3.2377
Iteration: 1625; Percent complete: 40.6%; Average loss: 3.3199
Iteration: 1626; Percent complete: 40.6%; Average loss: 3.0375
Iteration: 1627; Percent complete: 40.7%; Average loss: 3.4663
Iteration: 1628; Percent complete: 40.7%; Average loss: 3.3415
Iteration: 1629; Percent complete: 40.7%; Average loss: 3.4357
Iteration: 1630; Percent complete: 40.8%; Average loss: 3.1645
Iteration: 1631; Percent complete: 40.8%; Average loss: 3.2705
Iteration: 1632; Percent complete: 40.8%; Average loss: 3.1870
Iteration: 1633; Percent complete: 40.8%; Average loss: 3.1857
Iteration: 1634; Percent complete: 40.8%; Average loss: 3.1457
Iteration: 1635; Percent complete: 40.9%; Average loss: 3.0520
Iteration: 1636; Percent complete: 40.9%; Average loss: 3.3153
Iteration: 1637; Percent complete: 40.9%; Average loss: 3.1619
Iteration: 1638; Percent complete: 40.9%; Average loss: 3.1856
Iteration: 1639; Percent complete: 41.0%; Average loss: 3.2193
Iteration: 1640; Percent complete: 41.0%; Average loss: 3.0147
Iteration: 1641; Percent complete: 41.0%; Average loss: 3.3404
Iteration: 1642; Percent complete: 41.0%; Average loss: 3.1115
Iteration: 1643; Percent complete: 41.1%; Average loss: 3.0782
Iteration: 1644; Percent complete: 41.1%; Average loss: 3.4899
Iteration: 1645; Percent complete: 41.1%; Average loss: 3.1569
Iteration: 1646; Percent complete: 41.1%; Average loss: 3.2176
Iteration: 1647; Percent complete: 41.2%; Average loss: 3.4826
Iteration: 1648; Percent complete: 41.2%; Average loss: 3.3298
Iteration: 1649; Percent complete: 41.2%; Average loss: 3.3651
Iteration: 1650; Percent complete: 41.2%; Average loss: 3.2068
Iteration: 1651; Percent complete: 41.3%; Average loss: 3.0939
Iteration: 1652; Percent complete: 41.3%; Average loss: 3.1271
Iteration: 1653; Percent complete: 41.3%; Average loss: 3.3732
Iteration: 1654; Percent complete: 41.3%; Average loss: 3.2618
Iteration: 1655; Percent complete: 41.4%; Average loss: 3.1956
Iteration: 1656; Percent complete: 41.4%; Average loss: 3.5102
Iteration: 1657; Percent complete: 41.4%; Average loss: 3.2064
Iteration: 1658; Percent complete: 41.4%; Average loss: 3.2760
Iteration: 1659; Percent complete: 41.5%; Average loss: 3.3208
Iteration: 1660; Percent complete: 41.5%; Average loss: 3.1833
Iteration: 1661; Percent complete: 41.5%; Average loss: 3.1886
Iteration: 1662; Percent complete: 41.5%; Average loss: 3.1884
Iteration: 1663; Percent complete: 41.6%; Average loss: 3.1899
Iteration: 1664; Percent complete: 41.6%; Average loss: 3.0676
Iteration: 1665; Percent complete: 41.6%; Average loss: 3.1277
Iteration: 1666; Percent complete: 41.6%; Average loss: 3.3672
Iteration: 1667; Percent complete: 41.7%; Average loss: 3.2017
Iteration: 1668; Percent complete: 41.7%; Average loss: 3.3723
Iteration: 1669; Percent complete: 41.7%; Average loss: 3.4939
Iteration: 1670; Percent complete: 41.8%; Average loss: 3.3749
Iteration: 1671; Percent complete: 41.8%; Average loss: 3.4375
Iteration: 1672; Percent complete: 41.8%; Average loss: 3.2882
Iteration: 1673; Percent complete: 41.8%; Average loss: 3.2146
Iteration: 1674; Percent complete: 41.9%; Average loss: 3.3125
Iteration: 1675; Percent complete: 41.9%; Average loss: 3.2531
Iteration: 1676; Percent complete: 41.9%; Average loss: 2.9892
Iteration: 1677; Percent complete: 41.9%; Average loss: 3.1147
Iteration: 1678; Percent complete: 41.9%; Average loss: 3.1320
Iteration: 1679; Percent complete: 42.0%; Average loss: 3.4019
Iteration: 1680; Percent complete: 42.0%; Average loss: 3.2785
Iteration: 1681; Percent complete: 42.0%; Average loss: 3.0596
Iteration: 1682; Percent complete: 42.0%; Average loss: 3.1007
Iteration: 1683; Percent complete: 42.1%; Average loss: 3.2999
Iteration: 1684; Percent complete: 42.1%; Average loss: 3.3612
Iteration: 1685; Percent complete: 42.1%; Average loss: 3.4835
Iteration: 1686; Percent complete: 42.1%; Average loss: 2.9540
Iteration: 1687; Percent complete: 42.2%; Average loss: 2.9937
Iteration: 1688; Percent complete: 42.2%; Average loss: 3.3058
Iteration: 1689; Percent complete: 42.2%; Average loss: 3.3615
Iteration: 1690; Percent complete: 42.2%; Average loss: 3.1377
Iteration: 1691; Percent complete: 42.3%; Average loss: 3.4033
Iteration: 1692; Percent complete: 42.3%; Average loss: 3.3214
Iteration: 1693; Percent complete: 42.3%; Average loss: 2.8532
Iteration: 1694; Percent complete: 42.4%; Average loss: 3.1574
Iteration: 1695; Percent complete: 42.4%; Average loss: 3.0252
Iteration: 1696; Percent complete: 42.4%; Average loss: 3.2520
Iteration: 1697; Percent complete: 42.4%; Average loss: 3.3760
Iteration: 1698; Percent complete: 42.4%; Average loss: 3.1406
Iteration: 1699; Percent complete: 42.5%; Average loss: 3.4435
Iteration: 1700; Percent complete: 42.5%; Average loss: 3.1193
Iteration: 1701; Percent complete: 42.5%; Average loss: 3.1869
Iteration: 1702; Percent complete: 42.5%; Average loss: 3.1441
Iteration: 1703; Percent complete: 42.6%; Average loss: 3.1095
Iteration: 1704; Percent complete: 42.6%; Average loss: 3.3253
Iteration: 1705; Percent complete: 42.6%; Average loss: 3.4194
Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2620
Iteration: 1707; Percent complete: 42.7%; Average loss: 3.0790
Iteration: 1708; Percent complete: 42.7%; Average loss: 3.1850
Iteration: 1709; Percent complete: 42.7%; Average loss: 3.2986
Iteration: 1710; Percent complete: 42.8%; Average loss: 3.1609
Iteration: 1711; Percent complete: 42.8%; Average loss: 3.3375
Iteration: 1712; Percent complete: 42.8%; Average loss: 3.3342
Iteration: 1713; Percent complete: 42.8%; Average loss: 2.9746
Iteration: 1714; Percent complete: 42.9%; Average loss: 3.2223
Iteration: 1715; Percent complete: 42.9%; Average loss: 2.9360
Iteration: 1716; Percent complete: 42.9%; Average loss: 3.3547
Iteration: 1717; Percent complete: 42.9%; Average loss: 3.5296
Iteration: 1718; Percent complete: 43.0%; Average loss: 3.2047
Iteration: 1719; Percent complete: 43.0%; Average loss: 3.1180
Iteration: 1720; Percent complete: 43.0%; Average loss: 3.1440
Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3535
Iteration: 1722; Percent complete: 43.0%; Average loss: 3.2116
Iteration: 1723; Percent complete: 43.1%; Average loss: 3.0340
Iteration: 1724; Percent complete: 43.1%; Average loss: 3.3163
Iteration: 1725; Percent complete: 43.1%; Average loss: 3.1560
Iteration: 1726; Percent complete: 43.1%; Average loss: 3.0992
Iteration: 1727; Percent complete: 43.2%; Average loss: 3.1657
Iteration: 1728; Percent complete: 43.2%; Average loss: 3.3400
Iteration: 1729; Percent complete: 43.2%; Average loss: 3.1909
Iteration: 1730; Percent complete: 43.2%; Average loss: 3.1614
Iteration: 1731; Percent complete: 43.3%; Average loss: 3.0918
Iteration: 1732; Percent complete: 43.3%; Average loss: 3.3377
Iteration: 1733; Percent complete: 43.3%; Average loss: 3.1705
Iteration: 1734; Percent complete: 43.4%; Average loss: 3.2209
Iteration: 1735; Percent complete: 43.4%; Average loss: 3.1481
Iteration: 1736; Percent complete: 43.4%; Average loss: 3.2828
Iteration: 1737; Percent complete: 43.4%; Average loss: 3.0134
Iteration: 1738; Percent complete: 43.5%; Average loss: 3.3265
Iteration: 1739; Percent complete: 43.5%; Average loss: 3.4395
Iteration: 1740; Percent complete: 43.5%; Average loss: 3.1230
Iteration: 1741; Percent complete: 43.5%; Average loss: 3.4156
Iteration: 1742; Percent complete: 43.5%; Average loss: 3.3032
Iteration: 1743; Percent complete: 43.6%; Average loss: 3.1203
Iteration: 1744; Percent complete: 43.6%; Average loss: 3.1213
Iteration: 1745; Percent complete: 43.6%; Average loss: 3.1879
Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3432
Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2177
Iteration: 1748; Percent complete: 43.7%; Average loss: 3.1945
Iteration: 1749; Percent complete: 43.7%; Average loss: 3.1579
Iteration: 1750; Percent complete: 43.8%; Average loss: 3.0291
Iteration: 1751; Percent complete: 43.8%; Average loss: 3.1737
Iteration: 1752; Percent complete: 43.8%; Average loss: 3.0681
Iteration: 1753; Percent complete: 43.8%; Average loss: 3.3475
Iteration: 1754; Percent complete: 43.9%; Average loss: 3.1348
Iteration: 1755; Percent complete: 43.9%; Average loss: 3.3089
Iteration: 1756; Percent complete: 43.9%; Average loss: 3.1593
Iteration: 1757; Percent complete: 43.9%; Average loss: 3.1126
Iteration: 1758; Percent complete: 44.0%; Average loss: 3.0533
Iteration: 1759; Percent complete: 44.0%; Average loss: 3.1951
Iteration: 1760; Percent complete: 44.0%; Average loss: 3.1030
Iteration: 1761; Percent complete: 44.0%; Average loss: 3.1478
Iteration: 1762; Percent complete: 44.0%; Average loss: 3.4005
Iteration: 1763; Percent complete: 44.1%; Average loss: 3.3189
Iteration: 1764; Percent complete: 44.1%; Average loss: 3.5391
Iteration: 1765; Percent complete: 44.1%; Average loss: 3.2676
Iteration: 1766; Percent complete: 44.1%; Average loss: 3.3579
Iteration: 1767; Percent complete: 44.2%; Average loss: 3.2154
Iteration: 1768; Percent complete: 44.2%; Average loss: 3.1752
Iteration: 1769; Percent complete: 44.2%; Average loss: 3.2446
Iteration: 1770; Percent complete: 44.2%; Average loss: 3.1601
Iteration: 1771; Percent complete: 44.3%; Average loss: 3.2894
Iteration: 1772; Percent complete: 44.3%; Average loss: 3.1564
Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2488
Iteration: 1774; Percent complete: 44.4%; Average loss: 3.2445
Iteration: 1775; Percent complete: 44.4%; Average loss: 3.3568
Iteration: 1776; Percent complete: 44.4%; Average loss: 3.0300
Iteration: 1777; Percent complete: 44.4%; Average loss: 3.2202
Iteration: 1778; Percent complete: 44.5%; Average loss: 3.0222
Iteration: 1779; Percent complete: 44.5%; Average loss: 3.5710
Iteration: 1780; Percent complete: 44.5%; Average loss: 3.1113
Iteration: 1781; Percent complete: 44.5%; Average loss: 3.1269
Iteration: 1782; Percent complete: 44.5%; Average loss: 3.2214
Iteration: 1783; Percent complete: 44.6%; Average loss: 3.2217
Iteration: 1784; Percent complete: 44.6%; Average loss: 3.1664
Iteration: 1785; Percent complete: 44.6%; Average loss: 3.2198
Iteration: 1786; Percent complete: 44.6%; Average loss: 3.4879
Iteration: 1787; Percent complete: 44.7%; Average loss: 3.0733
Iteration: 1788; Percent complete: 44.7%; Average loss: 3.3408
Iteration: 1789; Percent complete: 44.7%; Average loss: 3.1705
Iteration: 1790; Percent complete: 44.8%; Average loss: 2.9102
Iteration: 1791; Percent complete: 44.8%; Average loss: 3.3274
Iteration: 1792; Percent complete: 44.8%; Average loss: 3.0552
Iteration: 1793; Percent complete: 44.8%; Average loss: 3.1966
Iteration: 1794; Percent complete: 44.9%; Average loss: 3.3613
Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1763
Iteration: 1796; Percent complete: 44.9%; Average loss: 3.1349
Iteration: 1797; Percent complete: 44.9%; Average loss: 2.9616
Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1373
Iteration: 1799; Percent complete: 45.0%; Average loss: 3.2667
Iteration: 1800; Percent complete: 45.0%; Average loss: 3.4856
Iteration: 1801; Percent complete: 45.0%; Average loss: 3.1926
Iteration: 1802; Percent complete: 45.1%; Average loss: 3.3462
Iteration: 1803; Percent complete: 45.1%; Average loss: 3.1325
Iteration: 1804; Percent complete: 45.1%; Average loss: 3.5008
Iteration: 1805; Percent complete: 45.1%; Average loss: 3.0834
Iteration: 1806; Percent complete: 45.1%; Average loss: 3.1550
Iteration: 1807; Percent complete: 45.2%; Average loss: 3.2168
Iteration: 1808; Percent complete: 45.2%; Average loss: 3.4726
Iteration: 1809; Percent complete: 45.2%; Average loss: 3.3872
Iteration: 1810; Percent complete: 45.2%; Average loss: 3.0755
Iteration: 1811; Percent complete: 45.3%; Average loss: 3.0674
Iteration: 1812; Percent complete: 45.3%; Average loss: 3.0537
Iteration: 1813; Percent complete: 45.3%; Average loss: 3.6765
Iteration: 1814; Percent complete: 45.4%; Average loss: 3.3836
Iteration: 1815; Percent complete: 45.4%; Average loss: 3.1724
Iteration: 1816; Percent complete: 45.4%; Average loss: 3.0890
Iteration: 1817; Percent complete: 45.4%; Average loss: 3.1938
Iteration: 1818; Percent complete: 45.5%; Average loss: 3.2576
Iteration: 1819; Percent complete: 45.5%; Average loss: 3.3282
Iteration: 1820; Percent complete: 45.5%; Average loss: 3.1400
Iteration: 1821; Percent complete: 45.5%; Average loss: 3.0908
Iteration: 1822; Percent complete: 45.6%; Average loss: 3.1469
Iteration: 1823; Percent complete: 45.6%; Average loss: 3.3386
Iteration: 1824; Percent complete: 45.6%; Average loss: 3.1414
Iteration: 1825; Percent complete: 45.6%; Average loss: 2.9829
Iteration: 1826; Percent complete: 45.6%; Average loss: 3.2136
Iteration: 1827; Percent complete: 45.7%; Average loss: 3.1314
Iteration: 1828; Percent complete: 45.7%; Average loss: 3.1886
Iteration: 1829; Percent complete: 45.7%; Average loss: 3.3376
Iteration: 1830; Percent complete: 45.8%; Average loss: 3.1808
Iteration: 1831; Percent complete: 45.8%; Average loss: 3.0352
Iteration: 1832; Percent complete: 45.8%; Average loss: 3.2307
Iteration: 1833; Percent complete: 45.8%; Average loss: 3.0049
Iteration: 1834; Percent complete: 45.9%; Average loss: 3.1096
Iteration: 1835; Percent complete: 45.9%; Average loss: 3.2061
Iteration: 1836; Percent complete: 45.9%; Average loss: 3.2139
Iteration: 1837; Percent complete: 45.9%; Average loss: 2.9230
Iteration: 1838; Percent complete: 46.0%; Average loss: 3.0764
Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2311
Iteration: 1840; Percent complete: 46.0%; Average loss: 3.3178
Iteration: 1841; Percent complete: 46.0%; Average loss: 3.3445
Iteration: 1842; Percent complete: 46.1%; Average loss: 3.0719
Iteration: 1843; Percent complete: 46.1%; Average loss: 2.8518
Iteration: 1844; Percent complete: 46.1%; Average loss: 3.0510
Iteration: 1845; Percent complete: 46.1%; Average loss: 3.2747
Iteration: 1846; Percent complete: 46.2%; Average loss: 3.0965
Iteration: 1847; Percent complete: 46.2%; Average loss: 3.2235
Iteration: 1848; Percent complete: 46.2%; Average loss: 3.1825
Iteration: 1849; Percent complete: 46.2%; Average loss: 2.9811
Iteration: 1850; Percent complete: 46.2%; Average loss: 3.2645
Iteration: 1851; Percent complete: 46.3%; Average loss: 2.8501
Iteration: 1852; Percent complete: 46.3%; Average loss: 3.1641
Iteration: 1853; Percent complete: 46.3%; Average loss: 3.2411
Iteration: 1854; Percent complete: 46.4%; Average loss: 3.1442
Iteration: 1855; Percent complete: 46.4%; Average loss: 2.9846
Iteration: 1856; Percent complete: 46.4%; Average loss: 3.0637
Iteration: 1857; Percent complete: 46.4%; Average loss: 3.2293
Iteration: 1858; Percent complete: 46.5%; Average loss: 3.4974
Iteration: 1859; Percent complete: 46.5%; Average loss: 3.3329
Iteration: 1860; Percent complete: 46.5%; Average loss: 3.4263
Iteration: 1861; Percent complete: 46.5%; Average loss: 3.3835
Iteration: 1862; Percent complete: 46.6%; Average loss: 2.9371
Iteration: 1863; Percent complete: 46.6%; Average loss: 3.3251
Iteration: 1864; Percent complete: 46.6%; Average loss: 3.3721
Iteration: 1865; Percent complete: 46.6%; Average loss: 3.0281
Iteration: 1866; Percent complete: 46.7%; Average loss: 3.4013
Iteration: 1867; Percent complete: 46.7%; Average loss: 3.2290
Iteration: 1868; Percent complete: 46.7%; Average loss: 3.1513
Iteration: 1869; Percent complete: 46.7%; Average loss: 3.0504
Iteration: 1870; Percent complete: 46.8%; Average loss: 3.1496
Iteration: 1871; Percent complete: 46.8%; Average loss: 3.2261
Iteration: 1872; Percent complete: 46.8%; Average loss: 3.1786
Iteration: 1873; Percent complete: 46.8%; Average loss: 2.8622
Iteration: 1874; Percent complete: 46.9%; Average loss: 3.3952
Iteration: 1875; Percent complete: 46.9%; Average loss: 3.1334
Iteration: 1876; Percent complete: 46.9%; Average loss: 3.2652
Iteration: 1877; Percent complete: 46.9%; Average loss: 3.1243
Iteration: 1878; Percent complete: 46.9%; Average loss: 3.3337
Iteration: 1879; Percent complete: 47.0%; Average loss: 3.4771
Iteration: 1880; Percent complete: 47.0%; Average loss: 3.0801
Iteration: 1881; Percent complete: 47.0%; Average loss: 3.1627
Iteration: 1882; Percent complete: 47.0%; Average loss: 3.0389
Iteration: 1883; Percent complete: 47.1%; Average loss: 3.1522
Iteration: 1884; Percent complete: 47.1%; Average loss: 3.1276
Iteration: 1885; Percent complete: 47.1%; Average loss: 2.9191
Iteration: 1886; Percent complete: 47.1%; Average loss: 3.0092
Iteration: 1887; Percent complete: 47.2%; Average loss: 3.1157
Iteration: 1888; Percent complete: 47.2%; Average loss: 3.2566
Iteration: 1889; Percent complete: 47.2%; Average loss: 3.2633
Iteration: 1890; Percent complete: 47.2%; Average loss: 3.2761
Iteration: 1891; Percent complete: 47.3%; Average loss: 3.1836
Iteration: 1892; Percent complete: 47.3%; Average loss: 3.1225
Iteration: 1893; Percent complete: 47.3%; Average loss: 3.1495
Iteration: 1894; Percent complete: 47.3%; Average loss: 2.8280
Iteration: 1895; Percent complete: 47.4%; Average loss: 3.1436
Iteration: 1896; Percent complete: 47.4%; Average loss: 2.8963
Iteration: 1897; Percent complete: 47.4%; Average loss: 3.1968
Iteration: 1898; Percent complete: 47.4%; Average loss: 3.4710
Iteration: 1899; Percent complete: 47.5%; Average loss: 3.0058
Iteration: 1900; Percent complete: 47.5%; Average loss: 3.3101
Iteration: 1901; Percent complete: 47.5%; Average loss: 3.5590
Iteration: 1902; Percent complete: 47.5%; Average loss: 3.3284
Iteration: 1903; Percent complete: 47.6%; Average loss: 3.1167
Iteration: 1904; Percent complete: 47.6%; Average loss: 3.4326
Iteration: 1905; Percent complete: 47.6%; Average loss: 3.0529
Iteration: 1906; Percent complete: 47.6%; Average loss: 2.9834
Iteration: 1907; Percent complete: 47.7%; Average loss: 3.2041
Iteration: 1908; Percent complete: 47.7%; Average loss: 3.2417
Iteration: 1909; Percent complete: 47.7%; Average loss: 3.0985
Iteration: 1910; Percent complete: 47.8%; Average loss: 3.1903
Iteration: 1911; Percent complete: 47.8%; Average loss: 3.3911
Iteration: 1912; Percent complete: 47.8%; Average loss: 3.1681
Iteration: 1913; Percent complete: 47.8%; Average loss: 3.1837
Iteration: 1914; Percent complete: 47.9%; Average loss: 3.1730
Iteration: 1915; Percent complete: 47.9%; Average loss: 3.3707
Iteration: 1916; Percent complete: 47.9%; Average loss: 3.2225
Iteration: 1917; Percent complete: 47.9%; Average loss: 2.9567
Iteration: 1918; Percent complete: 47.9%; Average loss: 3.1367
Iteration: 1919; Percent complete: 48.0%; Average loss: 3.0297
Iteration: 1920; Percent complete: 48.0%; Average loss: 3.1014
Iteration: 1921; Percent complete: 48.0%; Average loss: 3.0988
Iteration: 1922; Percent complete: 48.0%; Average loss: 3.4124
Iteration: 1923; Percent complete: 48.1%; Average loss: 3.2980
Iteration: 1924; Percent complete: 48.1%; Average loss: 3.1639
Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2313
Iteration: 1926; Percent complete: 48.1%; Average loss: 3.1205
Iteration: 1927; Percent complete: 48.2%; Average loss: 3.1382
Iteration: 1928; Percent complete: 48.2%; Average loss: 3.2774
Iteration: 1929; Percent complete: 48.2%; Average loss: 3.0719
Iteration: 1930; Percent complete: 48.2%; Average loss: 3.3584
Iteration: 1931; Percent complete: 48.3%; Average loss: 2.9175
Iteration: 1932; Percent complete: 48.3%; Average loss: 2.9736
Iteration: 1933; Percent complete: 48.3%; Average loss: 3.5139
Iteration: 1934; Percent complete: 48.4%; Average loss: 3.1716
Iteration: 1935; Percent complete: 48.4%; Average loss: 2.8996
Iteration: 1936; Percent complete: 48.4%; Average loss: 3.3451
Iteration: 1937; Percent complete: 48.4%; Average loss: 3.2765
Iteration: 1938; Percent complete: 48.4%; Average loss: 3.0365
Iteration: 1939; Percent complete: 48.5%; Average loss: 3.2972
Iteration: 1940; Percent complete: 48.5%; Average loss: 3.2668
Iteration: 1941; Percent complete: 48.5%; Average loss: 3.0381
Iteration: 1942; Percent complete: 48.5%; Average loss: 3.3979
Iteration: 1943; Percent complete: 48.6%; Average loss: 3.2495
Iteration: 1944; Percent complete: 48.6%; Average loss: 3.0881
Iteration: 1945; Percent complete: 48.6%; Average loss: 3.2125
Iteration: 1946; Percent complete: 48.6%; Average loss: 3.0537
Iteration: 1947; Percent complete: 48.7%; Average loss: 3.2816
Iteration: 1948; Percent complete: 48.7%; Average loss: 3.0659
Iteration: 1949; Percent complete: 48.7%; Average loss: 3.0152
Iteration: 1950; Percent complete: 48.8%; Average loss: 3.3659
Iteration: 1951; Percent complete: 48.8%; Average loss: 3.2581
Iteration: 1952; Percent complete: 48.8%; Average loss: 3.1537
Iteration: 1953; Percent complete: 48.8%; Average loss: 3.4376
Iteration: 1954; Percent complete: 48.9%; Average loss: 3.1786
Iteration: 1955; Percent complete: 48.9%; Average loss: 3.3011
Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2319
Iteration: 1957; Percent complete: 48.9%; Average loss: 2.9540
Iteration: 1958; Percent complete: 48.9%; Average loss: 2.9948
Iteration: 1959; Percent complete: 49.0%; Average loss: 3.2817
Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2590
Iteration: 1961; Percent complete: 49.0%; Average loss: 3.4050
Iteration: 1962; Percent complete: 49.0%; Average loss: 3.2115
Iteration: 1963; Percent complete: 49.1%; Average loss: 3.2280
Iteration: 1964; Percent complete: 49.1%; Average loss: 3.2866
Iteration: 1965; Percent complete: 49.1%; Average loss: 3.2800
Iteration: 1966; Percent complete: 49.1%; Average loss: 3.1790
Iteration: 1967; Percent complete: 49.2%; Average loss: 2.9337
Iteration: 1968; Percent complete: 49.2%; Average loss: 2.9679
Iteration: 1969; Percent complete: 49.2%; Average loss: 2.9818
Iteration: 1970; Percent complete: 49.2%; Average loss: 2.9851
Iteration: 1971; Percent complete: 49.3%; Average loss: 3.2793
Iteration: 1972; Percent complete: 49.3%; Average loss: 3.1478
Iteration: 1973; Percent complete: 49.3%; Average loss: 3.1870
Iteration: 1974; Percent complete: 49.4%; Average loss: 2.9428
Iteration: 1975; Percent complete: 49.4%; Average loss: 3.0529
Iteration: 1976; Percent complete: 49.4%; Average loss: 3.2631
Iteration: 1977; Percent complete: 49.4%; Average loss: 3.2193
Iteration: 1978; Percent complete: 49.5%; Average loss: 3.2497
Iteration: 1979; Percent complete: 49.5%; Average loss: 3.2042
Iteration: 1980; Percent complete: 49.5%; Average loss: 3.0270
Iteration: 1981; Percent complete: 49.5%; Average loss: 3.3092
Iteration: 1982; Percent complete: 49.5%; Average loss: 3.1955
Iteration: 1983; Percent complete: 49.6%; Average loss: 3.2741
Iteration: 1984; Percent complete: 49.6%; Average loss: 3.2086
Iteration: 1985; Percent complete: 49.6%; Average loss: 3.2250
Iteration: 1986; Percent complete: 49.6%; Average loss: 3.0298
Iteration: 1987; Percent complete: 49.7%; Average loss: 3.2413
Iteration: 1988; Percent complete: 49.7%; Average loss: 3.1765
Iteration: 1989; Percent complete: 49.7%; Average loss: 3.1372
Iteration: 1990; Percent complete: 49.8%; Average loss: 3.1113
Iteration: 1991; Percent complete: 49.8%; Average loss: 3.1510
Iteration: 1992; Percent complete: 49.8%; Average loss: 3.1575
Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2149
Iteration: 1994; Percent complete: 49.9%; Average loss: 3.1598
Iteration: 1995; Percent complete: 49.9%; Average loss: 3.1962
Iteration: 1996; Percent complete: 49.9%; Average loss: 2.9342
Iteration: 1997; Percent complete: 49.9%; Average loss: 3.0563
Iteration: 1998; Percent complete: 50.0%; Average loss: 3.4259
Iteration: 1999; Percent complete: 50.0%; Average loss: 3.0870
Iteration: 2000; Percent complete: 50.0%; Average loss: 3.1111
Iteration: 2001; Percent complete: 50.0%; Average loss: 3.0993
Iteration: 2002; Percent complete: 50.0%; Average loss: 3.2013
Iteration: 2003; Percent complete: 50.1%; Average loss: 3.0353
Iteration: 2004; Percent complete: 50.1%; Average loss: 3.3329
Iteration: 2005; Percent complete: 50.1%; Average loss: 3.1404
Iteration: 2006; Percent complete: 50.1%; Average loss: 3.2821
Iteration: 2007; Percent complete: 50.2%; Average loss: 3.1778
Iteration: 2008; Percent complete: 50.2%; Average loss: 3.3454
Iteration: 2009; Percent complete: 50.2%; Average loss: 3.3856
Iteration: 2010; Percent complete: 50.2%; Average loss: 3.1633
Iteration: 2011; Percent complete: 50.3%; Average loss: 3.1117
Iteration: 2012; Percent complete: 50.3%; Average loss: 3.1345
Iteration: 2013; Percent complete: 50.3%; Average loss: 3.1660
Iteration: 2014; Percent complete: 50.3%; Average loss: 3.3569
Iteration: 2015; Percent complete: 50.4%; Average loss: 2.9378
Iteration: 2016; Percent complete: 50.4%; Average loss: 3.0219
Iteration: 2017; Percent complete: 50.4%; Average loss: 3.2213
Iteration: 2018; Percent complete: 50.4%; Average loss: 3.1188
Iteration: 2019; Percent complete: 50.5%; Average loss: 3.0823
Iteration: 2020; Percent complete: 50.5%; Average loss: 3.1283
Iteration: 2021; Percent complete: 50.5%; Average loss: 3.1396
Iteration: 2022; Percent complete: 50.5%; Average loss: 3.1236
Iteration: 2023; Percent complete: 50.6%; Average loss: 3.1119
Iteration: 2024; Percent complete: 50.6%; Average loss: 3.3307
Iteration: 2025; Percent complete: 50.6%; Average loss: 2.9558
Iteration: 2026; Percent complete: 50.6%; Average loss: 3.2928
Iteration: 2027; Percent complete: 50.7%; Average loss: 3.2410
Iteration: 2028; Percent complete: 50.7%; Average loss: 3.0331
Iteration: 2029; Percent complete: 50.7%; Average loss: 3.2331
Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1226
Iteration: 2031; Percent complete: 50.8%; Average loss: 3.2830
Iteration: 2032; Percent complete: 50.8%; Average loss: 3.0464
Iteration: 2033; Percent complete: 50.8%; Average loss: 3.1457
Iteration: 2034; Percent complete: 50.8%; Average loss: 3.2074
Iteration: 2035; Percent complete: 50.9%; Average loss: 3.1563
Iteration: 2036; Percent complete: 50.9%; Average loss: 3.1683
Iteration: 2037; Percent complete: 50.9%; Average loss: 3.1797
Iteration: 2038; Percent complete: 50.9%; Average loss: 2.9706
Iteration: 2039; Percent complete: 51.0%; Average loss: 3.2146
Iteration: 2040; Percent complete: 51.0%; Average loss: 3.1416
Iteration: 2041; Percent complete: 51.0%; Average loss: 3.1518
Iteration: 2042; Percent complete: 51.0%; Average loss: 3.2919
Iteration: 2043; Percent complete: 51.1%; Average loss: 3.2502
Iteration: 2044; Percent complete: 51.1%; Average loss: 3.1035
Iteration: 2045; Percent complete: 51.1%; Average loss: 3.1583
Iteration: 2046; Percent complete: 51.1%; Average loss: 3.1914
Iteration: 2047; Percent complete: 51.2%; Average loss: 3.1373
Iteration: 2048; Percent complete: 51.2%; Average loss: 3.0292
Iteration: 2049; Percent complete: 51.2%; Average loss: 3.3461
Iteration: 2050; Percent complete: 51.2%; Average loss: 3.5478
Iteration: 2051; Percent complete: 51.3%; Average loss: 3.0385
Iteration: 2052; Percent complete: 51.3%; Average loss: 3.0215
Iteration: 2053; Percent complete: 51.3%; Average loss: 3.3260
Iteration: 2054; Percent complete: 51.3%; Average loss: 3.0994
Iteration: 2055; Percent complete: 51.4%; Average loss: 3.3191
Iteration: 2056; Percent complete: 51.4%; Average loss: 3.1834
Iteration: 2057; Percent complete: 51.4%; Average loss: 3.0319
Iteration: 2058; Percent complete: 51.4%; Average loss: 3.1634
Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0359
Iteration: 2060; Percent complete: 51.5%; Average loss: 3.1870
Iteration: 2061; Percent complete: 51.5%; Average loss: 3.0002
Iteration: 2062; Percent complete: 51.5%; Average loss: 3.1429
Iteration: 2063; Percent complete: 51.6%; Average loss: 2.9228
Iteration: 2064; Percent complete: 51.6%; Average loss: 3.0136
Iteration: 2065; Percent complete: 51.6%; Average loss: 3.3301
Iteration: 2066; Percent complete: 51.6%; Average loss: 2.8428
Iteration: 2067; Percent complete: 51.7%; Average loss: 3.4851
Iteration: 2068; Percent complete: 51.7%; Average loss: 3.1595
Iteration: 2069; Percent complete: 51.7%; Average loss: 2.8167
Iteration: 2070; Percent complete: 51.7%; Average loss: 3.1758
Iteration: 2071; Percent complete: 51.8%; Average loss: 3.1497
Iteration: 2072; Percent complete: 51.8%; Average loss: 2.7625
Iteration: 2073; Percent complete: 51.8%; Average loss: 3.1245
Iteration: 2074; Percent complete: 51.8%; Average loss: 3.1605
Iteration: 2075; Percent complete: 51.9%; Average loss: 3.0165
Iteration: 2076; Percent complete: 51.9%; Average loss: 3.0178
Iteration: 2077; Percent complete: 51.9%; Average loss: 3.2057
Iteration: 2078; Percent complete: 51.9%; Average loss: 3.0881
Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2362
Iteration: 2080; Percent complete: 52.0%; Average loss: 3.2388
Iteration: 2081; Percent complete: 52.0%; Average loss: 2.8827
Iteration: 2082; Percent complete: 52.0%; Average loss: 3.3537
Iteration: 2083; Percent complete: 52.1%; Average loss: 3.0849
Iteration: 2084; Percent complete: 52.1%; Average loss: 2.9757
Iteration: 2085; Percent complete: 52.1%; Average loss: 3.0576
Iteration: 2086; Percent complete: 52.1%; Average loss: 3.0509
Iteration: 2087; Percent complete: 52.2%; Average loss: 3.1254
Iteration: 2088; Percent complete: 52.2%; Average loss: 3.1930
Iteration: 2089; Percent complete: 52.2%; Average loss: 3.1363
Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0172
Iteration: 2091; Percent complete: 52.3%; Average loss: 3.0866
Iteration: 2092; Percent complete: 52.3%; Average loss: 3.3390
Iteration: 2093; Percent complete: 52.3%; Average loss: 3.2179
Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0343
Iteration: 2095; Percent complete: 52.4%; Average loss: 2.9242
Iteration: 2096; Percent complete: 52.4%; Average loss: 3.2978
Iteration: 2097; Percent complete: 52.4%; Average loss: 3.1718
Iteration: 2098; Percent complete: 52.4%; Average loss: 2.9698
Iteration: 2099; Percent complete: 52.5%; Average loss: 3.1324
Iteration: 2100; Percent complete: 52.5%; Average loss: 3.1287
Iteration: 2101; Percent complete: 52.5%; Average loss: 3.2742
Iteration: 2102; Percent complete: 52.5%; Average loss: 2.9340
Iteration: 2103; Percent complete: 52.6%; Average loss: 2.9764
Iteration: 2104; Percent complete: 52.6%; Average loss: 3.2426
Iteration: 2105; Percent complete: 52.6%; Average loss: 3.0340
Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0435
Iteration: 2107; Percent complete: 52.7%; Average loss: 3.3828
Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0578
Iteration: 2109; Percent complete: 52.7%; Average loss: 2.9239
Iteration: 2110; Percent complete: 52.8%; Average loss: 2.9464
Iteration: 2111; Percent complete: 52.8%; Average loss: 3.1377
Iteration: 2112; Percent complete: 52.8%; Average loss: 3.0695
Iteration: 2113; Percent complete: 52.8%; Average loss: 3.3641
Iteration: 2114; Percent complete: 52.8%; Average loss: 3.2665
Iteration: 2115; Percent complete: 52.9%; Average loss: 2.9539
Iteration: 2116; Percent complete: 52.9%; Average loss: 2.9813
Iteration: 2117; Percent complete: 52.9%; Average loss: 3.1615
Iteration: 2118; Percent complete: 52.9%; Average loss: 3.2039
Iteration: 2119; Percent complete: 53.0%; Average loss: 3.1362
Iteration: 2120; Percent complete: 53.0%; Average loss: 3.1111
Iteration: 2121; Percent complete: 53.0%; Average loss: 3.2311
Iteration: 2122; Percent complete: 53.0%; Average loss: 3.1336
Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2398
Iteration: 2124; Percent complete: 53.1%; Average loss: 3.0240
Iteration: 2125; Percent complete: 53.1%; Average loss: 3.1557
Iteration: 2126; Percent complete: 53.1%; Average loss: 3.1825
Iteration: 2127; Percent complete: 53.2%; Average loss: 3.2605
Iteration: 2128; Percent complete: 53.2%; Average loss: 3.1192
Iteration: 2129; Percent complete: 53.2%; Average loss: 3.2431
Iteration: 2130; Percent complete: 53.2%; Average loss: 3.2701
Iteration: 2131; Percent complete: 53.3%; Average loss: 3.0694
Iteration: 2132; Percent complete: 53.3%; Average loss: 3.0936
Iteration: 2133; Percent complete: 53.3%; Average loss: 2.8974
Iteration: 2134; Percent complete: 53.3%; Average loss: 2.9516
Iteration: 2135; Percent complete: 53.4%; Average loss: 3.2077
Iteration: 2136; Percent complete: 53.4%; Average loss: 3.1049
Iteration: 2137; Percent complete: 53.4%; Average loss: 2.9053
Iteration: 2138; Percent complete: 53.4%; Average loss: 3.1430
Iteration: 2139; Percent complete: 53.5%; Average loss: 3.2397
Iteration: 2140; Percent complete: 53.5%; Average loss: 3.2296
Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0753
Iteration: 2142; Percent complete: 53.5%; Average loss: 3.2060
Iteration: 2143; Percent complete: 53.6%; Average loss: 2.9273
Iteration: 2144; Percent complete: 53.6%; Average loss: 3.4794
Iteration: 2145; Percent complete: 53.6%; Average loss: 3.1550
Iteration: 2146; Percent complete: 53.6%; Average loss: 2.8411
Iteration: 2147; Percent complete: 53.7%; Average loss: 3.0470
Iteration: 2148; Percent complete: 53.7%; Average loss: 3.0581
Iteration: 2149; Percent complete: 53.7%; Average loss: 3.1382
Iteration: 2150; Percent complete: 53.8%; Average loss: 3.1212
Iteration: 2151; Percent complete: 53.8%; Average loss: 2.8873
Iteration: 2152; Percent complete: 53.8%; Average loss: 3.1353
Iteration: 2153; Percent complete: 53.8%; Average loss: 2.8773
Iteration: 2154; Percent complete: 53.8%; Average loss: 3.1028
Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1669
Iteration: 2156; Percent complete: 53.9%; Average loss: 3.3931
Iteration: 2157; Percent complete: 53.9%; Average loss: 3.3086
Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1134
Iteration: 2159; Percent complete: 54.0%; Average loss: 2.9940
Iteration: 2160; Percent complete: 54.0%; Average loss: 3.0697
Iteration: 2161; Percent complete: 54.0%; Average loss: 3.2132
Iteration: 2162; Percent complete: 54.0%; Average loss: 3.0536
Iteration: 2163; Percent complete: 54.1%; Average loss: 3.2203
Iteration: 2164; Percent complete: 54.1%; Average loss: 3.2449
Iteration: 2165; Percent complete: 54.1%; Average loss: 3.0459
Iteration: 2166; Percent complete: 54.1%; Average loss: 3.0718
Iteration: 2167; Percent complete: 54.2%; Average loss: 3.1628
Iteration: 2168; Percent complete: 54.2%; Average loss: 3.1384
Iteration: 2169; Percent complete: 54.2%; Average loss: 2.9878
Iteration: 2170; Percent complete: 54.2%; Average loss: 2.8741
Iteration: 2171; Percent complete: 54.3%; Average loss: 3.2768
Iteration: 2172; Percent complete: 54.3%; Average loss: 3.0470
Iteration: 2173; Percent complete: 54.3%; Average loss: 2.9580
Iteration: 2174; Percent complete: 54.4%; Average loss: 2.8803
Iteration: 2175; Percent complete: 54.4%; Average loss: 3.0202
Iteration: 2176; Percent complete: 54.4%; Average loss: 3.0115
Iteration: 2177; Percent complete: 54.4%; Average loss: 3.2648
Iteration: 2178; Percent complete: 54.4%; Average loss: 2.9407
Iteration: 2179; Percent complete: 54.5%; Average loss: 3.0076
Iteration: 2180; Percent complete: 54.5%; Average loss: 2.8910
Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9533
Iteration: 2182; Percent complete: 54.5%; Average loss: 3.0774
Iteration: 2183; Percent complete: 54.6%; Average loss: 3.1355
Iteration: 2184; Percent complete: 54.6%; Average loss: 3.2982
Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1886
Iteration: 2186; Percent complete: 54.6%; Average loss: 3.2393
Iteration: 2187; Percent complete: 54.7%; Average loss: 3.0539
Iteration: 2188; Percent complete: 54.7%; Average loss: 3.2539
Iteration: 2189; Percent complete: 54.7%; Average loss: 3.0000
Iteration: 2190; Percent complete: 54.8%; Average loss: 3.1494
Iteration: 2191; Percent complete: 54.8%; Average loss: 3.2575
Iteration: 2192; Percent complete: 54.8%; Average loss: 3.0064
Iteration: 2193; Percent complete: 54.8%; Average loss: 2.8109
Iteration: 2194; Percent complete: 54.9%; Average loss: 2.9152
Iteration: 2195; Percent complete: 54.9%; Average loss: 3.1218
Iteration: 2196; Percent complete: 54.9%; Average loss: 3.3083
Iteration: 2197; Percent complete: 54.9%; Average loss: 3.0769
Iteration: 2198; Percent complete: 54.9%; Average loss: 2.9659
Iteration: 2199; Percent complete: 55.0%; Average loss: 3.0562
Iteration: 2200; Percent complete: 55.0%; Average loss: 3.0005
Iteration: 2201; Percent complete: 55.0%; Average loss: 3.0325
Iteration: 2202; Percent complete: 55.0%; Average loss: 2.9929
Iteration: 2203; Percent complete: 55.1%; Average loss: 3.2102
Iteration: 2204; Percent complete: 55.1%; Average loss: 3.2063
Iteration: 2205; Percent complete: 55.1%; Average loss: 3.0742
Iteration: 2206; Percent complete: 55.1%; Average loss: 3.0768
Iteration: 2207; Percent complete: 55.2%; Average loss: 3.3035
Iteration: 2208; Percent complete: 55.2%; Average loss: 3.1224
Iteration: 2209; Percent complete: 55.2%; Average loss: 3.1547
Iteration: 2210; Percent complete: 55.2%; Average loss: 3.0879
Iteration: 2211; Percent complete: 55.3%; Average loss: 3.2295
Iteration: 2212; Percent complete: 55.3%; Average loss: 2.8911
Iteration: 2213; Percent complete: 55.3%; Average loss: 3.2706
Iteration: 2214; Percent complete: 55.4%; Average loss: 2.9688
Iteration: 2215; Percent complete: 55.4%; Average loss: 3.3188
Iteration: 2216; Percent complete: 55.4%; Average loss: 3.1731
Iteration: 2217; Percent complete: 55.4%; Average loss: 3.0598
Iteration: 2218; Percent complete: 55.5%; Average loss: 3.1358
Iteration: 2219; Percent complete: 55.5%; Average loss: 3.2699
Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9479
Iteration: 2221; Percent complete: 55.5%; Average loss: 3.0132
Iteration: 2222; Percent complete: 55.5%; Average loss: 2.9948
Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0097
Iteration: 2224; Percent complete: 55.6%; Average loss: 2.9455
Iteration: 2225; Percent complete: 55.6%; Average loss: 3.2243
Iteration: 2226; Percent complete: 55.6%; Average loss: 3.2539
Iteration: 2227; Percent complete: 55.7%; Average loss: 3.1498
Iteration: 2228; Percent complete: 55.7%; Average loss: 2.9026
Iteration: 2229; Percent complete: 55.7%; Average loss: 3.0303
Iteration: 2230; Percent complete: 55.8%; Average loss: 2.8220
Iteration: 2231; Percent complete: 55.8%; Average loss: 3.0626
Iteration: 2232; Percent complete: 55.8%; Average loss: 2.9218
Iteration: 2233; Percent complete: 55.8%; Average loss: 3.1953
Iteration: 2234; Percent complete: 55.9%; Average loss: 3.1721
Iteration: 2235; Percent complete: 55.9%; Average loss: 2.9952
Iteration: 2236; Percent complete: 55.9%; Average loss: 3.1398
Iteration: 2237; Percent complete: 55.9%; Average loss: 3.0132
Iteration: 2238; Percent complete: 56.0%; Average loss: 3.0293
Iteration: 2239; Percent complete: 56.0%; Average loss: 3.0281
Iteration: 2240; Percent complete: 56.0%; Average loss: 3.0163
Iteration: 2241; Percent complete: 56.0%; Average loss: 3.0737
Iteration: 2242; Percent complete: 56.0%; Average loss: 3.3140
Iteration: 2243; Percent complete: 56.1%; Average loss: 3.0611
Iteration: 2244; Percent complete: 56.1%; Average loss: 2.8895
Iteration: 2245; Percent complete: 56.1%; Average loss: 2.9727
Iteration: 2246; Percent complete: 56.1%; Average loss: 2.9695
Iteration: 2247; Percent complete: 56.2%; Average loss: 3.3366
Iteration: 2248; Percent complete: 56.2%; Average loss: 3.1659
Iteration: 2249; Percent complete: 56.2%; Average loss: 2.9446
Iteration: 2250; Percent complete: 56.2%; Average loss: 2.9740
Iteration: 2251; Percent complete: 56.3%; Average loss: 2.9550
Iteration: 2252; Percent complete: 56.3%; Average loss: 2.9169
Iteration: 2253; Percent complete: 56.3%; Average loss: 3.1027
Iteration: 2254; Percent complete: 56.4%; Average loss: 3.0809
Iteration: 2255; Percent complete: 56.4%; Average loss: 2.9651
Iteration: 2256; Percent complete: 56.4%; Average loss: 3.1326
Iteration: 2257; Percent complete: 56.4%; Average loss: 2.9456
Iteration: 2258; Percent complete: 56.5%; Average loss: 3.2628
Iteration: 2259; Percent complete: 56.5%; Average loss: 3.0666
Iteration: 2260; Percent complete: 56.5%; Average loss: 3.1143
Iteration: 2261; Percent complete: 56.5%; Average loss: 3.0704
Iteration: 2262; Percent complete: 56.5%; Average loss: 2.9948
Iteration: 2263; Percent complete: 56.6%; Average loss: 2.8214
Iteration: 2264; Percent complete: 56.6%; Average loss: 2.9950
Iteration: 2265; Percent complete: 56.6%; Average loss: 3.1857
Iteration: 2266; Percent complete: 56.6%; Average loss: 3.2437
Iteration: 2267; Percent complete: 56.7%; Average loss: 2.8890
Iteration: 2268; Percent complete: 56.7%; Average loss: 3.2245
Iteration: 2269; Percent complete: 56.7%; Average loss: 3.1576
Iteration: 2270; Percent complete: 56.8%; Average loss: 3.3469
Iteration: 2271; Percent complete: 56.8%; Average loss: 2.9428
Iteration: 2272; Percent complete: 56.8%; Average loss: 3.1261
Iteration: 2273; Percent complete: 56.8%; Average loss: 3.0154
Iteration: 2274; Percent complete: 56.9%; Average loss: 3.0412
Iteration: 2275; Percent complete: 56.9%; Average loss: 2.8648
Iteration: 2276; Percent complete: 56.9%; Average loss: 3.0504
Iteration: 2277; Percent complete: 56.9%; Average loss: 3.3264
Iteration: 2278; Percent complete: 57.0%; Average loss: 3.2619
Iteration: 2279; Percent complete: 57.0%; Average loss: 3.3380
Iteration: 2280; Percent complete: 57.0%; Average loss: 2.8709
Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1815
Iteration: 2282; Percent complete: 57.0%; Average loss: 3.1989
Iteration: 2283; Percent complete: 57.1%; Average loss: 3.1763
Iteration: 2284; Percent complete: 57.1%; Average loss: 2.9938
Iteration: 2285; Percent complete: 57.1%; Average loss: 3.3725
Iteration: 2286; Percent complete: 57.1%; Average loss: 2.8469
Iteration: 2287; Percent complete: 57.2%; Average loss: 3.0917
Iteration: 2288; Percent complete: 57.2%; Average loss: 3.1156
Iteration: 2289; Percent complete: 57.2%; Average loss: 3.1288
Iteration: 2290; Percent complete: 57.2%; Average loss: 3.2643
Iteration: 2291; Percent complete: 57.3%; Average loss: 3.0469
Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0497
Iteration: 2293; Percent complete: 57.3%; Average loss: 3.2226
Iteration: 2294; Percent complete: 57.4%; Average loss: 3.1702
Iteration: 2295; Percent complete: 57.4%; Average loss: 3.0088
Iteration: 2296; Percent complete: 57.4%; Average loss: 2.9569
Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0303
Iteration: 2298; Percent complete: 57.5%; Average loss: 3.2296
Iteration: 2299; Percent complete: 57.5%; Average loss: 3.0772
Iteration: 2300; Percent complete: 57.5%; Average loss: 3.3288
Iteration: 2301; Percent complete: 57.5%; Average loss: 2.9223
Iteration: 2302; Percent complete: 57.6%; Average loss: 3.0105
Iteration: 2303; Percent complete: 57.6%; Average loss: 2.9847
Iteration: 2304; Percent complete: 57.6%; Average loss: 3.1778
Iteration: 2305; Percent complete: 57.6%; Average loss: 3.0382
Iteration: 2306; Percent complete: 57.6%; Average loss: 2.9670
Iteration: 2307; Percent complete: 57.7%; Average loss: 3.1361
Iteration: 2308; Percent complete: 57.7%; Average loss: 3.3537
Iteration: 2309; Percent complete: 57.7%; Average loss: 2.8265
Iteration: 2310; Percent complete: 57.8%; Average loss: 3.1137
Iteration: 2311; Percent complete: 57.8%; Average loss: 2.9159
Iteration: 2312; Percent complete: 57.8%; Average loss: 3.0752
Iteration: 2313; Percent complete: 57.8%; Average loss: 2.8623
Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2976
Iteration: 2315; Percent complete: 57.9%; Average loss: 2.7687
Iteration: 2316; Percent complete: 57.9%; Average loss: 2.9943
Iteration: 2317; Percent complete: 57.9%; Average loss: 3.0785
Iteration: 2318; Percent complete: 58.0%; Average loss: 3.2430
Iteration: 2319; Percent complete: 58.0%; Average loss: 3.0092
Iteration: 2320; Percent complete: 58.0%; Average loss: 2.8620
Iteration: 2321; Percent complete: 58.0%; Average loss: 2.9569
Iteration: 2322; Percent complete: 58.1%; Average loss: 2.9517
Iteration: 2323; Percent complete: 58.1%; Average loss: 3.1651
Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9600
Iteration: 2325; Percent complete: 58.1%; Average loss: 2.9802
Iteration: 2326; Percent complete: 58.1%; Average loss: 2.9853
Iteration: 2327; Percent complete: 58.2%; Average loss: 3.0750
Iteration: 2328; Percent complete: 58.2%; Average loss: 3.0062
Iteration: 2329; Percent complete: 58.2%; Average loss: 3.2102
Iteration: 2330; Percent complete: 58.2%; Average loss: 3.0101
Iteration: 2331; Percent complete: 58.3%; Average loss: 3.2132
Iteration: 2332; Percent complete: 58.3%; Average loss: 3.1078
Iteration: 2333; Percent complete: 58.3%; Average loss: 2.8645
Iteration: 2334; Percent complete: 58.4%; Average loss: 3.1265
Iteration: 2335; Percent complete: 58.4%; Average loss: 3.2366
Iteration: 2336; Percent complete: 58.4%; Average loss: 3.1177
Iteration: 2337; Percent complete: 58.4%; Average loss: 3.0128
Iteration: 2338; Percent complete: 58.5%; Average loss: 2.9362
Iteration: 2339; Percent complete: 58.5%; Average loss: 3.0957
Iteration: 2340; Percent complete: 58.5%; Average loss: 3.1120
Iteration: 2341; Percent complete: 58.5%; Average loss: 3.1057
Iteration: 2342; Percent complete: 58.6%; Average loss: 3.0916
Iteration: 2343; Percent complete: 58.6%; Average loss: 3.1963
Iteration: 2344; Percent complete: 58.6%; Average loss: 2.9763
Iteration: 2345; Percent complete: 58.6%; Average loss: 2.9161
Iteration: 2346; Percent complete: 58.7%; Average loss: 2.9902
Iteration: 2347; Percent complete: 58.7%; Average loss: 2.9617
Iteration: 2348; Percent complete: 58.7%; Average loss: 3.0650
Iteration: 2349; Percent complete: 58.7%; Average loss: 2.9029
Iteration: 2350; Percent complete: 58.8%; Average loss: 3.0033
Iteration: 2351; Percent complete: 58.8%; Average loss: 2.7543
Iteration: 2352; Percent complete: 58.8%; Average loss: 2.9400
Iteration: 2353; Percent complete: 58.8%; Average loss: 2.9712
Iteration: 2354; Percent complete: 58.9%; Average loss: 2.9549
Iteration: 2355; Percent complete: 58.9%; Average loss: 3.1665
Iteration: 2356; Percent complete: 58.9%; Average loss: 2.8117
Iteration: 2357; Percent complete: 58.9%; Average loss: 2.9163
Iteration: 2358; Percent complete: 59.0%; Average loss: 3.0295
Iteration: 2359; Percent complete: 59.0%; Average loss: 2.6742
Iteration: 2360; Percent complete: 59.0%; Average loss: 2.9718
Iteration: 2361; Percent complete: 59.0%; Average loss: 3.0787
Iteration: 2362; Percent complete: 59.1%; Average loss: 3.1829
Iteration: 2363; Percent complete: 59.1%; Average loss: 3.0138
Iteration: 2364; Percent complete: 59.1%; Average loss: 2.9852
Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0188
Iteration: 2366; Percent complete: 59.2%; Average loss: 2.7069
Iteration: 2367; Percent complete: 59.2%; Average loss: 2.8893
Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0654
Iteration: 2369; Percent complete: 59.2%; Average loss: 2.8445
Iteration: 2370; Percent complete: 59.2%; Average loss: 3.1798
Iteration: 2371; Percent complete: 59.3%; Average loss: 3.0422
Iteration: 2372; Percent complete: 59.3%; Average loss: 3.0918
Iteration: 2373; Percent complete: 59.3%; Average loss: 3.1885
Iteration: 2374; Percent complete: 59.4%; Average loss: 2.9100
Iteration: 2375; Percent complete: 59.4%; Average loss: 2.9875
Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0054
Iteration: 2377; Percent complete: 59.4%; Average loss: 3.0735
Iteration: 2378; Percent complete: 59.5%; Average loss: 2.9295
Iteration: 2379; Percent complete: 59.5%; Average loss: 3.0174
Iteration: 2380; Percent complete: 59.5%; Average loss: 3.1699
Iteration: 2381; Percent complete: 59.5%; Average loss: 3.2249
Iteration: 2382; Percent complete: 59.6%; Average loss: 3.0148
Iteration: 2383; Percent complete: 59.6%; Average loss: 3.0795
Iteration: 2384; Percent complete: 59.6%; Average loss: 3.0327
Iteration: 2385; Percent complete: 59.6%; Average loss: 2.9287
Iteration: 2386; Percent complete: 59.7%; Average loss: 3.1455
Iteration: 2387; Percent complete: 59.7%; Average loss: 2.8606
Iteration: 2388; Percent complete: 59.7%; Average loss: 3.0627
Iteration: 2389; Percent complete: 59.7%; Average loss: 3.2466
Iteration: 2390; Percent complete: 59.8%; Average loss: 3.2737
Iteration: 2391; Percent complete: 59.8%; Average loss: 3.1571
Iteration: 2392; Percent complete: 59.8%; Average loss: 3.2260
Iteration: 2393; Percent complete: 59.8%; Average loss: 2.9456
Iteration: 2394; Percent complete: 59.9%; Average loss: 3.0944
Iteration: 2395; Percent complete: 59.9%; Average loss: 3.1106
Iteration: 2396; Percent complete: 59.9%; Average loss: 3.0993
Iteration: 2397; Percent complete: 59.9%; Average loss: 3.3091
Iteration: 2398; Percent complete: 60.0%; Average loss: 3.2030
Iteration: 2399; Percent complete: 60.0%; Average loss: 2.9865
Iteration: 2400; Percent complete: 60.0%; Average loss: 3.1098
Iteration: 2401; Percent complete: 60.0%; Average loss: 3.1353
Iteration: 2402; Percent complete: 60.1%; Average loss: 3.0184
Iteration: 2403; Percent complete: 60.1%; Average loss: 3.2073
Iteration: 2404; Percent complete: 60.1%; Average loss: 3.2116
Iteration: 2405; Percent complete: 60.1%; Average loss: 2.9011
Iteration: 2406; Percent complete: 60.2%; Average loss: 2.8064
Iteration: 2407; Percent complete: 60.2%; Average loss: 3.1460
Iteration: 2408; Percent complete: 60.2%; Average loss: 2.9474
Iteration: 2409; Percent complete: 60.2%; Average loss: 2.8409
Iteration: 2410; Percent complete: 60.2%; Average loss: 3.3371
Iteration: 2411; Percent complete: 60.3%; Average loss: 3.0393
Iteration: 2412; Percent complete: 60.3%; Average loss: 3.0917
Iteration: 2413; Percent complete: 60.3%; Average loss: 3.0013
Iteration: 2414; Percent complete: 60.4%; Average loss: 2.8798
Iteration: 2415; Percent complete: 60.4%; Average loss: 3.0795
Iteration: 2416; Percent complete: 60.4%; Average loss: 2.9672
Iteration: 2417; Percent complete: 60.4%; Average loss: 2.7818
Iteration: 2418; Percent complete: 60.5%; Average loss: 3.1542
Iteration: 2419; Percent complete: 60.5%; Average loss: 2.8641
Iteration: 2420; Percent complete: 60.5%; Average loss: 3.0424
Iteration: 2421; Percent complete: 60.5%; Average loss: 3.1248
Iteration: 2422; Percent complete: 60.6%; Average loss: 3.1081
Iteration: 2423; Percent complete: 60.6%; Average loss: 2.9775
Iteration: 2424; Percent complete: 60.6%; Average loss: 2.9124
Iteration: 2425; Percent complete: 60.6%; Average loss: 2.9886
Iteration: 2426; Percent complete: 60.7%; Average loss: 3.0180
Iteration: 2427; Percent complete: 60.7%; Average loss: 3.2768
Iteration: 2428; Percent complete: 60.7%; Average loss: 2.8564
Iteration: 2429; Percent complete: 60.7%; Average loss: 2.9831
Iteration: 2430; Percent complete: 60.8%; Average loss: 3.0951
Iteration: 2431; Percent complete: 60.8%; Average loss: 2.7039
Iteration: 2432; Percent complete: 60.8%; Average loss: 3.1777
Iteration: 2433; Percent complete: 60.8%; Average loss: 2.9826
Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0569
Iteration: 2435; Percent complete: 60.9%; Average loss: 2.8123
Iteration: 2436; Percent complete: 60.9%; Average loss: 3.1060
Iteration: 2437; Percent complete: 60.9%; Average loss: 3.0346
Iteration: 2438; Percent complete: 61.0%; Average loss: 3.2020
Iteration: 2439; Percent complete: 61.0%; Average loss: 3.0024
Iteration: 2440; Percent complete: 61.0%; Average loss: 2.9441
Iteration: 2441; Percent complete: 61.0%; Average loss: 2.8701
Iteration: 2442; Percent complete: 61.1%; Average loss: 2.8215
Iteration: 2443; Percent complete: 61.1%; Average loss: 2.9818
Iteration: 2444; Percent complete: 61.1%; Average loss: 3.1159
Iteration: 2445; Percent complete: 61.1%; Average loss: 3.2607
Iteration: 2446; Percent complete: 61.2%; Average loss: 2.8827
Iteration: 2447; Percent complete: 61.2%; Average loss: 3.0259
Iteration: 2448; Percent complete: 61.2%; Average loss: 3.2241
Iteration: 2449; Percent complete: 61.2%; Average loss: 3.1164
Iteration: 2450; Percent complete: 61.3%; Average loss: 3.2640
Iteration: 2451; Percent complete: 61.3%; Average loss: 3.0615
Iteration: 2452; Percent complete: 61.3%; Average loss: 2.8338
Iteration: 2453; Percent complete: 61.3%; Average loss: 2.8303
Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0409
Iteration: 2455; Percent complete: 61.4%; Average loss: 3.0045
Iteration: 2456; Percent complete: 61.4%; Average loss: 3.1318
Iteration: 2457; Percent complete: 61.4%; Average loss: 2.8655
Iteration: 2458; Percent complete: 61.5%; Average loss: 3.0814
Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8012
Iteration: 2460; Percent complete: 61.5%; Average loss: 3.0414
Iteration: 2461; Percent complete: 61.5%; Average loss: 3.1442
Iteration: 2462; Percent complete: 61.6%; Average loss: 3.2332
Iteration: 2463; Percent complete: 61.6%; Average loss: 2.9251
Iteration: 2464; Percent complete: 61.6%; Average loss: 2.8538
Iteration: 2465; Percent complete: 61.6%; Average loss: 2.9422
Iteration: 2466; Percent complete: 61.7%; Average loss: 2.9963
Iteration: 2467; Percent complete: 61.7%; Average loss: 2.8299
Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0472
Iteration: 2469; Percent complete: 61.7%; Average loss: 3.0498
Iteration: 2470; Percent complete: 61.8%; Average loss: 3.0301
Iteration: 2471; Percent complete: 61.8%; Average loss: 3.0514
Iteration: 2472; Percent complete: 61.8%; Average loss: 2.8468
Iteration: 2473; Percent complete: 61.8%; Average loss: 3.0262
Iteration: 2474; Percent complete: 61.9%; Average loss: 3.3057
Iteration: 2475; Percent complete: 61.9%; Average loss: 3.0870
Iteration: 2476; Percent complete: 61.9%; Average loss: 2.9560
Iteration: 2477; Percent complete: 61.9%; Average loss: 3.1173
Iteration: 2478; Percent complete: 62.0%; Average loss: 2.9696
Iteration: 2479; Percent complete: 62.0%; Average loss: 3.0882
Iteration: 2480; Percent complete: 62.0%; Average loss: 2.9036
Iteration: 2481; Percent complete: 62.0%; Average loss: 2.8438
Iteration: 2482; Percent complete: 62.1%; Average loss: 2.9087
Iteration: 2483; Percent complete: 62.1%; Average loss: 2.9870
Iteration: 2484; Percent complete: 62.1%; Average loss: 2.8409
Iteration: 2485; Percent complete: 62.1%; Average loss: 2.6984
Iteration: 2486; Percent complete: 62.2%; Average loss: 3.2408
Iteration: 2487; Percent complete: 62.2%; Average loss: 3.3965
Iteration: 2488; Percent complete: 62.2%; Average loss: 2.9328
Iteration: 2489; Percent complete: 62.2%; Average loss: 3.2328
Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1974
Iteration: 2491; Percent complete: 62.3%; Average loss: 2.9166
Iteration: 2492; Percent complete: 62.3%; Average loss: 2.9093
Iteration: 2493; Percent complete: 62.3%; Average loss: 3.2048
Iteration: 2494; Percent complete: 62.4%; Average loss: 3.2080
Iteration: 2495; Percent complete: 62.4%; Average loss: 2.8005
Iteration: 2496; Percent complete: 62.4%; Average loss: 2.9831
Iteration: 2497; Percent complete: 62.4%; Average loss: 2.8872
Iteration: 2498; Percent complete: 62.5%; Average loss: 3.0982
Iteration: 2499; Percent complete: 62.5%; Average loss: 2.9772
Iteration: 2500; Percent complete: 62.5%; Average loss: 2.9129
Iteration: 2501; Percent complete: 62.5%; Average loss: 3.1914
Iteration: 2502; Percent complete: 62.5%; Average loss: 2.9057
Iteration: 2503; Percent complete: 62.6%; Average loss: 3.0291
Iteration: 2504; Percent complete: 62.6%; Average loss: 3.0584
Iteration: 2505; Percent complete: 62.6%; Average loss: 2.7385
Iteration: 2506; Percent complete: 62.6%; Average loss: 3.0077
Iteration: 2507; Percent complete: 62.7%; Average loss: 3.0227
Iteration: 2508; Percent complete: 62.7%; Average loss: 2.7585
Iteration: 2509; Percent complete: 62.7%; Average loss: 3.0279
Iteration: 2510; Percent complete: 62.7%; Average loss: 3.0508
Iteration: 2511; Percent complete: 62.8%; Average loss: 2.9631
Iteration: 2512; Percent complete: 62.8%; Average loss: 2.9503
Iteration: 2513; Percent complete: 62.8%; Average loss: 2.8562
Iteration: 2514; Percent complete: 62.8%; Average loss: 3.0678
Iteration: 2515; Percent complete: 62.9%; Average loss: 3.1440
Iteration: 2516; Percent complete: 62.9%; Average loss: 3.0209
Iteration: 2517; Percent complete: 62.9%; Average loss: 3.3226
Iteration: 2518; Percent complete: 62.9%; Average loss: 2.9999
Iteration: 2519; Percent complete: 63.0%; Average loss: 3.0301
Iteration: 2520; Percent complete: 63.0%; Average loss: 3.1240
Iteration: 2521; Percent complete: 63.0%; Average loss: 2.9352
Iteration: 2522; Percent complete: 63.0%; Average loss: 3.0491
Iteration: 2523; Percent complete: 63.1%; Average loss: 2.9446
Iteration: 2524; Percent complete: 63.1%; Average loss: 2.9660
Iteration: 2525; Percent complete: 63.1%; Average loss: 3.1855
Iteration: 2526; Percent complete: 63.1%; Average loss: 3.0386
Iteration: 2527; Percent complete: 63.2%; Average loss: 3.3887
Iteration: 2528; Percent complete: 63.2%; Average loss: 2.9359
Iteration: 2529; Percent complete: 63.2%; Average loss: 3.0279
Iteration: 2530; Percent complete: 63.2%; Average loss: 2.9547
Iteration: 2531; Percent complete: 63.3%; Average loss: 2.9999
Iteration: 2532; Percent complete: 63.3%; Average loss: 2.9955
Iteration: 2533; Percent complete: 63.3%; Average loss: 2.8532
Iteration: 2534; Percent complete: 63.3%; Average loss: 2.9681
Iteration: 2535; Percent complete: 63.4%; Average loss: 2.9210
Iteration: 2536; Percent complete: 63.4%; Average loss: 2.9640
Iteration: 2537; Percent complete: 63.4%; Average loss: 3.0895
Iteration: 2538; Percent complete: 63.4%; Average loss: 2.9255
Iteration: 2539; Percent complete: 63.5%; Average loss: 2.9719
Iteration: 2540; Percent complete: 63.5%; Average loss: 3.2343
Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0900
Iteration: 2542; Percent complete: 63.5%; Average loss: 2.8804
Iteration: 2543; Percent complete: 63.6%; Average loss: 3.0946
Iteration: 2544; Percent complete: 63.6%; Average loss: 3.0695
Iteration: 2545; Percent complete: 63.6%; Average loss: 3.0935
Iteration: 2546; Percent complete: 63.6%; Average loss: 3.1093
Iteration: 2547; Percent complete: 63.7%; Average loss: 3.1115
Iteration: 2548; Percent complete: 63.7%; Average loss: 3.2553
Iteration: 2549; Percent complete: 63.7%; Average loss: 3.0268
Iteration: 2550; Percent complete: 63.7%; Average loss: 3.1233
Iteration: 2551; Percent complete: 63.8%; Average loss: 2.7833
Iteration: 2552; Percent complete: 63.8%; Average loss: 2.9578
Iteration: 2553; Percent complete: 63.8%; Average loss: 2.9100
Iteration: 2554; Percent complete: 63.8%; Average loss: 2.9859
Iteration: 2555; Percent complete: 63.9%; Average loss: 2.9081
Iteration: 2556; Percent complete: 63.9%; Average loss: 2.9286
Iteration: 2557; Percent complete: 63.9%; Average loss: 2.9045
Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0830
Iteration: 2559; Percent complete: 64.0%; Average loss: 3.0736
Iteration: 2560; Percent complete: 64.0%; Average loss: 3.0131
Iteration: 2561; Percent complete: 64.0%; Average loss: 3.0530
Iteration: 2562; Percent complete: 64.0%; Average loss: 2.8951
Iteration: 2563; Percent complete: 64.1%; Average loss: 2.8008
Iteration: 2564; Percent complete: 64.1%; Average loss: 2.9658
Iteration: 2565; Percent complete: 64.1%; Average loss: 2.9901
Iteration: 2566; Percent complete: 64.1%; Average loss: 2.8729
Iteration: 2567; Percent complete: 64.2%; Average loss: 2.9667
Iteration: 2568; Percent complete: 64.2%; Average loss: 2.8002
Iteration: 2569; Percent complete: 64.2%; Average loss: 2.8990
Iteration: 2570; Percent complete: 64.2%; Average loss: 2.7801
Iteration: 2571; Percent complete: 64.3%; Average loss: 3.0152
Iteration: 2572; Percent complete: 64.3%; Average loss: 2.8626
Iteration: 2573; Percent complete: 64.3%; Average loss: 2.9350
Iteration: 2574; Percent complete: 64.3%; Average loss: 2.7444
Iteration: 2575; Percent complete: 64.4%; Average loss: 2.9900
Iteration: 2576; Percent complete: 64.4%; Average loss: 2.8845
Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9495
Iteration: 2578; Percent complete: 64.5%; Average loss: 3.0445
Iteration: 2579; Percent complete: 64.5%; Average loss: 3.1703
Iteration: 2580; Percent complete: 64.5%; Average loss: 2.9187
Iteration: 2581; Percent complete: 64.5%; Average loss: 2.9211
Iteration: 2582; Percent complete: 64.5%; Average loss: 2.7255
Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8864
Iteration: 2584; Percent complete: 64.6%; Average loss: 2.9791
Iteration: 2585; Percent complete: 64.6%; Average loss: 2.9048
Iteration: 2586; Percent complete: 64.6%; Average loss: 2.9839
Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0328
Iteration: 2588; Percent complete: 64.7%; Average loss: 2.9910
Iteration: 2589; Percent complete: 64.7%; Average loss: 2.9731
Iteration: 2590; Percent complete: 64.8%; Average loss: 2.8131
Iteration: 2591; Percent complete: 64.8%; Average loss: 2.7332
Iteration: 2592; Percent complete: 64.8%; Average loss: 3.1282
Iteration: 2593; Percent complete: 64.8%; Average loss: 2.8392
Iteration: 2594; Percent complete: 64.8%; Average loss: 2.9054
Iteration: 2595; Percent complete: 64.9%; Average loss: 3.1392
Iteration: 2596; Percent complete: 64.9%; Average loss: 2.8861
Iteration: 2597; Percent complete: 64.9%; Average loss: 3.0454
Iteration: 2598; Percent complete: 65.0%; Average loss: 2.9453
Iteration: 2599; Percent complete: 65.0%; Average loss: 2.8275
Iteration: 2600; Percent complete: 65.0%; Average loss: 2.7986
Iteration: 2601; Percent complete: 65.0%; Average loss: 3.1941
Iteration: 2602; Percent complete: 65.0%; Average loss: 3.0798
Iteration: 2603; Percent complete: 65.1%; Average loss: 2.9102
Iteration: 2604; Percent complete: 65.1%; Average loss: 2.9323
Iteration: 2605; Percent complete: 65.1%; Average loss: 2.9707
Iteration: 2606; Percent complete: 65.1%; Average loss: 2.8387
Iteration: 2607; Percent complete: 65.2%; Average loss: 2.7759
Iteration: 2608; Percent complete: 65.2%; Average loss: 3.1329
Iteration: 2609; Percent complete: 65.2%; Average loss: 2.6562
Iteration: 2610; Percent complete: 65.2%; Average loss: 3.0321
Iteration: 2611; Percent complete: 65.3%; Average loss: 2.8531
Iteration: 2612; Percent complete: 65.3%; Average loss: 3.0625
Iteration: 2613; Percent complete: 65.3%; Average loss: 3.0696
Iteration: 2614; Percent complete: 65.3%; Average loss: 3.1685
Iteration: 2615; Percent complete: 65.4%; Average loss: 2.9267
Iteration: 2616; Percent complete: 65.4%; Average loss: 2.8271
Iteration: 2617; Percent complete: 65.4%; Average loss: 3.0967
Iteration: 2618; Percent complete: 65.5%; Average loss: 3.2340
Iteration: 2619; Percent complete: 65.5%; Average loss: 2.9189
Iteration: 2620; Percent complete: 65.5%; Average loss: 2.9827
Iteration: 2621; Percent complete: 65.5%; Average loss: 2.7368
Iteration: 2622; Percent complete: 65.5%; Average loss: 3.1687
Iteration: 2623; Percent complete: 65.6%; Average loss: 2.9528
Iteration: 2624; Percent complete: 65.6%; Average loss: 2.8697
Iteration: 2625; Percent complete: 65.6%; Average loss: 2.9121
Iteration: 2626; Percent complete: 65.6%; Average loss: 2.8544
Iteration: 2627; Percent complete: 65.7%; Average loss: 2.8652
Iteration: 2628; Percent complete: 65.7%; Average loss: 2.9008
Iteration: 2629; Percent complete: 65.7%; Average loss: 2.8881
Iteration: 2630; Percent complete: 65.8%; Average loss: 2.9999
Iteration: 2631; Percent complete: 65.8%; Average loss: 2.8830
Iteration: 2632; Percent complete: 65.8%; Average loss: 2.6815
Iteration: 2633; Percent complete: 65.8%; Average loss: 3.1727
Iteration: 2634; Percent complete: 65.8%; Average loss: 2.8178
Iteration: 2635; Percent complete: 65.9%; Average loss: 2.9421
Iteration: 2636; Percent complete: 65.9%; Average loss: 2.9746
Iteration: 2637; Percent complete: 65.9%; Average loss: 2.8146
Iteration: 2638; Percent complete: 66.0%; Average loss: 3.1203
Iteration: 2639; Percent complete: 66.0%; Average loss: 2.8894
Iteration: 2640; Percent complete: 66.0%; Average loss: 2.9753
Iteration: 2641; Percent complete: 66.0%; Average loss: 2.9855
Iteration: 2642; Percent complete: 66.0%; Average loss: 2.9942
Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8798
Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9738
Iteration: 2645; Percent complete: 66.1%; Average loss: 2.8845
Iteration: 2646; Percent complete: 66.1%; Average loss: 2.8736
Iteration: 2647; Percent complete: 66.2%; Average loss: 2.8688
Iteration: 2648; Percent complete: 66.2%; Average loss: 3.0409
Iteration: 2649; Percent complete: 66.2%; Average loss: 3.0589
Iteration: 2650; Percent complete: 66.2%; Average loss: 3.1503
Iteration: 2651; Percent complete: 66.3%; Average loss: 3.2452
Iteration: 2652; Percent complete: 66.3%; Average loss: 2.9201
Iteration: 2653; Percent complete: 66.3%; Average loss: 3.0702
Iteration: 2654; Percent complete: 66.3%; Average loss: 2.6553
Iteration: 2655; Percent complete: 66.4%; Average loss: 2.8393
Iteration: 2656; Percent complete: 66.4%; Average loss: 2.9633
Iteration: 2657; Percent complete: 66.4%; Average loss: 2.9447
Iteration: 2658; Percent complete: 66.5%; Average loss: 3.0426
Iteration: 2659; Percent complete: 66.5%; Average loss: 3.0398
Iteration: 2660; Percent complete: 66.5%; Average loss: 2.6702
Iteration: 2661; Percent complete: 66.5%; Average loss: 3.1302
Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0367
Iteration: 2663; Percent complete: 66.6%; Average loss: 3.1019
Iteration: 2664; Percent complete: 66.6%; Average loss: 3.0334
Iteration: 2665; Percent complete: 66.6%; Average loss: 2.9883
Iteration: 2666; Percent complete: 66.6%; Average loss: 3.1682
Iteration: 2667; Percent complete: 66.7%; Average loss: 2.9807
Iteration: 2668; Percent complete: 66.7%; Average loss: 3.1469
Iteration: 2669; Percent complete: 66.7%; Average loss: 2.8150
Iteration: 2670; Percent complete: 66.8%; Average loss: 2.7562
Iteration: 2671; Percent complete: 66.8%; Average loss: 2.8710
Iteration: 2672; Percent complete: 66.8%; Average loss: 2.7706
Iteration: 2673; Percent complete: 66.8%; Average loss: 3.1310
Iteration: 2674; Percent complete: 66.8%; Average loss: 3.0804
Iteration: 2675; Percent complete: 66.9%; Average loss: 3.0093
Iteration: 2676; Percent complete: 66.9%; Average loss: 3.0626
Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9312
Iteration: 2678; Percent complete: 67.0%; Average loss: 3.0677
Iteration: 2679; Percent complete: 67.0%; Average loss: 2.9261
Iteration: 2680; Percent complete: 67.0%; Average loss: 2.9729
Iteration: 2681; Percent complete: 67.0%; Average loss: 2.8190
Iteration: 2682; Percent complete: 67.0%; Average loss: 2.8081
Iteration: 2683; Percent complete: 67.1%; Average loss: 3.1177
Iteration: 2684; Percent complete: 67.1%; Average loss: 2.8072
Iteration: 2685; Percent complete: 67.1%; Average loss: 2.9849
Iteration: 2686; Percent complete: 67.2%; Average loss: 2.9985
Iteration: 2687; Percent complete: 67.2%; Average loss: 2.8028
Iteration: 2688; Percent complete: 67.2%; Average loss: 2.8153
Iteration: 2689; Percent complete: 67.2%; Average loss: 3.1988
Iteration: 2690; Percent complete: 67.2%; Average loss: 2.9277
Iteration: 2691; Percent complete: 67.3%; Average loss: 2.9550
Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9950
Iteration: 2693; Percent complete: 67.3%; Average loss: 2.8758
Iteration: 2694; Percent complete: 67.3%; Average loss: 3.0470
Iteration: 2695; Percent complete: 67.4%; Average loss: 2.9088
Iteration: 2696; Percent complete: 67.4%; Average loss: 2.7458
Iteration: 2697; Percent complete: 67.4%; Average loss: 3.0239
Iteration: 2698; Percent complete: 67.5%; Average loss: 2.7880
Iteration: 2699; Percent complete: 67.5%; Average loss: 2.8983
Iteration: 2700; Percent complete: 67.5%; Average loss: 2.7738
Iteration: 2701; Percent complete: 67.5%; Average loss: 3.1415
Iteration: 2702; Percent complete: 67.5%; Average loss: 3.1330
Iteration: 2703; Percent complete: 67.6%; Average loss: 3.0684
Iteration: 2704; Percent complete: 67.6%; Average loss: 3.0295
Iteration: 2705; Percent complete: 67.6%; Average loss: 2.8672
Iteration: 2706; Percent complete: 67.7%; Average loss: 3.0025
Iteration: 2707; Percent complete: 67.7%; Average loss: 3.2653
Iteration: 2708; Percent complete: 67.7%; Average loss: 2.8975
Iteration: 2709; Percent complete: 67.7%; Average loss: 2.7278
Iteration: 2710; Percent complete: 67.8%; Average loss: 2.9024
Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8031
Iteration: 2712; Percent complete: 67.8%; Average loss: 2.7489
Iteration: 2713; Percent complete: 67.8%; Average loss: 3.0478
Iteration: 2714; Percent complete: 67.8%; Average loss: 2.9477
Iteration: 2715; Percent complete: 67.9%; Average loss: 2.8283
Iteration: 2716; Percent complete: 67.9%; Average loss: 3.1368
Iteration: 2717; Percent complete: 67.9%; Average loss: 3.0264
Iteration: 2718; Percent complete: 68.0%; Average loss: 2.9360
Iteration: 2719; Percent complete: 68.0%; Average loss: 2.7889
Iteration: 2720; Percent complete: 68.0%; Average loss: 2.8577
Iteration: 2721; Percent complete: 68.0%; Average loss: 2.9968
Iteration: 2722; Percent complete: 68.0%; Average loss: 2.8918
Iteration: 2723; Percent complete: 68.1%; Average loss: 2.8583
Iteration: 2724; Percent complete: 68.1%; Average loss: 3.0674
Iteration: 2725; Percent complete: 68.1%; Average loss: 2.8371
Iteration: 2726; Percent complete: 68.2%; Average loss: 2.8877
Iteration: 2727; Percent complete: 68.2%; Average loss: 2.9519
Iteration: 2728; Percent complete: 68.2%; Average loss: 2.8901
Iteration: 2729; Percent complete: 68.2%; Average loss: 2.7210
Iteration: 2730; Percent complete: 68.2%; Average loss: 2.8766
Iteration: 2731; Percent complete: 68.3%; Average loss: 2.7442
Iteration: 2732; Percent complete: 68.3%; Average loss: 2.9567
Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9745
Iteration: 2734; Percent complete: 68.3%; Average loss: 3.0360
Iteration: 2735; Percent complete: 68.4%; Average loss: 3.0243
Iteration: 2736; Percent complete: 68.4%; Average loss: 2.8520
Iteration: 2737; Percent complete: 68.4%; Average loss: 2.6008
Iteration: 2738; Percent complete: 68.5%; Average loss: 2.8274
Iteration: 2739; Percent complete: 68.5%; Average loss: 2.6957
Iteration: 2740; Percent complete: 68.5%; Average loss: 2.8264
Iteration: 2741; Percent complete: 68.5%; Average loss: 2.9238
Iteration: 2742; Percent complete: 68.5%; Average loss: 3.0727
Iteration: 2743; Percent complete: 68.6%; Average loss: 2.7475
Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0709
Iteration: 2745; Percent complete: 68.6%; Average loss: 2.7157
Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0245
Iteration: 2747; Percent complete: 68.7%; Average loss: 2.6467
Iteration: 2748; Percent complete: 68.7%; Average loss: 3.1862
Iteration: 2749; Percent complete: 68.7%; Average loss: 2.9379
Iteration: 2750; Percent complete: 68.8%; Average loss: 2.9341
Iteration: 2751; Percent complete: 68.8%; Average loss: 2.7753
Iteration: 2752; Percent complete: 68.8%; Average loss: 2.7836
Iteration: 2753; Percent complete: 68.8%; Average loss: 2.6840
Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8957
Iteration: 2755; Percent complete: 68.9%; Average loss: 2.8364
Iteration: 2756; Percent complete: 68.9%; Average loss: 2.9356
Iteration: 2757; Percent complete: 68.9%; Average loss: 2.9417
Iteration: 2758; Percent complete: 69.0%; Average loss: 2.8398
Iteration: 2759; Percent complete: 69.0%; Average loss: 2.7319
Iteration: 2760; Percent complete: 69.0%; Average loss: 3.0304
Iteration: 2761; Percent complete: 69.0%; Average loss: 3.0084
Iteration: 2762; Percent complete: 69.0%; Average loss: 2.9359
Iteration: 2763; Percent complete: 69.1%; Average loss: 2.9731
Iteration: 2764; Percent complete: 69.1%; Average loss: 2.9686
Iteration: 2765; Percent complete: 69.1%; Average loss: 2.9811
Iteration: 2766; Percent complete: 69.2%; Average loss: 2.9799
Iteration: 2767; Percent complete: 69.2%; Average loss: 2.9158
Iteration: 2768; Percent complete: 69.2%; Average loss: 2.8459
Iteration: 2769; Percent complete: 69.2%; Average loss: 3.0486
Iteration: 2770; Percent complete: 69.2%; Average loss: 2.8666
Iteration: 2771; Percent complete: 69.3%; Average loss: 3.0918
Iteration: 2772; Percent complete: 69.3%; Average loss: 3.0020
Iteration: 2773; Percent complete: 69.3%; Average loss: 2.8489
Iteration: 2774; Percent complete: 69.3%; Average loss: 2.9069
Iteration: 2775; Percent complete: 69.4%; Average loss: 2.9765
Iteration: 2776; Percent complete: 69.4%; Average loss: 3.2588
Iteration: 2777; Percent complete: 69.4%; Average loss: 2.8548
Iteration: 2778; Percent complete: 69.5%; Average loss: 3.1138
Iteration: 2779; Percent complete: 69.5%; Average loss: 2.8166
Iteration: 2780; Percent complete: 69.5%; Average loss: 3.1473
Iteration: 2781; Percent complete: 69.5%; Average loss: 2.8365
Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9437
Iteration: 2783; Percent complete: 69.6%; Average loss: 2.7398
Iteration: 2784; Percent complete: 69.6%; Average loss: 2.7197
Iteration: 2785; Percent complete: 69.6%; Average loss: 2.8796
Iteration: 2786; Percent complete: 69.7%; Average loss: 2.9841
Iteration: 2787; Percent complete: 69.7%; Average loss: 2.9497
Iteration: 2788; Percent complete: 69.7%; Average loss: 2.7980
Iteration: 2789; Percent complete: 69.7%; Average loss: 2.9899
Iteration: 2790; Percent complete: 69.8%; Average loss: 3.0079
Iteration: 2791; Percent complete: 69.8%; Average loss: 2.9373
Iteration: 2792; Percent complete: 69.8%; Average loss: 3.1041
Iteration: 2793; Percent complete: 69.8%; Average loss: 3.0986
Iteration: 2794; Percent complete: 69.8%; Average loss: 3.0010
Iteration: 2795; Percent complete: 69.9%; Average loss: 2.8871
Iteration: 2796; Percent complete: 69.9%; Average loss: 3.2586
Iteration: 2797; Percent complete: 69.9%; Average loss: 2.8085
Iteration: 2798; Percent complete: 70.0%; Average loss: 2.6728
Iteration: 2799; Percent complete: 70.0%; Average loss: 2.8755
Iteration: 2800; Percent complete: 70.0%; Average loss: 2.8159
Iteration: 2801; Percent complete: 70.0%; Average loss: 3.0292
Iteration: 2802; Percent complete: 70.0%; Average loss: 2.9316
Iteration: 2803; Percent complete: 70.1%; Average loss: 3.1479
Iteration: 2804; Percent complete: 70.1%; Average loss: 2.9683
Iteration: 2805; Percent complete: 70.1%; Average loss: 2.8638
Iteration: 2806; Percent complete: 70.2%; Average loss: 2.9433
Iteration: 2807; Percent complete: 70.2%; Average loss: 2.8236
Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0382
Iteration: 2809; Percent complete: 70.2%; Average loss: 2.7814
Iteration: 2810; Percent complete: 70.2%; Average loss: 3.0819
Iteration: 2811; Percent complete: 70.3%; Average loss: 2.8064
Iteration: 2812; Percent complete: 70.3%; Average loss: 2.9458
Iteration: 2813; Percent complete: 70.3%; Average loss: 2.9014
Iteration: 2814; Percent complete: 70.3%; Average loss: 3.1121
Iteration: 2815; Percent complete: 70.4%; Average loss: 3.0984
Iteration: 2816; Percent complete: 70.4%; Average loss: 2.9557
Iteration: 2817; Percent complete: 70.4%; Average loss: 2.8569
Iteration: 2818; Percent complete: 70.5%; Average loss: 3.0811
Iteration: 2819; Percent complete: 70.5%; Average loss: 2.6544
Iteration: 2820; Percent complete: 70.5%; Average loss: 2.9354
Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8446
Iteration: 2822; Percent complete: 70.5%; Average loss: 2.7498
Iteration: 2823; Percent complete: 70.6%; Average loss: 2.8866
Iteration: 2824; Percent complete: 70.6%; Average loss: 2.9357
Iteration: 2825; Percent complete: 70.6%; Average loss: 2.9605
Iteration: 2826; Percent complete: 70.7%; Average loss: 2.8304
Iteration: 2827; Percent complete: 70.7%; Average loss: 2.7846
Iteration: 2828; Percent complete: 70.7%; Average loss: 2.7882
Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7882
Iteration: 2830; Percent complete: 70.8%; Average loss: 2.9400
Iteration: 2831; Percent complete: 70.8%; Average loss: 2.9566
Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9406
Iteration: 2833; Percent complete: 70.8%; Average loss: 3.0429
Iteration: 2834; Percent complete: 70.9%; Average loss: 2.8389
Iteration: 2835; Percent complete: 70.9%; Average loss: 3.2425
Iteration: 2836; Percent complete: 70.9%; Average loss: 2.8406
Iteration: 2837; Percent complete: 70.9%; Average loss: 2.7849
Iteration: 2838; Percent complete: 71.0%; Average loss: 2.8239
Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8265
Iteration: 2840; Percent complete: 71.0%; Average loss: 3.2290
Iteration: 2841; Percent complete: 71.0%; Average loss: 2.8152
Iteration: 2842; Percent complete: 71.0%; Average loss: 3.0844
Iteration: 2843; Percent complete: 71.1%; Average loss: 3.1401
Iteration: 2844; Percent complete: 71.1%; Average loss: 3.0793
Iteration: 2845; Percent complete: 71.1%; Average loss: 2.9543
Iteration: 2846; Percent complete: 71.2%; Average loss: 2.8498
Iteration: 2847; Percent complete: 71.2%; Average loss: 2.8540
Iteration: 2848; Percent complete: 71.2%; Average loss: 2.6078
Iteration: 2849; Percent complete: 71.2%; Average loss: 2.8940
Iteration: 2850; Percent complete: 71.2%; Average loss: 2.7049
Iteration: 2851; Percent complete: 71.3%; Average loss: 2.8987
Iteration: 2852; Percent complete: 71.3%; Average loss: 3.0614
Iteration: 2853; Percent complete: 71.3%; Average loss: 3.0483
Iteration: 2854; Percent complete: 71.4%; Average loss: 2.8187
Iteration: 2855; Percent complete: 71.4%; Average loss: 2.9823
Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8002
Iteration: 2857; Percent complete: 71.4%; Average loss: 3.1534
Iteration: 2858; Percent complete: 71.5%; Average loss: 2.6970
Iteration: 2859; Percent complete: 71.5%; Average loss: 2.9319
Iteration: 2860; Percent complete: 71.5%; Average loss: 2.7853
Iteration: 2861; Percent complete: 71.5%; Average loss: 2.9957
Iteration: 2862; Percent complete: 71.5%; Average loss: 2.8167
Iteration: 2863; Percent complete: 71.6%; Average loss: 2.9745
Iteration: 2864; Percent complete: 71.6%; Average loss: 2.6964
Iteration: 2865; Percent complete: 71.6%; Average loss: 3.1352
Iteration: 2866; Percent complete: 71.7%; Average loss: 2.7170
Iteration: 2867; Percent complete: 71.7%; Average loss: 2.7501
Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0286
Iteration: 2869; Percent complete: 71.7%; Average loss: 2.7805
Iteration: 2870; Percent complete: 71.8%; Average loss: 3.1592
Iteration: 2871; Percent complete: 71.8%; Average loss: 2.9851
Iteration: 2872; Percent complete: 71.8%; Average loss: 2.9763
Iteration: 2873; Percent complete: 71.8%; Average loss: 2.9400
Iteration: 2874; Percent complete: 71.9%; Average loss: 2.6373
Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9755
Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9034
Iteration: 2877; Percent complete: 71.9%; Average loss: 3.1658
Iteration: 2878; Percent complete: 72.0%; Average loss: 2.6517
Iteration: 2879; Percent complete: 72.0%; Average loss: 2.9068
Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8022
Iteration: 2881; Percent complete: 72.0%; Average loss: 2.9855
Iteration: 2882; Percent complete: 72.0%; Average loss: 2.7719
Iteration: 2883; Percent complete: 72.1%; Average loss: 2.7170
Iteration: 2884; Percent complete: 72.1%; Average loss: 3.1602
Iteration: 2885; Percent complete: 72.1%; Average loss: 2.7449
Iteration: 2886; Percent complete: 72.2%; Average loss: 2.6777
Iteration: 2887; Percent complete: 72.2%; Average loss: 2.7038
Iteration: 2888; Percent complete: 72.2%; Average loss: 2.5039
Iteration: 2889; Percent complete: 72.2%; Average loss: 3.1117
Iteration: 2890; Percent complete: 72.2%; Average loss: 2.8514
Iteration: 2891; Percent complete: 72.3%; Average loss: 3.1396
Iteration: 2892; Percent complete: 72.3%; Average loss: 3.0822
Iteration: 2893; Percent complete: 72.3%; Average loss: 2.7226
Iteration: 2894; Percent complete: 72.4%; Average loss: 2.8082
Iteration: 2895; Percent complete: 72.4%; Average loss: 2.8619
Iteration: 2896; Percent complete: 72.4%; Average loss: 2.6440
Iteration: 2897; Percent complete: 72.4%; Average loss: 2.9831
Iteration: 2898; Percent complete: 72.5%; Average loss: 2.7650
Iteration: 2899; Percent complete: 72.5%; Average loss: 2.9007
Iteration: 2900; Percent complete: 72.5%; Average loss: 2.8064
Iteration: 2901; Percent complete: 72.5%; Average loss: 3.0079
Iteration: 2902; Percent complete: 72.5%; Average loss: 2.7787
Iteration: 2903; Percent complete: 72.6%; Average loss: 2.8581
Iteration: 2904; Percent complete: 72.6%; Average loss: 2.8286
Iteration: 2905; Percent complete: 72.6%; Average loss: 2.8013
Iteration: 2906; Percent complete: 72.7%; Average loss: 2.7526
Iteration: 2907; Percent complete: 72.7%; Average loss: 2.7314
Iteration: 2908; Percent complete: 72.7%; Average loss: 2.9287
Iteration: 2909; Percent complete: 72.7%; Average loss: 2.8544
Iteration: 2910; Percent complete: 72.8%; Average loss: 2.5582
Iteration: 2911; Percent complete: 72.8%; Average loss: 2.9952
Iteration: 2912; Percent complete: 72.8%; Average loss: 3.1687
Iteration: 2913; Percent complete: 72.8%; Average loss: 3.0786
Iteration: 2914; Percent complete: 72.9%; Average loss: 2.9636
Iteration: 2915; Percent complete: 72.9%; Average loss: 2.6720
Iteration: 2916; Percent complete: 72.9%; Average loss: 2.8198
Iteration: 2917; Percent complete: 72.9%; Average loss: 2.9642
Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9419
Iteration: 2919; Percent complete: 73.0%; Average loss: 2.9781
Iteration: 2920; Percent complete: 73.0%; Average loss: 3.0168
Iteration: 2921; Percent complete: 73.0%; Average loss: 3.0095
Iteration: 2922; Percent complete: 73.0%; Average loss: 2.8079
Iteration: 2923; Percent complete: 73.1%; Average loss: 2.7020
Iteration: 2924; Percent complete: 73.1%; Average loss: 2.9236
Iteration: 2925; Percent complete: 73.1%; Average loss: 2.9568
Iteration: 2926; Percent complete: 73.2%; Average loss: 2.9201
Iteration: 2927; Percent complete: 73.2%; Average loss: 2.8321
Iteration: 2928; Percent complete: 73.2%; Average loss: 2.8426
Iteration: 2929; Percent complete: 73.2%; Average loss: 2.7244
Iteration: 2930; Percent complete: 73.2%; Average loss: 3.1746
Iteration: 2931; Percent complete: 73.3%; Average loss: 2.8120
Iteration: 2932; Percent complete: 73.3%; Average loss: 2.8925
Iteration: 2933; Percent complete: 73.3%; Average loss: 2.7976
Iteration: 2934; Percent complete: 73.4%; Average loss: 2.9082
Iteration: 2935; Percent complete: 73.4%; Average loss: 3.0257
Iteration: 2936; Percent complete: 73.4%; Average loss: 2.9670
Iteration: 2937; Percent complete: 73.4%; Average loss: 2.7480
Iteration: 2938; Percent complete: 73.5%; Average loss: 3.0253
Iteration: 2939; Percent complete: 73.5%; Average loss: 2.7017
Iteration: 2940; Percent complete: 73.5%; Average loss: 2.9113
Iteration: 2941; Percent complete: 73.5%; Average loss: 2.7552
Iteration: 2942; Percent complete: 73.6%; Average loss: 2.8510
Iteration: 2943; Percent complete: 73.6%; Average loss: 2.8661
Iteration: 2944; Percent complete: 73.6%; Average loss: 2.9398
Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8343
Iteration: 2946; Percent complete: 73.7%; Average loss: 2.6525
Iteration: 2947; Percent complete: 73.7%; Average loss: 2.5211
Iteration: 2948; Percent complete: 73.7%; Average loss: 2.7524
Iteration: 2949; Percent complete: 73.7%; Average loss: 3.1576
Iteration: 2950; Percent complete: 73.8%; Average loss: 2.8225
Iteration: 2951; Percent complete: 73.8%; Average loss: 2.8445
Iteration: 2952; Percent complete: 73.8%; Average loss: 2.8899
Iteration: 2953; Percent complete: 73.8%; Average loss: 2.8421
Iteration: 2954; Percent complete: 73.9%; Average loss: 3.0457
Iteration: 2955; Percent complete: 73.9%; Average loss: 2.9299
Iteration: 2956; Percent complete: 73.9%; Average loss: 2.8171
Iteration: 2957; Percent complete: 73.9%; Average loss: 3.0635
Iteration: 2958; Percent complete: 74.0%; Average loss: 2.9053
Iteration: 2959; Percent complete: 74.0%; Average loss: 2.9277
Iteration: 2960; Percent complete: 74.0%; Average loss: 2.9178
Iteration: 2961; Percent complete: 74.0%; Average loss: 2.7771
Iteration: 2962; Percent complete: 74.1%; Average loss: 2.8880
Iteration: 2963; Percent complete: 74.1%; Average loss: 2.5963
Iteration: 2964; Percent complete: 74.1%; Average loss: 2.9342
Iteration: 2965; Percent complete: 74.1%; Average loss: 2.6302
Iteration: 2966; Percent complete: 74.2%; Average loss: 2.8868
Iteration: 2967; Percent complete: 74.2%; Average loss: 2.7839
Iteration: 2968; Percent complete: 74.2%; Average loss: 2.7500
Iteration: 2969; Percent complete: 74.2%; Average loss: 2.6621
Iteration: 2970; Percent complete: 74.2%; Average loss: 2.6952
Iteration: 2971; Percent complete: 74.3%; Average loss: 2.9189
Iteration: 2972; Percent complete: 74.3%; Average loss: 2.6669
Iteration: 2973; Percent complete: 74.3%; Average loss: 2.6923
Iteration: 2974; Percent complete: 74.4%; Average loss: 2.8408
Iteration: 2975; Percent complete: 74.4%; Average loss: 2.9852
Iteration: 2976; Percent complete: 74.4%; Average loss: 2.9194
Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9247
Iteration: 2978; Percent complete: 74.5%; Average loss: 2.6199
Iteration: 2979; Percent complete: 74.5%; Average loss: 2.7579
Iteration: 2980; Percent complete: 74.5%; Average loss: 2.6471
Iteration: 2981; Percent complete: 74.5%; Average loss: 2.8016
Iteration: 2982; Percent complete: 74.6%; Average loss: 3.0183
Iteration: 2983; Percent complete: 74.6%; Average loss: 3.0220
Iteration: 2984; Percent complete: 74.6%; Average loss: 2.7912
Iteration: 2985; Percent complete: 74.6%; Average loss: 2.8625
Iteration: 2986; Percent complete: 74.7%; Average loss: 3.0264
Iteration: 2987; Percent complete: 74.7%; Average loss: 3.0023
Iteration: 2988; Percent complete: 74.7%; Average loss: 2.8315
Iteration: 2989; Percent complete: 74.7%; Average loss: 2.8683
Iteration: 2990; Percent complete: 74.8%; Average loss: 2.6501
Iteration: 2991; Percent complete: 74.8%; Average loss: 2.8008
Iteration: 2992; Percent complete: 74.8%; Average loss: 2.8991
Iteration: 2993; Percent complete: 74.8%; Average loss: 2.8604
Iteration: 2994; Percent complete: 74.9%; Average loss: 3.0076
Iteration: 2995; Percent complete: 74.9%; Average loss: 2.9355
Iteration: 2996; Percent complete: 74.9%; Average loss: 2.8431
Iteration: 2997; Percent complete: 74.9%; Average loss: 2.9188
Iteration: 2998; Percent complete: 75.0%; Average loss: 2.9510
Iteration: 2999; Percent complete: 75.0%; Average loss: 2.9275
Iteration: 3000; Percent complete: 75.0%; Average loss: 3.0060
Iteration: 3001; Percent complete: 75.0%; Average loss: 2.9529
Iteration: 3002; Percent complete: 75.0%; Average loss: 2.7111
Iteration: 3003; Percent complete: 75.1%; Average loss: 2.8301
Iteration: 3004; Percent complete: 75.1%; Average loss: 3.0234
Iteration: 3005; Percent complete: 75.1%; Average loss: 2.8221
Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8186
Iteration: 3007; Percent complete: 75.2%; Average loss: 2.7513
Iteration: 3008; Percent complete: 75.2%; Average loss: 2.8156
Iteration: 3009; Percent complete: 75.2%; Average loss: 2.9508
Iteration: 3010; Percent complete: 75.2%; Average loss: 2.8960
Iteration: 3011; Percent complete: 75.3%; Average loss: 2.7210
Iteration: 3012; Percent complete: 75.3%; Average loss: 2.6701
Iteration: 3013; Percent complete: 75.3%; Average loss: 2.8689
Iteration: 3014; Percent complete: 75.3%; Average loss: 2.8210
Iteration: 3015; Percent complete: 75.4%; Average loss: 2.7349
Iteration: 3016; Percent complete: 75.4%; Average loss: 2.9228
Iteration: 3017; Percent complete: 75.4%; Average loss: 2.8660
Iteration: 3018; Percent complete: 75.4%; Average loss: 2.9891
Iteration: 3019; Percent complete: 75.5%; Average loss: 2.7715
Iteration: 3020; Percent complete: 75.5%; Average loss: 2.8156
Iteration: 3021; Percent complete: 75.5%; Average loss: 2.8722
Iteration: 3022; Percent complete: 75.5%; Average loss: 2.8501
Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8980
Iteration: 3024; Percent complete: 75.6%; Average loss: 2.9378
Iteration: 3025; Percent complete: 75.6%; Average loss: 2.7746
Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8938
Iteration: 3027; Percent complete: 75.7%; Average loss: 2.8516
Iteration: 3028; Percent complete: 75.7%; Average loss: 2.6298
Iteration: 3029; Percent complete: 75.7%; Average loss: 2.9437
Iteration: 3030; Percent complete: 75.8%; Average loss: 2.6006
Iteration: 3031; Percent complete: 75.8%; Average loss: 3.0702
Iteration: 3032; Percent complete: 75.8%; Average loss: 2.7178
Iteration: 3033; Percent complete: 75.8%; Average loss: 2.9496
Iteration: 3034; Percent complete: 75.8%; Average loss: 3.1546
Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7585
Iteration: 3036; Percent complete: 75.9%; Average loss: 2.8012
Iteration: 3037; Percent complete: 75.9%; Average loss: 3.0406
Iteration: 3038; Percent complete: 75.9%; Average loss: 2.9049
Iteration: 3039; Percent complete: 76.0%; Average loss: 2.8734
Iteration: 3040; Percent complete: 76.0%; Average loss: 2.8572
Iteration: 3041; Percent complete: 76.0%; Average loss: 3.0013
Iteration: 3042; Percent complete: 76.0%; Average loss: 2.6921
Iteration: 3043; Percent complete: 76.1%; Average loss: 2.5255
Iteration: 3044; Percent complete: 76.1%; Average loss: 2.8535
Iteration: 3045; Percent complete: 76.1%; Average loss: 2.9725
Iteration: 3046; Percent complete: 76.1%; Average loss: 3.0562
Iteration: 3047; Percent complete: 76.2%; Average loss: 2.7737
Iteration: 3048; Percent complete: 76.2%; Average loss: 2.6513
Iteration: 3049; Percent complete: 76.2%; Average loss: 2.8432
Iteration: 3050; Percent complete: 76.2%; Average loss: 2.9105
Iteration: 3051; Percent complete: 76.3%; Average loss: 2.3728
Iteration: 3052; Percent complete: 76.3%; Average loss: 2.8505
Iteration: 3053; Percent complete: 76.3%; Average loss: 2.5920
Iteration: 3054; Percent complete: 76.3%; Average loss: 2.7074
Iteration: 3055; Percent complete: 76.4%; Average loss: 2.9091
Iteration: 3056; Percent complete: 76.4%; Average loss: 3.0633
Iteration: 3057; Percent complete: 76.4%; Average loss: 2.8975
Iteration: 3058; Percent complete: 76.4%; Average loss: 2.9128
Iteration: 3059; Percent complete: 76.5%; Average loss: 2.8197
Iteration: 3060; Percent complete: 76.5%; Average loss: 2.8819
Iteration: 3061; Percent complete: 76.5%; Average loss: 3.0454
Iteration: 3062; Percent complete: 76.5%; Average loss: 2.8857
Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8192
Iteration: 3064; Percent complete: 76.6%; Average loss: 2.9674
Iteration: 3065; Percent complete: 76.6%; Average loss: 2.8905
Iteration: 3066; Percent complete: 76.6%; Average loss: 2.8115
Iteration: 3067; Percent complete: 76.7%; Average loss: 2.9582
Iteration: 3068; Percent complete: 76.7%; Average loss: 2.8019
Iteration: 3069; Percent complete: 76.7%; Average loss: 2.8745
Iteration: 3070; Percent complete: 76.8%; Average loss: 2.6480
Iteration: 3071; Percent complete: 76.8%; Average loss: 2.6668
Iteration: 3072; Percent complete: 76.8%; Average loss: 2.9777
Iteration: 3073; Percent complete: 76.8%; Average loss: 2.8275
Iteration: 3074; Percent complete: 76.8%; Average loss: 2.6863
Iteration: 3075; Percent complete: 76.9%; Average loss: 2.6882
Iteration: 3076; Percent complete: 76.9%; Average loss: 2.7825
Iteration: 3077; Percent complete: 76.9%; Average loss: 2.8158
Iteration: 3078; Percent complete: 77.0%; Average loss: 2.8235
Iteration: 3079; Percent complete: 77.0%; Average loss: 2.7510
Iteration: 3080; Percent complete: 77.0%; Average loss: 2.9217
Iteration: 3081; Percent complete: 77.0%; Average loss: 2.7219
Iteration: 3082; Percent complete: 77.0%; Average loss: 2.8807
Iteration: 3083; Percent complete: 77.1%; Average loss: 2.8891
Iteration: 3084; Percent complete: 77.1%; Average loss: 2.9973
Iteration: 3085; Percent complete: 77.1%; Average loss: 2.7425
Iteration: 3086; Percent complete: 77.1%; Average loss: 2.9492
Iteration: 3087; Percent complete: 77.2%; Average loss: 2.8586
Iteration: 3088; Percent complete: 77.2%; Average loss: 3.0360
Iteration: 3089; Percent complete: 77.2%; Average loss: 2.9818
Iteration: 3090; Percent complete: 77.2%; Average loss: 2.9994
Iteration: 3091; Percent complete: 77.3%; Average loss: 3.0016
Iteration: 3092; Percent complete: 77.3%; Average loss: 2.6710
Iteration: 3093; Percent complete: 77.3%; Average loss: 2.9742
Iteration: 3094; Percent complete: 77.3%; Average loss: 3.1235
Iteration: 3095; Percent complete: 77.4%; Average loss: 3.0099
Iteration: 3096; Percent complete: 77.4%; Average loss: 2.6282
Iteration: 3097; Percent complete: 77.4%; Average loss: 2.7179
Iteration: 3098; Percent complete: 77.5%; Average loss: 3.0358
Iteration: 3099; Percent complete: 77.5%; Average loss: 2.7024
Iteration: 3100; Percent complete: 77.5%; Average loss: 3.0760
Iteration: 3101; Percent complete: 77.5%; Average loss: 2.6837
Iteration: 3102; Percent complete: 77.5%; Average loss: 2.6250
Iteration: 3103; Percent complete: 77.6%; Average loss: 2.8224
Iteration: 3104; Percent complete: 77.6%; Average loss: 2.7084
Iteration: 3105; Percent complete: 77.6%; Average loss: 2.6996
Iteration: 3106; Percent complete: 77.6%; Average loss: 2.7561
Iteration: 3107; Percent complete: 77.7%; Average loss: 2.7064
Iteration: 3108; Percent complete: 77.7%; Average loss: 2.7237
Iteration: 3109; Percent complete: 77.7%; Average loss: 2.6985
Iteration: 3110; Percent complete: 77.8%; Average loss: 2.8581
Iteration: 3111; Percent complete: 77.8%; Average loss: 2.6321
Iteration: 3112; Percent complete: 77.8%; Average loss: 2.7278
Iteration: 3113; Percent complete: 77.8%; Average loss: 2.8196
Iteration: 3114; Percent complete: 77.8%; Average loss: 2.6472
Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7363
Iteration: 3116; Percent complete: 77.9%; Average loss: 2.7015
Iteration: 3117; Percent complete: 77.9%; Average loss: 2.9535
Iteration: 3118; Percent complete: 78.0%; Average loss: 3.1782
Iteration: 3119; Percent complete: 78.0%; Average loss: 2.7767
Iteration: 3120; Percent complete: 78.0%; Average loss: 2.6673
Iteration: 3121; Percent complete: 78.0%; Average loss: 2.7086
Iteration: 3122; Percent complete: 78.0%; Average loss: 2.8879
Iteration: 3123; Percent complete: 78.1%; Average loss: 2.8007
Iteration: 3124; Percent complete: 78.1%; Average loss: 2.9647
Iteration: 3125; Percent complete: 78.1%; Average loss: 2.6398
Iteration: 3126; Percent complete: 78.1%; Average loss: 2.7181
Iteration: 3127; Percent complete: 78.2%; Average loss: 2.8006
Iteration: 3128; Percent complete: 78.2%; Average loss: 2.5524
Iteration: 3129; Percent complete: 78.2%; Average loss: 2.6819
Iteration: 3130; Percent complete: 78.2%; Average loss: 2.6739
Iteration: 3131; Percent complete: 78.3%; Average loss: 2.9455
Iteration: 3132; Percent complete: 78.3%; Average loss: 2.8397
Iteration: 3133; Percent complete: 78.3%; Average loss: 2.6455
Iteration: 3134; Percent complete: 78.3%; Average loss: 2.6042
Iteration: 3135; Percent complete: 78.4%; Average loss: 2.9393
Iteration: 3136; Percent complete: 78.4%; Average loss: 2.8492
Iteration: 3137; Percent complete: 78.4%; Average loss: 2.8376
Iteration: 3138; Percent complete: 78.5%; Average loss: 2.5904
Iteration: 3139; Percent complete: 78.5%; Average loss: 2.8958
Iteration: 3140; Percent complete: 78.5%; Average loss: 2.8174
Iteration: 3141; Percent complete: 78.5%; Average loss: 2.8172
Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8837
Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8635
Iteration: 3144; Percent complete: 78.6%; Average loss: 2.8880
Iteration: 3145; Percent complete: 78.6%; Average loss: 2.8987
Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7776
Iteration: 3147; Percent complete: 78.7%; Average loss: 2.8812
Iteration: 3148; Percent complete: 78.7%; Average loss: 2.7024
Iteration: 3149; Percent complete: 78.7%; Average loss: 2.6039
Iteration: 3150; Percent complete: 78.8%; Average loss: 2.9784
Iteration: 3151; Percent complete: 78.8%; Average loss: 2.8091
Iteration: 3152; Percent complete: 78.8%; Average loss: 2.8348
Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6893
Iteration: 3154; Percent complete: 78.8%; Average loss: 3.0368
Iteration: 3155; Percent complete: 78.9%; Average loss: 2.5063
Iteration: 3156; Percent complete: 78.9%; Average loss: 2.8985
Iteration: 3157; Percent complete: 78.9%; Average loss: 2.8715
Iteration: 3158; Percent complete: 79.0%; Average loss: 2.7545
Iteration: 3159; Percent complete: 79.0%; Average loss: 2.7262
Iteration: 3160; Percent complete: 79.0%; Average loss: 2.8158
Iteration: 3161; Percent complete: 79.0%; Average loss: 2.8369
Iteration: 3162; Percent complete: 79.0%; Average loss: 2.5169
Iteration: 3163; Percent complete: 79.1%; Average loss: 2.4944
Iteration: 3164; Percent complete: 79.1%; Average loss: 2.7119
Iteration: 3165; Percent complete: 79.1%; Average loss: 2.7614
Iteration: 3166; Percent complete: 79.1%; Average loss: 2.8996
Iteration: 3167; Percent complete: 79.2%; Average loss: 2.7654
Iteration: 3168; Percent complete: 79.2%; Average loss: 2.7208
Iteration: 3169; Percent complete: 79.2%; Average loss: 3.0231
Iteration: 3170; Percent complete: 79.2%; Average loss: 2.7967
Iteration: 3171; Percent complete: 79.3%; Average loss: 3.0161
Iteration: 3172; Percent complete: 79.3%; Average loss: 2.8374
Iteration: 3173; Percent complete: 79.3%; Average loss: 2.7228
Iteration: 3174; Percent complete: 79.3%; Average loss: 2.9901
Iteration: 3175; Percent complete: 79.4%; Average loss: 2.6782
Iteration: 3176; Percent complete: 79.4%; Average loss: 2.7575
Iteration: 3177; Percent complete: 79.4%; Average loss: 2.7289
Iteration: 3178; Percent complete: 79.5%; Average loss: 2.8437
Iteration: 3179; Percent complete: 79.5%; Average loss: 2.9820
Iteration: 3180; Percent complete: 79.5%; Average loss: 2.7002
Iteration: 3181; Percent complete: 79.5%; Average loss: 2.7631
Iteration: 3182; Percent complete: 79.5%; Average loss: 2.8601
Iteration: 3183; Percent complete: 79.6%; Average loss: 2.7080
Iteration: 3184; Percent complete: 79.6%; Average loss: 2.9672
Iteration: 3185; Percent complete: 79.6%; Average loss: 2.8603
Iteration: 3186; Percent complete: 79.7%; Average loss: 2.9738
Iteration: 3187; Percent complete: 79.7%; Average loss: 2.7065
Iteration: 3188; Percent complete: 79.7%; Average loss: 2.8564
Iteration: 3189; Percent complete: 79.7%; Average loss: 2.8130
Iteration: 3190; Percent complete: 79.8%; Average loss: 2.6340
Iteration: 3191; Percent complete: 79.8%; Average loss: 3.0834
Iteration: 3192; Percent complete: 79.8%; Average loss: 2.7536
Iteration: 3193; Percent complete: 79.8%; Average loss: 2.8511
Iteration: 3194; Percent complete: 79.8%; Average loss: 2.8461
Iteration: 3195; Percent complete: 79.9%; Average loss: 2.8772
Iteration: 3196; Percent complete: 79.9%; Average loss: 2.6728
Iteration: 3197; Percent complete: 79.9%; Average loss: 3.0271
Iteration: 3198; Percent complete: 80.0%; Average loss: 2.9013
Iteration: 3199; Percent complete: 80.0%; Average loss: 3.0376
Iteration: 3200; Percent complete: 80.0%; Average loss: 2.9687
Iteration: 3201; Percent complete: 80.0%; Average loss: 2.9780
Iteration: 3202; Percent complete: 80.0%; Average loss: 2.5884
Iteration: 3203; Percent complete: 80.1%; Average loss: 2.8782
Iteration: 3204; Percent complete: 80.1%; Average loss: 2.7178
Iteration: 3205; Percent complete: 80.1%; Average loss: 2.7631
Iteration: 3206; Percent complete: 80.2%; Average loss: 2.9393
Iteration: 3207; Percent complete: 80.2%; Average loss: 2.5161
Iteration: 3208; Percent complete: 80.2%; Average loss: 3.0023
Iteration: 3209; Percent complete: 80.2%; Average loss: 2.7596
Iteration: 3210; Percent complete: 80.2%; Average loss: 2.6243
Iteration: 3211; Percent complete: 80.3%; Average loss: 2.7260
Iteration: 3212; Percent complete: 80.3%; Average loss: 2.7088
Iteration: 3213; Percent complete: 80.3%; Average loss: 2.6878
Iteration: 3214; Percent complete: 80.3%; Average loss: 2.8459
Iteration: 3215; Percent complete: 80.4%; Average loss: 2.5773
Iteration: 3216; Percent complete: 80.4%; Average loss: 2.5738
Iteration: 3217; Percent complete: 80.4%; Average loss: 2.6398
Iteration: 3218; Percent complete: 80.5%; Average loss: 2.7384
Iteration: 3219; Percent complete: 80.5%; Average loss: 2.8072
Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8464
Iteration: 3221; Percent complete: 80.5%; Average loss: 2.7307
Iteration: 3222; Percent complete: 80.5%; Average loss: 2.7129
Iteration: 3223; Percent complete: 80.6%; Average loss: 2.8768
Iteration: 3224; Percent complete: 80.6%; Average loss: 2.5498
Iteration: 3225; Percent complete: 80.6%; Average loss: 2.6411
Iteration: 3226; Percent complete: 80.7%; Average loss: 2.7934
Iteration: 3227; Percent complete: 80.7%; Average loss: 2.9803
Iteration: 3228; Percent complete: 80.7%; Average loss: 2.9562
Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7660
Iteration: 3230; Percent complete: 80.8%; Average loss: 2.9130
Iteration: 3231; Percent complete: 80.8%; Average loss: 2.7207
Iteration: 3232; Percent complete: 80.8%; Average loss: 2.8232
Iteration: 3233; Percent complete: 80.8%; Average loss: 2.9088
Iteration: 3234; Percent complete: 80.8%; Average loss: 2.9734
Iteration: 3235; Percent complete: 80.9%; Average loss: 2.6747
Iteration: 3236; Percent complete: 80.9%; Average loss: 2.8179
Iteration: 3237; Percent complete: 80.9%; Average loss: 2.9456
Iteration: 3238; Percent complete: 81.0%; Average loss: 2.6793
Iteration: 3239; Percent complete: 81.0%; Average loss: 2.6358
Iteration: 3240; Percent complete: 81.0%; Average loss: 3.0437
Iteration: 3241; Percent complete: 81.0%; Average loss: 2.5300
Iteration: 3242; Percent complete: 81.0%; Average loss: 2.9246
Iteration: 3243; Percent complete: 81.1%; Average loss: 2.7378
Iteration: 3244; Percent complete: 81.1%; Average loss: 2.9438
Iteration: 3245; Percent complete: 81.1%; Average loss: 3.1256
Iteration: 3246; Percent complete: 81.2%; Average loss: 2.5883
Iteration: 3247; Percent complete: 81.2%; Average loss: 2.5367
Iteration: 3248; Percent complete: 81.2%; Average loss: 2.7274
Iteration: 3249; Percent complete: 81.2%; Average loss: 2.7132
Iteration: 3250; Percent complete: 81.2%; Average loss: 2.9266
Iteration: 3251; Percent complete: 81.3%; Average loss: 2.7650
Iteration: 3252; Percent complete: 81.3%; Average loss: 2.9165
Iteration: 3253; Percent complete: 81.3%; Average loss: 2.8508
Iteration: 3254; Percent complete: 81.3%; Average loss: 2.9833
Iteration: 3255; Percent complete: 81.4%; Average loss: 2.6651
Iteration: 3256; Percent complete: 81.4%; Average loss: 3.0203
Iteration: 3257; Percent complete: 81.4%; Average loss: 2.8469
Iteration: 3258; Percent complete: 81.5%; Average loss: 2.8080
Iteration: 3259; Percent complete: 81.5%; Average loss: 2.7520
Iteration: 3260; Percent complete: 81.5%; Average loss: 2.9630
Iteration: 3261; Percent complete: 81.5%; Average loss: 2.7839
Iteration: 3262; Percent complete: 81.5%; Average loss: 2.8871
Iteration: 3263; Percent complete: 81.6%; Average loss: 2.8670
Iteration: 3264; Percent complete: 81.6%; Average loss: 2.9236
Iteration: 3265; Percent complete: 81.6%; Average loss: 2.8445
Iteration: 3266; Percent complete: 81.7%; Average loss: 2.9562
Iteration: 3267; Percent complete: 81.7%; Average loss: 2.7594
Iteration: 3268; Percent complete: 81.7%; Average loss: 2.6067
Iteration: 3269; Percent complete: 81.7%; Average loss: 2.6027
Iteration: 3270; Percent complete: 81.8%; Average loss: 2.8866
Iteration: 3271; Percent complete: 81.8%; Average loss: 2.8243
Iteration: 3272; Percent complete: 81.8%; Average loss: 2.8899
Iteration: 3273; Percent complete: 81.8%; Average loss: 2.8559
Iteration: 3274; Percent complete: 81.8%; Average loss: 2.6377
Iteration: 3275; Percent complete: 81.9%; Average loss: 2.7951
Iteration: 3276; Percent complete: 81.9%; Average loss: 2.9363
Iteration: 3277; Percent complete: 81.9%; Average loss: 2.9483
Iteration: 3278; Percent complete: 82.0%; Average loss: 2.8836
Iteration: 3279; Percent complete: 82.0%; Average loss: 2.6772
Iteration: 3280; Percent complete: 82.0%; Average loss: 2.7886
Iteration: 3281; Percent complete: 82.0%; Average loss: 2.8470
Iteration: 3282; Percent complete: 82.0%; Average loss: 2.8122
Iteration: 3283; Percent complete: 82.1%; Average loss: 2.8848
Iteration: 3284; Percent complete: 82.1%; Average loss: 2.6925
Iteration: 3285; Percent complete: 82.1%; Average loss: 2.7230
Iteration: 3286; Percent complete: 82.2%; Average loss: 2.6903
Iteration: 3287; Percent complete: 82.2%; Average loss: 2.8669
Iteration: 3288; Percent complete: 82.2%; Average loss: 2.8743
Iteration: 3289; Percent complete: 82.2%; Average loss: 2.8267
Iteration: 3290; Percent complete: 82.2%; Average loss: 2.8058
Iteration: 3291; Percent complete: 82.3%; Average loss: 2.7867
Iteration: 3292; Percent complete: 82.3%; Average loss: 2.9304
Iteration: 3293; Percent complete: 82.3%; Average loss: 2.8491
Iteration: 3294; Percent complete: 82.3%; Average loss: 2.6964
Iteration: 3295; Percent complete: 82.4%; Average loss: 2.6711
Iteration: 3296; Percent complete: 82.4%; Average loss: 2.9123
Iteration: 3297; Percent complete: 82.4%; Average loss: 2.7870
Iteration: 3298; Percent complete: 82.5%; Average loss: 2.6157
Iteration: 3299; Percent complete: 82.5%; Average loss: 2.6615
Iteration: 3300; Percent complete: 82.5%; Average loss: 2.8601
Iteration: 3301; Percent complete: 82.5%; Average loss: 2.5016
Iteration: 3302; Percent complete: 82.5%; Average loss: 2.8562
Iteration: 3303; Percent complete: 82.6%; Average loss: 2.7267
Iteration: 3304; Percent complete: 82.6%; Average loss: 2.8227
Iteration: 3305; Percent complete: 82.6%; Average loss: 2.5978
Iteration: 3306; Percent complete: 82.7%; Average loss: 2.7513
Iteration: 3307; Percent complete: 82.7%; Average loss: 2.6801
Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7770
Iteration: 3309; Percent complete: 82.7%; Average loss: 3.0093
Iteration: 3310; Percent complete: 82.8%; Average loss: 2.9351
Iteration: 3311; Percent complete: 82.8%; Average loss: 2.8551
Iteration: 3312; Percent complete: 82.8%; Average loss: 2.6693
Iteration: 3313; Percent complete: 82.8%; Average loss: 2.8588
Iteration: 3314; Percent complete: 82.8%; Average loss: 2.8619
Iteration: 3315; Percent complete: 82.9%; Average loss: 2.9191
Iteration: 3316; Percent complete: 82.9%; Average loss: 2.8666
Iteration: 3317; Percent complete: 82.9%; Average loss: 2.8266
Iteration: 3318; Percent complete: 83.0%; Average loss: 2.8659
Iteration: 3319; Percent complete: 83.0%; Average loss: 2.8434
Iteration: 3320; Percent complete: 83.0%; Average loss: 2.9426
Iteration: 3321; Percent complete: 83.0%; Average loss: 2.6485
Iteration: 3322; Percent complete: 83.0%; Average loss: 3.0274
Iteration: 3323; Percent complete: 83.1%; Average loss: 2.6847
Iteration: 3324; Percent complete: 83.1%; Average loss: 2.8848
Iteration: 3325; Percent complete: 83.1%; Average loss: 2.9829
Iteration: 3326; Percent complete: 83.2%; Average loss: 2.5901
Iteration: 3327; Percent complete: 83.2%; Average loss: 2.8355
Iteration: 3328; Percent complete: 83.2%; Average loss: 2.8455
Iteration: 3329; Percent complete: 83.2%; Average loss: 2.7252
Iteration: 3330; Percent complete: 83.2%; Average loss: 2.9133
Iteration: 3331; Percent complete: 83.3%; Average loss: 2.6623
Iteration: 3332; Percent complete: 83.3%; Average loss: 2.8265
Iteration: 3333; Percent complete: 83.3%; Average loss: 2.6567
Iteration: 3334; Percent complete: 83.4%; Average loss: 2.7338
Iteration: 3335; Percent complete: 83.4%; Average loss: 2.8082
Iteration: 3336; Percent complete: 83.4%; Average loss: 2.6896
Iteration: 3337; Percent complete: 83.4%; Average loss: 2.6481
Iteration: 3338; Percent complete: 83.5%; Average loss: 3.0246
Iteration: 3339; Percent complete: 83.5%; Average loss: 2.8985
Iteration: 3340; Percent complete: 83.5%; Average loss: 2.7855
Iteration: 3341; Percent complete: 83.5%; Average loss: 2.8395
Iteration: 3342; Percent complete: 83.5%; Average loss: 2.8617
Iteration: 3343; Percent complete: 83.6%; Average loss: 2.4894
Iteration: 3344; Percent complete: 83.6%; Average loss: 2.9073
Iteration: 3345; Percent complete: 83.6%; Average loss: 2.6774
Iteration: 3346; Percent complete: 83.7%; Average loss: 2.8881
Iteration: 3347; Percent complete: 83.7%; Average loss: 2.7718
Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6345
Iteration: 3349; Percent complete: 83.7%; Average loss: 2.6276
Iteration: 3350; Percent complete: 83.8%; Average loss: 2.5552
Iteration: 3351; Percent complete: 83.8%; Average loss: 2.5933
Iteration: 3352; Percent complete: 83.8%; Average loss: 2.9425
Iteration: 3353; Percent complete: 83.8%; Average loss: 2.8393
Iteration: 3354; Percent complete: 83.9%; Average loss: 2.8734
Iteration: 3355; Percent complete: 83.9%; Average loss: 2.9761
Iteration: 3356; Percent complete: 83.9%; Average loss: 2.7621
Iteration: 3357; Percent complete: 83.9%; Average loss: 2.8314
Iteration: 3358; Percent complete: 84.0%; Average loss: 2.8362
Iteration: 3359; Percent complete: 84.0%; Average loss: 2.9555
Iteration: 3360; Percent complete: 84.0%; Average loss: 2.9351
Iteration: 3361; Percent complete: 84.0%; Average loss: 2.8396
Iteration: 3362; Percent complete: 84.0%; Average loss: 2.6652
Iteration: 3363; Percent complete: 84.1%; Average loss: 2.7560
Iteration: 3364; Percent complete: 84.1%; Average loss: 2.6976
Iteration: 3365; Percent complete: 84.1%; Average loss: 2.9286
Iteration: 3366; Percent complete: 84.2%; Average loss: 2.8724
Iteration: 3367; Percent complete: 84.2%; Average loss: 2.6606
Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9826
Iteration: 3369; Percent complete: 84.2%; Average loss: 2.6828
Iteration: 3370; Percent complete: 84.2%; Average loss: 2.8508
Iteration: 3371; Percent complete: 84.3%; Average loss: 2.8288
Iteration: 3372; Percent complete: 84.3%; Average loss: 2.5854
Iteration: 3373; Percent complete: 84.3%; Average loss: 2.8255
Iteration: 3374; Percent complete: 84.4%; Average loss: 2.7246
Iteration: 3375; Percent complete: 84.4%; Average loss: 2.6571
Iteration: 3376; Percent complete: 84.4%; Average loss: 2.5491
Iteration: 3377; Percent complete: 84.4%; Average loss: 2.7585
Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7852
Iteration: 3379; Percent complete: 84.5%; Average loss: 2.6650
Iteration: 3380; Percent complete: 84.5%; Average loss: 2.9411
Iteration: 3381; Percent complete: 84.5%; Average loss: 2.8249
Iteration: 3382; Percent complete: 84.5%; Average loss: 2.6304
Iteration: 3383; Percent complete: 84.6%; Average loss: 2.9521
Iteration: 3384; Percent complete: 84.6%; Average loss: 3.0279
Iteration: 3385; Percent complete: 84.6%; Average loss: 2.8668
Iteration: 3386; Percent complete: 84.7%; Average loss: 2.6509
Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7300
Iteration: 3388; Percent complete: 84.7%; Average loss: 2.8834
Iteration: 3389; Percent complete: 84.7%; Average loss: 2.7151
Iteration: 3390; Percent complete: 84.8%; Average loss: 2.8877
Iteration: 3391; Percent complete: 84.8%; Average loss: 2.8917
Iteration: 3392; Percent complete: 84.8%; Average loss: 2.7614
Iteration: 3393; Percent complete: 84.8%; Average loss: 2.7866
Iteration: 3394; Percent complete: 84.9%; Average loss: 2.9455
Iteration: 3395; Percent complete: 84.9%; Average loss: 2.8883
Iteration: 3396; Percent complete: 84.9%; Average loss: 2.8473
Iteration: 3397; Percent complete: 84.9%; Average loss: 2.7834
Iteration: 3398; Percent complete: 85.0%; Average loss: 2.9438
Iteration: 3399; Percent complete: 85.0%; Average loss: 2.6922
Iteration: 3400; Percent complete: 85.0%; Average loss: 2.7118
Iteration: 3401; Percent complete: 85.0%; Average loss: 2.8800
Iteration: 3402; Percent complete: 85.0%; Average loss: 2.6535
Iteration: 3403; Percent complete: 85.1%; Average loss: 2.7536
Iteration: 3404; Percent complete: 85.1%; Average loss: 2.7657
Iteration: 3405; Percent complete: 85.1%; Average loss: 2.5919
Iteration: 3406; Percent complete: 85.2%; Average loss: 2.5177
Iteration: 3407; Percent complete: 85.2%; Average loss: 2.9337
Iteration: 3408; Percent complete: 85.2%; Average loss: 2.6913
Iteration: 3409; Percent complete: 85.2%; Average loss: 2.4926
Iteration: 3410; Percent complete: 85.2%; Average loss: 2.3692
Iteration: 3411; Percent complete: 85.3%; Average loss: 2.8413
Iteration: 3412; Percent complete: 85.3%; Average loss: 2.6993
Iteration: 3413; Percent complete: 85.3%; Average loss: 2.8513
Iteration: 3414; Percent complete: 85.4%; Average loss: 2.8199
Iteration: 3415; Percent complete: 85.4%; Average loss: 2.6471
Iteration: 3416; Percent complete: 85.4%; Average loss: 2.3790
Iteration: 3417; Percent complete: 85.4%; Average loss: 2.5572
Iteration: 3418; Percent complete: 85.5%; Average loss: 2.6620
Iteration: 3419; Percent complete: 85.5%; Average loss: 2.9061
Iteration: 3420; Percent complete: 85.5%; Average loss: 2.9040
Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7081
Iteration: 3422; Percent complete: 85.5%; Average loss: 2.6645
Iteration: 3423; Percent complete: 85.6%; Average loss: 3.0349
Iteration: 3424; Percent complete: 85.6%; Average loss: 2.6892
Iteration: 3425; Percent complete: 85.6%; Average loss: 2.6236
Iteration: 3426; Percent complete: 85.7%; Average loss: 2.7542
Iteration: 3427; Percent complete: 85.7%; Average loss: 2.7056
Iteration: 3428; Percent complete: 85.7%; Average loss: 2.7261
Iteration: 3429; Percent complete: 85.7%; Average loss: 2.6214
Iteration: 3430; Percent complete: 85.8%; Average loss: 2.8812
Iteration: 3431; Percent complete: 85.8%; Average loss: 2.6921
Iteration: 3432; Percent complete: 85.8%; Average loss: 2.7916
Iteration: 3433; Percent complete: 85.8%; Average loss: 2.7689
Iteration: 3434; Percent complete: 85.9%; Average loss: 2.8141
Iteration: 3435; Percent complete: 85.9%; Average loss: 2.8346
Iteration: 3436; Percent complete: 85.9%; Average loss: 2.6709
Iteration: 3437; Percent complete: 85.9%; Average loss: 2.8663
Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8613
Iteration: 3439; Percent complete: 86.0%; Average loss: 2.9386
Iteration: 3440; Percent complete: 86.0%; Average loss: 2.8511
Iteration: 3441; Percent complete: 86.0%; Average loss: 2.7350
Iteration: 3442; Percent complete: 86.1%; Average loss: 2.6627
Iteration: 3443; Percent complete: 86.1%; Average loss: 2.5204
Iteration: 3444; Percent complete: 86.1%; Average loss: 2.9818
Iteration: 3445; Percent complete: 86.1%; Average loss: 2.8522
Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5325
Iteration: 3447; Percent complete: 86.2%; Average loss: 2.7203
Iteration: 3448; Percent complete: 86.2%; Average loss: 2.7108
Iteration: 3449; Percent complete: 86.2%; Average loss: 2.7027
Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8157
Iteration: 3451; Percent complete: 86.3%; Average loss: 2.8417
Iteration: 3452; Percent complete: 86.3%; Average loss: 2.7222
Iteration: 3453; Percent complete: 86.3%; Average loss: 2.7147
Iteration: 3454; Percent complete: 86.4%; Average loss: 2.5535
Iteration: 3455; Percent complete: 86.4%; Average loss: 2.9164
Iteration: 3456; Percent complete: 86.4%; Average loss: 2.9950
Iteration: 3457; Percent complete: 86.4%; Average loss: 3.0773
Iteration: 3458; Percent complete: 86.5%; Average loss: 2.7961
Iteration: 3459; Percent complete: 86.5%; Average loss: 2.7303
Iteration: 3460; Percent complete: 86.5%; Average loss: 2.7104
Iteration: 3461; Percent complete: 86.5%; Average loss: 2.9361
Iteration: 3462; Percent complete: 86.6%; Average loss: 2.7251
Iteration: 3463; Percent complete: 86.6%; Average loss: 2.8491
Iteration: 3464; Percent complete: 86.6%; Average loss: 2.8373
Iteration: 3465; Percent complete: 86.6%; Average loss: 2.4315
Iteration: 3466; Percent complete: 86.7%; Average loss: 2.7381
Iteration: 3467; Percent complete: 86.7%; Average loss: 2.7595
Iteration: 3468; Percent complete: 86.7%; Average loss: 2.7521
Iteration: 3469; Percent complete: 86.7%; Average loss: 2.8254
Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7628
Iteration: 3471; Percent complete: 86.8%; Average loss: 2.8040
Iteration: 3472; Percent complete: 86.8%; Average loss: 2.9286
Iteration: 3473; Percent complete: 86.8%; Average loss: 2.6675
Iteration: 3474; Percent complete: 86.9%; Average loss: 2.7599
Iteration: 3475; Percent complete: 86.9%; Average loss: 2.7778
Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7091
Iteration: 3477; Percent complete: 86.9%; Average loss: 2.6690
Iteration: 3478; Percent complete: 87.0%; Average loss: 2.5496
Iteration: 3479; Percent complete: 87.0%; Average loss: 2.6722
Iteration: 3480; Percent complete: 87.0%; Average loss: 2.5250
Iteration: 3481; Percent complete: 87.0%; Average loss: 2.9266
Iteration: 3482; Percent complete: 87.1%; Average loss: 2.3821
Iteration: 3483; Percent complete: 87.1%; Average loss: 2.8777
Iteration: 3484; Percent complete: 87.1%; Average loss: 2.8514
Iteration: 3485; Percent complete: 87.1%; Average loss: 2.4600
Iteration: 3486; Percent complete: 87.2%; Average loss: 2.7432
Iteration: 3487; Percent complete: 87.2%; Average loss: 2.9001
Iteration: 3488; Percent complete: 87.2%; Average loss: 2.8526
Iteration: 3489; Percent complete: 87.2%; Average loss: 2.6613
Iteration: 3490; Percent complete: 87.2%; Average loss: 2.6243
Iteration: 3491; Percent complete: 87.3%; Average loss: 2.6897
Iteration: 3492; Percent complete: 87.3%; Average loss: 2.6565
Iteration: 3493; Percent complete: 87.3%; Average loss: 2.5146
Iteration: 3494; Percent complete: 87.4%; Average loss: 3.0512
Iteration: 3495; Percent complete: 87.4%; Average loss: 2.9317
Iteration: 3496; Percent complete: 87.4%; Average loss: 3.0604
Iteration: 3497; Percent complete: 87.4%; Average loss: 3.1079
Iteration: 3498; Percent complete: 87.5%; Average loss: 2.6510
Iteration: 3499; Percent complete: 87.5%; Average loss: 2.6081
Iteration: 3500; Percent complete: 87.5%; Average loss: 2.7306
Iteration: 3501; Percent complete: 87.5%; Average loss: 2.8657
Iteration: 3502; Percent complete: 87.5%; Average loss: 2.8483
Iteration: 3503; Percent complete: 87.6%; Average loss: 3.1120
Iteration: 3504; Percent complete: 87.6%; Average loss: 2.5484
Iteration: 3505; Percent complete: 87.6%; Average loss: 2.6289
Iteration: 3506; Percent complete: 87.6%; Average loss: 2.6263
Iteration: 3507; Percent complete: 87.7%; Average loss: 2.5340
Iteration: 3508; Percent complete: 87.7%; Average loss: 2.8179
Iteration: 3509; Percent complete: 87.7%; Average loss: 2.9311
Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6910
Iteration: 3511; Percent complete: 87.8%; Average loss: 2.6775
Iteration: 3512; Percent complete: 87.8%; Average loss: 2.7290
Iteration: 3513; Percent complete: 87.8%; Average loss: 2.7009
Iteration: 3514; Percent complete: 87.8%; Average loss: 2.7493
Iteration: 3515; Percent complete: 87.9%; Average loss: 2.9130
Iteration: 3516; Percent complete: 87.9%; Average loss: 2.6873
Iteration: 3517; Percent complete: 87.9%; Average loss: 2.8826
Iteration: 3518; Percent complete: 87.9%; Average loss: 2.8718
Iteration: 3519; Percent complete: 88.0%; Average loss: 2.4632
Iteration: 3520; Percent complete: 88.0%; Average loss: 2.9662
Iteration: 3521; Percent complete: 88.0%; Average loss: 2.8239
Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7137
Iteration: 3523; Percent complete: 88.1%; Average loss: 2.6854
Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7278
Iteration: 3525; Percent complete: 88.1%; Average loss: 2.5379
Iteration: 3526; Percent complete: 88.1%; Average loss: 2.5188
Iteration: 3527; Percent complete: 88.2%; Average loss: 2.6058
Iteration: 3528; Percent complete: 88.2%; Average loss: 2.8909
Iteration: 3529; Percent complete: 88.2%; Average loss: 2.6096
Iteration: 3530; Percent complete: 88.2%; Average loss: 2.7954
Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7203
Iteration: 3532; Percent complete: 88.3%; Average loss: 2.6732
Iteration: 3533; Percent complete: 88.3%; Average loss: 2.6889
Iteration: 3534; Percent complete: 88.3%; Average loss: 2.9926
Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8530
Iteration: 3536; Percent complete: 88.4%; Average loss: 2.7927
Iteration: 3537; Percent complete: 88.4%; Average loss: 2.9504
Iteration: 3538; Percent complete: 88.4%; Average loss: 2.6733
Iteration: 3539; Percent complete: 88.5%; Average loss: 2.6024
Iteration: 3540; Percent complete: 88.5%; Average loss: 2.5939
Iteration: 3541; Percent complete: 88.5%; Average loss: 2.7755
Iteration: 3542; Percent complete: 88.5%; Average loss: 3.0082
Iteration: 3543; Percent complete: 88.6%; Average loss: 2.5474
Iteration: 3544; Percent complete: 88.6%; Average loss: 2.7644
Iteration: 3545; Percent complete: 88.6%; Average loss: 2.7807
Iteration: 3546; Percent complete: 88.6%; Average loss: 2.4771
Iteration: 3547; Percent complete: 88.7%; Average loss: 2.7120
Iteration: 3548; Percent complete: 88.7%; Average loss: 2.6423
Iteration: 3549; Percent complete: 88.7%; Average loss: 2.6868
Iteration: 3550; Percent complete: 88.8%; Average loss: 2.6867
Iteration: 3551; Percent complete: 88.8%; Average loss: 2.7617
Iteration: 3552; Percent complete: 88.8%; Average loss: 2.7160
Iteration: 3553; Percent complete: 88.8%; Average loss: 2.7792
Iteration: 3554; Percent complete: 88.8%; Average loss: 2.7250
Iteration: 3555; Percent complete: 88.9%; Average loss: 2.5160
Iteration: 3556; Percent complete: 88.9%; Average loss: 2.6513
Iteration: 3557; Percent complete: 88.9%; Average loss: 2.4777
Iteration: 3558; Percent complete: 88.9%; Average loss: 3.0667
Iteration: 3559; Percent complete: 89.0%; Average loss: 2.6068
Iteration: 3560; Percent complete: 89.0%; Average loss: 2.7955
Iteration: 3561; Percent complete: 89.0%; Average loss: 2.7780
Iteration: 3562; Percent complete: 89.0%; Average loss: 2.5255
Iteration: 3563; Percent complete: 89.1%; Average loss: 2.9182
Iteration: 3564; Percent complete: 89.1%; Average loss: 2.6890
Iteration: 3565; Percent complete: 89.1%; Average loss: 2.7686
Iteration: 3566; Percent complete: 89.1%; Average loss: 2.8039
Iteration: 3567; Percent complete: 89.2%; Average loss: 2.9228
Iteration: 3568; Percent complete: 89.2%; Average loss: 2.5671
Iteration: 3569; Percent complete: 89.2%; Average loss: 2.8844
Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8359
Iteration: 3571; Percent complete: 89.3%; Average loss: 2.6615
Iteration: 3572; Percent complete: 89.3%; Average loss: 2.7042
Iteration: 3573; Percent complete: 89.3%; Average loss: 2.7103
Iteration: 3574; Percent complete: 89.3%; Average loss: 2.5302
Iteration: 3575; Percent complete: 89.4%; Average loss: 2.7122
Iteration: 3576; Percent complete: 89.4%; Average loss: 2.6309
Iteration: 3577; Percent complete: 89.4%; Average loss: 2.6525
Iteration: 3578; Percent complete: 89.5%; Average loss: 2.8183
Iteration: 3579; Percent complete: 89.5%; Average loss: 2.7372
Iteration: 3580; Percent complete: 89.5%; Average loss: 2.8244
Iteration: 3581; Percent complete: 89.5%; Average loss: 2.6091
Iteration: 3582; Percent complete: 89.5%; Average loss: 2.5080
Iteration: 3583; Percent complete: 89.6%; Average loss: 2.9124
Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7027
Iteration: 3585; Percent complete: 89.6%; Average loss: 2.5932
Iteration: 3586; Percent complete: 89.6%; Average loss: 2.6253
Iteration: 3587; Percent complete: 89.7%; Average loss: 2.5987
Iteration: 3588; Percent complete: 89.7%; Average loss: 2.7179
Iteration: 3589; Percent complete: 89.7%; Average loss: 2.6970
Iteration: 3590; Percent complete: 89.8%; Average loss: 2.6932
Iteration: 3591; Percent complete: 89.8%; Average loss: 2.8139
Iteration: 3592; Percent complete: 89.8%; Average loss: 2.6318
Iteration: 3593; Percent complete: 89.8%; Average loss: 2.7783
Iteration: 3594; Percent complete: 89.8%; Average loss: 2.6286
Iteration: 3595; Percent complete: 89.9%; Average loss: 2.6090
Iteration: 3596; Percent complete: 89.9%; Average loss: 2.8028
Iteration: 3597; Percent complete: 89.9%; Average loss: 2.7276
Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7901
Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6364
Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7214
Iteration: 3601; Percent complete: 90.0%; Average loss: 2.8412
Iteration: 3602; Percent complete: 90.0%; Average loss: 2.5108
Iteration: 3603; Percent complete: 90.1%; Average loss: 2.7446
Iteration: 3604; Percent complete: 90.1%; Average loss: 2.9707
Iteration: 3605; Percent complete: 90.1%; Average loss: 2.6045
Iteration: 3606; Percent complete: 90.1%; Average loss: 2.6589
Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7334
Iteration: 3608; Percent complete: 90.2%; Average loss: 2.9794
Iteration: 3609; Percent complete: 90.2%; Average loss: 2.8584
Iteration: 3610; Percent complete: 90.2%; Average loss: 2.6656
Iteration: 3611; Percent complete: 90.3%; Average loss: 2.6605
Iteration: 3612; Percent complete: 90.3%; Average loss: 2.6883
Iteration: 3613; Percent complete: 90.3%; Average loss: 2.7843
Iteration: 3614; Percent complete: 90.3%; Average loss: 2.8493
Iteration: 3615; Percent complete: 90.4%; Average loss: 2.7007
Iteration: 3616; Percent complete: 90.4%; Average loss: 2.6629
Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6254
Iteration: 3618; Percent complete: 90.5%; Average loss: 2.7731
Iteration: 3619; Percent complete: 90.5%; Average loss: 2.6492
Iteration: 3620; Percent complete: 90.5%; Average loss: 2.6348
Iteration: 3621; Percent complete: 90.5%; Average loss: 2.5393
Iteration: 3622; Percent complete: 90.5%; Average loss: 2.4841
Iteration: 3623; Percent complete: 90.6%; Average loss: 2.7594
Iteration: 3624; Percent complete: 90.6%; Average loss: 2.5551
Iteration: 3625; Percent complete: 90.6%; Average loss: 2.6043
Iteration: 3626; Percent complete: 90.6%; Average loss: 2.8572
Iteration: 3627; Percent complete: 90.7%; Average loss: 2.7952
Iteration: 3628; Percent complete: 90.7%; Average loss: 2.5598
Iteration: 3629; Percent complete: 90.7%; Average loss: 2.8287
Iteration: 3630; Percent complete: 90.8%; Average loss: 2.7348
Iteration: 3631; Percent complete: 90.8%; Average loss: 2.7169
Iteration: 3632; Percent complete: 90.8%; Average loss: 2.6566
Iteration: 3633; Percent complete: 90.8%; Average loss: 2.6676
Iteration: 3634; Percent complete: 90.8%; Average loss: 2.6422
Iteration: 3635; Percent complete: 90.9%; Average loss: 2.6582
Iteration: 3636; Percent complete: 90.9%; Average loss: 2.7876
Iteration: 3637; Percent complete: 90.9%; Average loss: 2.5723
Iteration: 3638; Percent complete: 91.0%; Average loss: 2.9246
Iteration: 3639; Percent complete: 91.0%; Average loss: 2.5999
Iteration: 3640; Percent complete: 91.0%; Average loss: 2.5870
Iteration: 3641; Percent complete: 91.0%; Average loss: 2.6390
Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7855
Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7385
Iteration: 3644; Percent complete: 91.1%; Average loss: 2.8226
Iteration: 3645; Percent complete: 91.1%; Average loss: 2.3945
Iteration: 3646; Percent complete: 91.1%; Average loss: 2.8902
Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6232
Iteration: 3648; Percent complete: 91.2%; Average loss: 2.8243
Iteration: 3649; Percent complete: 91.2%; Average loss: 2.5406
Iteration: 3650; Percent complete: 91.2%; Average loss: 2.6726
Iteration: 3651; Percent complete: 91.3%; Average loss: 2.4717
Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7889
Iteration: 3653; Percent complete: 91.3%; Average loss: 2.6123
Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6128
Iteration: 3655; Percent complete: 91.4%; Average loss: 2.5295
Iteration: 3656; Percent complete: 91.4%; Average loss: 2.5392
Iteration: 3657; Percent complete: 91.4%; Average loss: 2.7993
Iteration: 3658; Percent complete: 91.5%; Average loss: 2.7344
Iteration: 3659; Percent complete: 91.5%; Average loss: 2.8054
Iteration: 3660; Percent complete: 91.5%; Average loss: 2.6754
Iteration: 3661; Percent complete: 91.5%; Average loss: 2.4879
Iteration: 3662; Percent complete: 91.5%; Average loss: 2.5808
Iteration: 3663; Percent complete: 91.6%; Average loss: 2.6963
Iteration: 3664; Percent complete: 91.6%; Average loss: 2.7837
Iteration: 3665; Percent complete: 91.6%; Average loss: 2.6573
Iteration: 3666; Percent complete: 91.6%; Average loss: 2.8868
Iteration: 3667; Percent complete: 91.7%; Average loss: 2.6719
Iteration: 3668; Percent complete: 91.7%; Average loss: 2.4524
Iteration: 3669; Percent complete: 91.7%; Average loss: 2.7566
Iteration: 3670; Percent complete: 91.8%; Average loss: 2.6485
Iteration: 3671; Percent complete: 91.8%; Average loss: 2.7423
Iteration: 3672; Percent complete: 91.8%; Average loss: 2.6471
Iteration: 3673; Percent complete: 91.8%; Average loss: 2.4449
Iteration: 3674; Percent complete: 91.8%; Average loss: 2.6414
Iteration: 3675; Percent complete: 91.9%; Average loss: 2.5097
Iteration: 3676; Percent complete: 91.9%; Average loss: 2.8132
Iteration: 3677; Percent complete: 91.9%; Average loss: 2.5716
Iteration: 3678; Percent complete: 92.0%; Average loss: 2.5354
Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7579
Iteration: 3680; Percent complete: 92.0%; Average loss: 2.7738
Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7490
Iteration: 3682; Percent complete: 92.0%; Average loss: 2.8270
Iteration: 3683; Percent complete: 92.1%; Average loss: 2.4670
Iteration: 3684; Percent complete: 92.1%; Average loss: 2.9435
Iteration: 3685; Percent complete: 92.1%; Average loss: 2.6117
Iteration: 3686; Percent complete: 92.2%; Average loss: 2.7684
Iteration: 3687; Percent complete: 92.2%; Average loss: 2.7302
Iteration: 3688; Percent complete: 92.2%; Average loss: 2.6142
Iteration: 3689; Percent complete: 92.2%; Average loss: 2.7829
Iteration: 3690; Percent complete: 92.2%; Average loss: 2.4172
Iteration: 3691; Percent complete: 92.3%; Average loss: 2.6407
Iteration: 3692; Percent complete: 92.3%; Average loss: 2.6411
Iteration: 3693; Percent complete: 92.3%; Average loss: 2.5585
Iteration: 3694; Percent complete: 92.3%; Average loss: 2.6748
Iteration: 3695; Percent complete: 92.4%; Average loss: 2.5749
Iteration: 3696; Percent complete: 92.4%; Average loss: 2.7181
Iteration: 3697; Percent complete: 92.4%; Average loss: 2.8311
Iteration: 3698; Percent complete: 92.5%; Average loss: 2.6221
Iteration: 3699; Percent complete: 92.5%; Average loss: 2.6787
Iteration: 3700; Percent complete: 92.5%; Average loss: 2.4961
Iteration: 3701; Percent complete: 92.5%; Average loss: 2.6432
Iteration: 3702; Percent complete: 92.5%; Average loss: 2.3923
Iteration: 3703; Percent complete: 92.6%; Average loss: 2.7051
Iteration: 3704; Percent complete: 92.6%; Average loss: 2.5804
Iteration: 3705; Percent complete: 92.6%; Average loss: 2.5979
Iteration: 3706; Percent complete: 92.7%; Average loss: 2.4688
Iteration: 3707; Percent complete: 92.7%; Average loss: 2.5206
Iteration: 3708; Percent complete: 92.7%; Average loss: 2.8214
Iteration: 3709; Percent complete: 92.7%; Average loss: 2.8291
Iteration: 3710; Percent complete: 92.8%; Average loss: 2.7635
Iteration: 3711; Percent complete: 92.8%; Average loss: 2.5068
Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6246
Iteration: 3713; Percent complete: 92.8%; Average loss: 3.0590
Iteration: 3714; Percent complete: 92.8%; Average loss: 2.4897
Iteration: 3715; Percent complete: 92.9%; Average loss: 2.3458
Iteration: 3716; Percent complete: 92.9%; Average loss: 2.9313
Iteration: 3717; Percent complete: 92.9%; Average loss: 2.7736
Iteration: 3718; Percent complete: 93.0%; Average loss: 2.6306
Iteration: 3719; Percent complete: 93.0%; Average loss: 2.8064
Iteration: 3720; Percent complete: 93.0%; Average loss: 2.6959
Iteration: 3721; Percent complete: 93.0%; Average loss: 2.7046
Iteration: 3722; Percent complete: 93.0%; Average loss: 2.7374
Iteration: 3723; Percent complete: 93.1%; Average loss: 2.4711
Iteration: 3724; Percent complete: 93.1%; Average loss: 2.7568
Iteration: 3725; Percent complete: 93.1%; Average loss: 2.5284
Iteration: 3726; Percent complete: 93.2%; Average loss: 2.6571
Iteration: 3727; Percent complete: 93.2%; Average loss: 2.4700
Iteration: 3728; Percent complete: 93.2%; Average loss: 2.7436
Iteration: 3729; Percent complete: 93.2%; Average loss: 2.7097
Iteration: 3730; Percent complete: 93.2%; Average loss: 2.6919
Iteration: 3731; Percent complete: 93.3%; Average loss: 2.5844
Iteration: 3732; Percent complete: 93.3%; Average loss: 3.0084
Iteration: 3733; Percent complete: 93.3%; Average loss: 2.6111
Iteration: 3734; Percent complete: 93.3%; Average loss: 2.9037
Iteration: 3735; Percent complete: 93.4%; Average loss: 2.7319
Iteration: 3736; Percent complete: 93.4%; Average loss: 2.6545
Iteration: 3737; Percent complete: 93.4%; Average loss: 2.3590
Iteration: 3738; Percent complete: 93.5%; Average loss: 2.5628
Iteration: 3739; Percent complete: 93.5%; Average loss: 2.7605
Iteration: 3740; Percent complete: 93.5%; Average loss: 2.6615
Iteration: 3741; Percent complete: 93.5%; Average loss: 2.4992
Iteration: 3742; Percent complete: 93.5%; Average loss: 2.6185
Iteration: 3743; Percent complete: 93.6%; Average loss: 2.7220
Iteration: 3744; Percent complete: 93.6%; Average loss: 2.5687
Iteration: 3745; Percent complete: 93.6%; Average loss: 2.6348
Iteration: 3746; Percent complete: 93.7%; Average loss: 2.6823
Iteration: 3747; Percent complete: 93.7%; Average loss: 2.4050
Iteration: 3748; Percent complete: 93.7%; Average loss: 2.6360
Iteration: 3749; Percent complete: 93.7%; Average loss: 2.6847
Iteration: 3750; Percent complete: 93.8%; Average loss: 2.5423
Iteration: 3751; Percent complete: 93.8%; Average loss: 2.4419
Iteration: 3752; Percent complete: 93.8%; Average loss: 2.4874
Iteration: 3753; Percent complete: 93.8%; Average loss: 2.5466
Iteration: 3754; Percent complete: 93.8%; Average loss: 2.9755
Iteration: 3755; Percent complete: 93.9%; Average loss: 2.5847
Iteration: 3756; Percent complete: 93.9%; Average loss: 2.6573
Iteration: 3757; Percent complete: 93.9%; Average loss: 2.9109
Iteration: 3758; Percent complete: 94.0%; Average loss: 2.7288
Iteration: 3759; Percent complete: 94.0%; Average loss: 2.6764
Iteration: 3760; Percent complete: 94.0%; Average loss: 2.5076
Iteration: 3761; Percent complete: 94.0%; Average loss: 2.7772
Iteration: 3762; Percent complete: 94.0%; Average loss: 2.6723
Iteration: 3763; Percent complete: 94.1%; Average loss: 2.4418
Iteration: 3764; Percent complete: 94.1%; Average loss: 2.5303
Iteration: 3765; Percent complete: 94.1%; Average loss: 2.9245
Iteration: 3766; Percent complete: 94.2%; Average loss: 2.7428
Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6873
Iteration: 3768; Percent complete: 94.2%; Average loss: 2.6122
Iteration: 3769; Percent complete: 94.2%; Average loss: 2.8401
Iteration: 3770; Percent complete: 94.2%; Average loss: 2.7242
Iteration: 3771; Percent complete: 94.3%; Average loss: 2.6213
Iteration: 3772; Percent complete: 94.3%; Average loss: 2.3705
Iteration: 3773; Percent complete: 94.3%; Average loss: 2.7162
Iteration: 3774; Percent complete: 94.3%; Average loss: 2.6722
Iteration: 3775; Percent complete: 94.4%; Average loss: 2.4917
Iteration: 3776; Percent complete: 94.4%; Average loss: 2.6054
Iteration: 3777; Percent complete: 94.4%; Average loss: 2.6953
Iteration: 3778; Percent complete: 94.5%; Average loss: 2.8521
Iteration: 3779; Percent complete: 94.5%; Average loss: 2.6244
Iteration: 3780; Percent complete: 94.5%; Average loss: 2.5502
Iteration: 3781; Percent complete: 94.5%; Average loss: 3.0270
Iteration: 3782; Percent complete: 94.5%; Average loss: 2.4810
Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6321
Iteration: 3784; Percent complete: 94.6%; Average loss: 2.6537
Iteration: 3785; Percent complete: 94.6%; Average loss: 2.6352
Iteration: 3786; Percent complete: 94.7%; Average loss: 2.5537
Iteration: 3787; Percent complete: 94.7%; Average loss: 2.6139
Iteration: 3788; Percent complete: 94.7%; Average loss: 2.6197
Iteration: 3789; Percent complete: 94.7%; Average loss: 2.7285
Iteration: 3790; Percent complete: 94.8%; Average loss: 2.7635
Iteration: 3791; Percent complete: 94.8%; Average loss: 2.7366
Iteration: 3792; Percent complete: 94.8%; Average loss: 2.7220
Iteration: 3793; Percent complete: 94.8%; Average loss: 2.8766
Iteration: 3794; Percent complete: 94.8%; Average loss: 2.4982
Iteration: 3795; Percent complete: 94.9%; Average loss: 2.5639
Iteration: 3796; Percent complete: 94.9%; Average loss: 2.5792
Iteration: 3797; Percent complete: 94.9%; Average loss: 2.8906
Iteration: 3798; Percent complete: 95.0%; Average loss: 2.6513
Iteration: 3799; Percent complete: 95.0%; Average loss: 2.7346
Iteration: 3800; Percent complete: 95.0%; Average loss: 2.7417
Iteration: 3801; Percent complete: 95.0%; Average loss: 2.5296
Iteration: 3802; Percent complete: 95.0%; Average loss: 2.4656
Iteration: 3803; Percent complete: 95.1%; Average loss: 2.6997
Iteration: 3804; Percent complete: 95.1%; Average loss: 2.5420
Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6946
Iteration: 3806; Percent complete: 95.2%; Average loss: 2.9833
Iteration: 3807; Percent complete: 95.2%; Average loss: 2.6181
Iteration: 3808; Percent complete: 95.2%; Average loss: 2.5057
Iteration: 3809; Percent complete: 95.2%; Average loss: 2.7134
Iteration: 3810; Percent complete: 95.2%; Average loss: 2.7492
Iteration: 3811; Percent complete: 95.3%; Average loss: 2.6979
Iteration: 3812; Percent complete: 95.3%; Average loss: 2.6371
Iteration: 3813; Percent complete: 95.3%; Average loss: 2.7367
Iteration: 3814; Percent complete: 95.3%; Average loss: 2.5101
Iteration: 3815; Percent complete: 95.4%; Average loss: 2.5604
Iteration: 3816; Percent complete: 95.4%; Average loss: 2.3895
Iteration: 3817; Percent complete: 95.4%; Average loss: 2.5798
Iteration: 3818; Percent complete: 95.5%; Average loss: 2.5369
Iteration: 3819; Percent complete: 95.5%; Average loss: 2.6881
Iteration: 3820; Percent complete: 95.5%; Average loss: 2.5964
Iteration: 3821; Percent complete: 95.5%; Average loss: 2.4422
Iteration: 3822; Percent complete: 95.5%; Average loss: 2.5341
Iteration: 3823; Percent complete: 95.6%; Average loss: 2.6443
Iteration: 3824; Percent complete: 95.6%; Average loss: 2.7012
Iteration: 3825; Percent complete: 95.6%; Average loss: 2.7828
Iteration: 3826; Percent complete: 95.7%; Average loss: 2.6444
Iteration: 3827; Percent complete: 95.7%; Average loss: 2.7535
Iteration: 3828; Percent complete: 95.7%; Average loss: 2.7342
Iteration: 3829; Percent complete: 95.7%; Average loss: 2.4832
Iteration: 3830; Percent complete: 95.8%; Average loss: 2.6639
Iteration: 3831; Percent complete: 95.8%; Average loss: 2.4859
Iteration: 3832; Percent complete: 95.8%; Average loss: 2.8703
Iteration: 3833; Percent complete: 95.8%; Average loss: 2.5154
Iteration: 3834; Percent complete: 95.9%; Average loss: 2.7461
Iteration: 3835; Percent complete: 95.9%; Average loss: 2.6598
Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6063
Iteration: 3837; Percent complete: 95.9%; Average loss: 2.6233
Iteration: 3838; Percent complete: 96.0%; Average loss: 2.6242
Iteration: 3839; Percent complete: 96.0%; Average loss: 2.8047
Iteration: 3840; Percent complete: 96.0%; Average loss: 2.5327
Iteration: 3841; Percent complete: 96.0%; Average loss: 2.6423
Iteration: 3842; Percent complete: 96.0%; Average loss: 2.6511
Iteration: 3843; Percent complete: 96.1%; Average loss: 2.4475
Iteration: 3844; Percent complete: 96.1%; Average loss: 2.7603
Iteration: 3845; Percent complete: 96.1%; Average loss: 2.4596
Iteration: 3846; Percent complete: 96.2%; Average loss: 2.6402
Iteration: 3847; Percent complete: 96.2%; Average loss: 2.7302
Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6615
Iteration: 3849; Percent complete: 96.2%; Average loss: 2.4723
Iteration: 3850; Percent complete: 96.2%; Average loss: 2.7343
Iteration: 3851; Percent complete: 96.3%; Average loss: 2.7442
Iteration: 3852; Percent complete: 96.3%; Average loss: 2.6477
Iteration: 3853; Percent complete: 96.3%; Average loss: 2.5590
Iteration: 3854; Percent complete: 96.4%; Average loss: 2.5009
Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7304
Iteration: 3856; Percent complete: 96.4%; Average loss: 2.6179
Iteration: 3857; Percent complete: 96.4%; Average loss: 2.6437
Iteration: 3858; Percent complete: 96.5%; Average loss: 2.6710
Iteration: 3859; Percent complete: 96.5%; Average loss: 2.7254
Iteration: 3860; Percent complete: 96.5%; Average loss: 2.5728
Iteration: 3861; Percent complete: 96.5%; Average loss: 2.7087
Iteration: 3862; Percent complete: 96.5%; Average loss: 2.5224
Iteration: 3863; Percent complete: 96.6%; Average loss: 2.5253
Iteration: 3864; Percent complete: 96.6%; Average loss: 2.4973
Iteration: 3865; Percent complete: 96.6%; Average loss: 2.3932
Iteration: 3866; Percent complete: 96.7%; Average loss: 2.5885
Iteration: 3867; Percent complete: 96.7%; Average loss: 2.7824
Iteration: 3868; Percent complete: 96.7%; Average loss: 2.7303
Iteration: 3869; Percent complete: 96.7%; Average loss: 2.8342
Iteration: 3870; Percent complete: 96.8%; Average loss: 2.6155
Iteration: 3871; Percent complete: 96.8%; Average loss: 2.7106
Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6723
Iteration: 3873; Percent complete: 96.8%; Average loss: 2.4291
Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6030
Iteration: 3875; Percent complete: 96.9%; Average loss: 2.4746
Iteration: 3876; Percent complete: 96.9%; Average loss: 2.6608
Iteration: 3877; Percent complete: 96.9%; Average loss: 2.7455
Iteration: 3878; Percent complete: 97.0%; Average loss: 2.7632
Iteration: 3879; Percent complete: 97.0%; Average loss: 2.5642
Iteration: 3880; Percent complete: 97.0%; Average loss: 2.3951
Iteration: 3881; Percent complete: 97.0%; Average loss: 2.5219
Iteration: 3882; Percent complete: 97.0%; Average loss: 2.6190
Iteration: 3883; Percent complete: 97.1%; Average loss: 2.8569
Iteration: 3884; Percent complete: 97.1%; Average loss: 2.5101
Iteration: 3885; Percent complete: 97.1%; Average loss: 2.5361
Iteration: 3886; Percent complete: 97.2%; Average loss: 2.5738
Iteration: 3887; Percent complete: 97.2%; Average loss: 2.6133
Iteration: 3888; Percent complete: 97.2%; Average loss: 2.7991
Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5416
Iteration: 3890; Percent complete: 97.2%; Average loss: 2.5235
Iteration: 3891; Percent complete: 97.3%; Average loss: 2.6923
Iteration: 3892; Percent complete: 97.3%; Average loss: 2.7874
Iteration: 3893; Percent complete: 97.3%; Average loss: 2.4941
Iteration: 3894; Percent complete: 97.4%; Average loss: 2.6830
Iteration: 3895; Percent complete: 97.4%; Average loss: 2.7087
Iteration: 3896; Percent complete: 97.4%; Average loss: 2.5512
Iteration: 3897; Percent complete: 97.4%; Average loss: 2.7469
Iteration: 3898; Percent complete: 97.5%; Average loss: 2.7134
Iteration: 3899; Percent complete: 97.5%; Average loss: 2.5190
Iteration: 3900; Percent complete: 97.5%; Average loss: 2.7308
Iteration: 3901; Percent complete: 97.5%; Average loss: 2.7401
Iteration: 3902; Percent complete: 97.5%; Average loss: 2.6683
Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6329
Iteration: 3904; Percent complete: 97.6%; Average loss: 2.5472
Iteration: 3905; Percent complete: 97.6%; Average loss: 2.6098
Iteration: 3906; Percent complete: 97.7%; Average loss: 2.6070
Iteration: 3907; Percent complete: 97.7%; Average loss: 2.7004
Iteration: 3908; Percent complete: 97.7%; Average loss: 2.6224
Iteration: 3909; Percent complete: 97.7%; Average loss: 2.6315
Iteration: 3910; Percent complete: 97.8%; Average loss: 2.7108
Iteration: 3911; Percent complete: 97.8%; Average loss: 2.5894
Iteration: 3912; Percent complete: 97.8%; Average loss: 2.5861
Iteration: 3913; Percent complete: 97.8%; Average loss: 2.4238
Iteration: 3914; Percent complete: 97.9%; Average loss: 2.6302
Iteration: 3915; Percent complete: 97.9%; Average loss: 2.3918
Iteration: 3916; Percent complete: 97.9%; Average loss: 2.5667
Iteration: 3917; Percent complete: 97.9%; Average loss: 2.7287
Iteration: 3918; Percent complete: 98.0%; Average loss: 2.8957
Iteration: 3919; Percent complete: 98.0%; Average loss: 2.5609
Iteration: 3920; Percent complete: 98.0%; Average loss: 2.6459
Iteration: 3921; Percent complete: 98.0%; Average loss: 2.8214
Iteration: 3922; Percent complete: 98.0%; Average loss: 2.4650
Iteration: 3923; Percent complete: 98.1%; Average loss: 2.7163
Iteration: 3924; Percent complete: 98.1%; Average loss: 2.6693
Iteration: 3925; Percent complete: 98.1%; Average loss: 2.5800
Iteration: 3926; Percent complete: 98.2%; Average loss: 2.7373
Iteration: 3927; Percent complete: 98.2%; Average loss: 2.6720
Iteration: 3928; Percent complete: 98.2%; Average loss: 2.4867
Iteration: 3929; Percent complete: 98.2%; Average loss: 2.6583
Iteration: 3930; Percent complete: 98.2%; Average loss: 2.5096
Iteration: 3931; Percent complete: 98.3%; Average loss: 2.9107
Iteration: 3932; Percent complete: 98.3%; Average loss: 2.5779
Iteration: 3933; Percent complete: 98.3%; Average loss: 2.5709
Iteration: 3934; Percent complete: 98.4%; Average loss: 2.6207
Iteration: 3935; Percent complete: 98.4%; Average loss: 2.9017
Iteration: 3936; Percent complete: 98.4%; Average loss: 2.6471
Iteration: 3937; Percent complete: 98.4%; Average loss: 2.8703
Iteration: 3938; Percent complete: 98.5%; Average loss: 2.6123
Iteration: 3939; Percent complete: 98.5%; Average loss: 2.5535
Iteration: 3940; Percent complete: 98.5%; Average loss: 2.5669
Iteration: 3941; Percent complete: 98.5%; Average loss: 2.6354
Iteration: 3942; Percent complete: 98.6%; Average loss: 2.7116
Iteration: 3943; Percent complete: 98.6%; Average loss: 2.6624
Iteration: 3944; Percent complete: 98.6%; Average loss: 2.6446
Iteration: 3945; Percent complete: 98.6%; Average loss: 2.5375
Iteration: 3946; Percent complete: 98.7%; Average loss: 2.7349
Iteration: 3947; Percent complete: 98.7%; Average loss: 2.4954
Iteration: 3948; Percent complete: 98.7%; Average loss: 2.5012
Iteration: 3949; Percent complete: 98.7%; Average loss: 2.5889
Iteration: 3950; Percent complete: 98.8%; Average loss: 2.6728
Iteration: 3951; Percent complete: 98.8%; Average loss: 2.5596
Iteration: 3952; Percent complete: 98.8%; Average loss: 2.6458
Iteration: 3953; Percent complete: 98.8%; Average loss: 2.7054
Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6882
Iteration: 3955; Percent complete: 98.9%; Average loss: 2.5917
Iteration: 3956; Percent complete: 98.9%; Average loss: 2.5727
Iteration: 3957; Percent complete: 98.9%; Average loss: 2.8703
Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4320
Iteration: 3959; Percent complete: 99.0%; Average loss: 2.7789
Iteration: 3960; Percent complete: 99.0%; Average loss: 2.2746
Iteration: 3961; Percent complete: 99.0%; Average loss: 2.4238
Iteration: 3962; Percent complete: 99.1%; Average loss: 2.6802
Iteration: 3963; Percent complete: 99.1%; Average loss: 2.3586
Iteration: 3964; Percent complete: 99.1%; Average loss: 2.8622
Iteration: 3965; Percent complete: 99.1%; Average loss: 2.7323
Iteration: 3966; Percent complete: 99.2%; Average loss: 2.5206
Iteration: 3967; Percent complete: 99.2%; Average loss: 2.4179
Iteration: 3968; Percent complete: 99.2%; Average loss: 2.5335
Iteration: 3969; Percent complete: 99.2%; Average loss: 2.5269
Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5906
Iteration: 3971; Percent complete: 99.3%; Average loss: 2.4696
Iteration: 3972; Percent complete: 99.3%; Average loss: 2.7056
Iteration: 3973; Percent complete: 99.3%; Average loss: 2.7517
Iteration: 3974; Percent complete: 99.4%; Average loss: 2.4791
Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5498
Iteration: 3976; Percent complete: 99.4%; Average loss: 2.5887
Iteration: 3977; Percent complete: 99.4%; Average loss: 2.5572
Iteration: 3978; Percent complete: 99.5%; Average loss: 2.3568
Iteration: 3979; Percent complete: 99.5%; Average loss: 2.5008
Iteration: 3980; Percent complete: 99.5%; Average loss: 2.5920
Iteration: 3981; Percent complete: 99.5%; Average loss: 2.6363
Iteration: 3982; Percent complete: 99.6%; Average loss: 2.5730
Iteration: 3983; Percent complete: 99.6%; Average loss: 2.6398
Iteration: 3984; Percent complete: 99.6%; Average loss: 2.6089
Iteration: 3985; Percent complete: 99.6%; Average loss: 2.3198
Iteration: 3986; Percent complete: 99.7%; Average loss: 2.4799
Iteration: 3987; Percent complete: 99.7%; Average loss: 2.5157
Iteration: 3988; Percent complete: 99.7%; Average loss: 2.4743
Iteration: 3989; Percent complete: 99.7%; Average loss: 2.6713
Iteration: 3990; Percent complete: 99.8%; Average loss: 2.5821
Iteration: 3991; Percent complete: 99.8%; Average loss: 2.5586
Iteration: 3992; Percent complete: 99.8%; Average loss: 2.6279
Iteration: 3993; Percent complete: 99.8%; Average loss: 2.5887
Iteration: 3994; Percent complete: 99.9%; Average loss: 2.5636
Iteration: 3995; Percent complete: 99.9%; Average loss: 2.3607
Iteration: 3996; Percent complete: 99.9%; Average loss: 2.4145
Iteration: 3997; Percent complete: 99.9%; Average loss: 2.5746
Iteration: 3998; Percent complete: 100.0%; Average loss: 2.5166
Iteration: 3999; Percent complete: 100.0%; Average loss: 2.6843
Iteration: 4000; Percent complete: 100.0%; Average loss: 2.7039
</pre></div>
</div>
</div>
<div class="section" id="id19">
<h3>평가 수행하기<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>여러분의 모델과 채팅을 해보고 싶다면 다음 블록을 수행하면 됩니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dropout 레이어를 평가 모드로 설정합니다</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 탐색 모듈을 초기화합니다</span>
<span class="n">searcher</span> <span class="o">=</span> <span class="n">GreedySearchDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>

<span class="c1"># 채팅을 시작합니다 (다음 줄의 주석을 제거하면 시작해볼 수 있습니다)</span>
<span class="c1"># evaluateInput(encoder, decoder, searcher, voc)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id20">
<h2>맺음말<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h2>
<p>이번 튜토리얼을 이것으로 마무리하겠습니다. 축하합니다! 여러분은 이제 생성
챗봇 모델을 만들기 위한 기초 지식을 습득했습니다. 만약 좀 더 관심이 있다면
모델이나 학습 패러미터를 수정해 보면서, 혹은 모델을 학습할 데이터를 바꿔
보면서 챗봇의 행동을 수정해볼 수 있을 것입니다.</p>
<p>그 외에도 딥러닝의 멋진 활용 예에 대한 PyTorch 튜토리얼이 있으니 한 번
확인해 보기 바랍니다!</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 4 minutes  57.931 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-chatbot-tutorial-py">
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../_downloads/d90127b0deeb355be3a350521d770206/chatbot_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">chatbot_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../_downloads/44a84f8c1764dbf61662d306ff9ed43a/chatbot_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">chatbot_tutorial.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr class="helpful-hr hr-top">
      <div class="helpful-container">
        <div class="helpful-question">이 문서가 도움이 되었나요?</div>
        <div class="helpful-question yes-link" data-behavior="was-this-helpful-event" data-response="yes">네</div>
        <div class="helpful-question no-link" data-behavior="was-this-helpful-event" data-response="no">아니오</div>
        <div class="was-helpful-thank-you">피드백을 주셔서 감사합니다.</div>
      </div>
    <hr class="helpful-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">챗봇 튜토리얼</a><ul>
<li><a class="reference internal" href="#id3">준비 단계</a></li>
<li><a class="reference internal" href="#id4">데이터 읽기 &amp; 전처리하기</a><ul>
<li><a class="reference internal" href="#id5">원하는 형식의 데이터 파일로 만들기</a></li>
<li><a class="reference internal" href="#id6">데이터 읽고 정리하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id7">모델을 위한 데이터 준비하기</a></li>
<li><a class="reference internal" href="#id8">모델 정의하기</a><ul>
<li><a class="reference internal" href="#seq2seq">Seq2Seq 모델</a></li>
<li><a class="reference internal" href="#id9">인코더</a></li>
<li><a class="reference internal" href="#id10">디코더</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11">학습 프로시저 정의하기</a><ul>
<li><a class="reference internal" href="#masked-loss">Masked loss</a></li>
<li><a class="reference internal" href="#id12">한 번의 학습 단계</a></li>
<li><a class="reference internal" href="#id13">학습 단계</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id14">평가 정의하기</a><ul>
<li><a class="reference internal" href="#id15">탐욕적 디코딩</a></li>
<li><a class="reference internal" href="#id16">내 텍스트 평가하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">모델 수행하기</a><ul>
<li><a class="reference internal" href="#id18">학습 수행하기</a></li>
<li><a class="reference internal" href="#id19">평가 수행하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id20">맺음말</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-3', 'auto');
  ga('send', 'pageview');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: 'Download',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   $("[data-behavior='was-this-helpful-event']").on('click', function(){
    $(".helpful-question").hide();
    $(".was-helpful-thank-you").show();
    ga('send', {
      hitType: 'event',
      eventCategory: 'Was this Helpful?',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   if (location.pathname == "/") {
     $(".helpful-container").hide();
     $(".hr-bottom").hide();
   }
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="footer-container container">
      <div class="footer-logo-wrapper"><a href="https://pytorch.kr" class="footer-logo"></a></div>
      <div class="footer-links-wrapper pb-2">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org">PyTorch 홈페이지 (공식)</a></li>
            <li><a href="https://pytorch.org">공식 홈페이지</a></li>
            <li><a href="https://pytorch.org/tutorials">공식 튜토리얼</a></li>
            <li><a href="https://pytorch.org/docs">공식 문서</a></li>
          </ul>
        </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr">한국어 홈페이지 (비공식)</a></li>
            <li><a href="https://pytorch.kr/about" class="">사이트 소개</a></li>
            <li><a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a></li>
            <li><a href="https://github.com/9bow/PyTorch-tutorials-kr" target="_blank">한국어 튜토리얼 저장소</a></li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>