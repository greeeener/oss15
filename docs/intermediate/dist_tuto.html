


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch로 분산 어플리케이션 개발하기 &mdash; PyTorch Tutorials 1.6.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Getting Started with Distributed RPC Framework" href="rpc_tutorial.html" />
    <link rel="prev" title="Getting Started with Distributed Data Parallel" href="ddp_tutorial.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<body class="pytorch-body">
  <nav class="navbar sticky-top navbar-dark fixed-top navbar-expand-lg" style="background: rgba(55,55,55,.8)">
    <div class="container-fluid">
      <div class="navbar-brand">
        <a href="https://pytorch.kr/" aria-label="PyTorch">
          <img src="../_static/images/logo-kr.svg" width="260" height="28" fill="white" />
        </a>
      </div>
      <button type="button" aria-label="Toggle navigation" class="navbar-toggler collapsed" aria-expanded="false" aria-controls="nav-collapse"><span class="navbar-toggler-icon"></span></button>
      <div id="nav-collapse" class="navbar-collapse collapse">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a href="//pytorch.kr/" target="_self" class="nav-link">홈</a></li>
          <li class="nav-item">
            <a href="//tutorials.pytorch.kr/" target="_self" class="nav-link">튜토리얼</a>
          </li>
          <li class="nav-item">
            <a href="//pytorch.kr/about" target="_self" class="nav-link">
              소개
            </a></li>
        </ul>
      </div>
    </div>
  </nav>

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.6.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">모든 레시피 보기</a></li>
</ul>
<p class="caption"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">파이토치(PyTorch)로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision 객체 검출 미세조정(Finetuning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_preprocessing_tutorial.html">torchaudio Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">nn.Transformer 와 TorchText 로 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">TorchText로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_translation_tutorial.html">TorchText로 언어 번역하기</a></li>
</ul>
<p class="caption"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="flask_rest_api_tutorial.html">Flask를 이용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
</ul>
<p class="caption"><span class="caption-text">프론트엔드 API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="named_tensor_tutorial.html">(prototype) Introduction to Named Tensors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Dispatcher in C++</a></li>
</ul>
<p class="caption"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(beta) 컴퓨터 비전(Vision) 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
</ul>
<p class="caption"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">단일 머신을 이용한 모델 병렬화 실습 예제</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/aws_distributed_training_tutorial.html">(advanced) PyTorch 1.0 Distributed Trainer with Amazon AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>PyTorch로 분산 어플리케이션 개발하기</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/intermediate/dist_tuto.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">intermediate/dist_tuto</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pytorch">
<h1>PyTorch로 분산 어플리케이션 개발하기<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h1>
<dl class="simple">
<dt><strong>Author</strong>: <a class="reference external" href="https://seba1511.com">Séb Arnold</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/9bow">박정환</a></p>
</dd>
</dl>
<p>선수과목(Prerequisites):</p>
<ul class="simple">
<li><p><a class="reference external" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></p></li>
</ul>
<p>이 짧은 튜토리얼에서는 PyTorch의 분산 패키지를 둘러볼 예정입니다.
여기에서는 어떻게 분산 환경을 설정하는지와 서로 다른 통신 방법을 사용하는지를
알아보고, 패키지 내부도 일부 살펴보도록 하겠습니다.</p>
<div class="section" id="setup">
<h2>설정(Setup)<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<!--
* Processes & machines
* variables and init_process_group
--><p>PyTorch에 포함된 분산 패키지(예. <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>)는 연구자와 실무자가
여러 프로세스와 클러스터의 기기에서 계산을 쉽게 병렬화 할 수 있게 합니다.
이를 위해, 각 프로세스가 다른 프로세스와 데이터를 교환할 수 있도록 메시지 교환
규약(messaging passing semantics)을 활용합니다. 멀티프로세싱(<code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code>)
패키지와 달리, 프로세스는 다른 커뮤니케이션 백엔드(backend)를 사용할 수 있으며
동일 기기 상에서 실행되는 것에 제약이 없습니다.</p>
<p>이 튜토리얼을 시작하기 위해 여러 프로세스를 동시에 실행할 수 있어야 합니다.
연산 클러스터에 접근하는 경우에는 시스템 관리자에게 확인하거나 선호하는 코디네이션
도구(coordination tool)를 사용하시면 됩니다. (예. <a class="reference external" href="https://linux.die.net/man/1/pdsh">pdsh</a>,
<a class="reference external" href="https://cea-hpc.github.io/clustershell/">clustershell</a>, 또는
<a class="reference external" href="https://slurm.schedmd.com/">others</a>) 이 튜토리얼에서는 다음 템플릿을 사용하여
단일 기기에서 여러 프로세스를 생성(fork)하겠습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;run.py:&quot;&quot;&quot;</span>
<span class="c1">#!/usr/bin/env python</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Distributed function to be implemented later. &quot;&quot;&quot;</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">run</span><span class="p">))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>위 스크립트는 2개의 프로세스를 생성(spawn)하여 각자 다른 분산 환경을 설정하고,
프로세스 그룹(<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code>)을 초기화하고, 최종적으로는 <code class="docutils literal notranslate"><span class="pre">run</span></code>
함수를 실행합니다.</p>
<p>이제 <code class="docutils literal notranslate"><span class="pre">init_process</span></code> 함수를 살펴보도록 하겠습니다. 이 함수는 모든 프로세스가
마스터를 통해 조정(coordinate)될 수 있도록 동일한 IP 주소와 포트를 사용합니다.
여기에서는 <code class="docutils literal notranslate"><span class="pre">gloo</span></code> 백엔드를 사용하였으나 다른 백엔드들도 사용이 가능합니다.
(<a class="reference external" href="#communication-backends">섹션 5.1</a> 참고) 이 튜토리얼의 마지막 부분에 있는
<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code> 에서 일어나는 놀라운 일을 살펴볼 것이지만, 기본적으로는
프로세스가 자신의 위치를 공유함으로써 서로 통신할 수 있도록 합니다.</p>
</div>
<div class="section" id="point-to-point">
<h2>지점-대-지점 간(Point-to-Point) 통신<a class="headerlink" href="#point-to-point" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="../_images/send_recv.png"><img alt="송신과 수신" src="../_images/send_recv.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">송신과 수신</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>하나의 프로세스에서 다른 프로세스로 데이터를 전송하는 것을 지점-대-지점 간 통신이라고 합니다.
지점간 통신은  <code class="docutils literal notranslate"><span class="pre">send</span></code> 와 <code class="docutils literal notranslate"><span class="pre">recv</span></code> 함수 또는 즉시 응답하는(<em>immediate</em> counter-parts)
<code class="docutils literal notranslate"><span class="pre">isend</span></code> 와 <code class="docutils literal notranslate"><span class="pre">irecv</span></code> 를 사용합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;블로킹(blocking) 지점-대-지점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>위 예제에서 두 프로세스는 값이 0인 Tensor로 시작한 후, 0번 프로세스가 Tensor의 값을
증가시킨 후 1번 프로세스로 보내서 둘 다 1.0으로 종료됩니다. 이 떄, 프로세스 1은
수신한 데이터를 저장할 메모리를 할당해두야 합니다.</p>
<p>또한 <code class="docutils literal notranslate"><span class="pre">send</span></code>/<code class="docutils literal notranslate"><span class="pre">recv</span></code> 는 모두 <strong>블로킹</strong> 입니다: 두 프로세스는 통신이 완료될 때까지
멈춰있습니다. 반면에 즉시 응답하는 것이 <strong>논-블로킹</strong> 입니다; 스크립트는 실행을
계속하고 메소드는 <code class="docutils literal notranslate"><span class="pre">wait()</span></code> 를 선택할 수 있는 <code class="docutils literal notranslate"><span class="pre">Work</span></code> 객체를 반환합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;논-블로킹(non-blocking) 지점-대-지점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">req</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank 0 started sending&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 started receiving&#39;</span><span class="p">)</span>
    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>즉시 응답하는 함수들을 사용할 때는 보내고 받는 Tensor에 대한 사용법에 주의해야 합니다.
데이터가 언제 다른 프로세스로 송수신되는지 모르기 때문에, <code class="docutils literal notranslate"><span class="pre">req.wait()</span></code> 가 완료되기
전에는 전송된 Tensor를 수정하거나 수신된 Tensor에 접근해서는 안됩니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.isend()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 쓰면 정의되지 않은 동작이 발생합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.irecv()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 읽으면 정의되지 않은 동작이 발생합니다.</p></li>
</ul>
<p>그러나, <code class="docutils literal notranslate"><span class="pre">req.wait()</span></code> 를 실행한 후에는 통신이 이루어진 것을 보장받을 수 있기 때문에,
<code class="docutils literal notranslate"><span class="pre">tensor[0]</span></code> 에 저장된 값은 1.0이 됩니다.</p>
<p>지점-대-지점 간 통신은 프로세스 간 통신에 대한 세밀한 제어를 원할 때 유용합니다.
<a class="reference external" href="https://github.com/baidu-research/baidu-allreduce">바이두(Baidu)의 DeepSpeech</a> 나
<a class="reference external" href="https://research.fb.com/publications/imagenet1kin1h/">페이스북(Facebook)의 대규모 실험</a>
에서 사용하는 것과 같은 멋진 알고리즘을 구현할 때 사용할 수 있습니다.
(<a class="reference external" href="#ring-allreduce">섹션 4.1</a> 참고)</p>
</div>
<div class="section" id="collective-communication">
<h2>집합 통신(Collective Communication)<a class="headerlink" href="#collective-communication" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="../_images/scatter.png"><img alt="Scatter" src="../_images/scatter.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Scatter</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
</td>
<td><div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="../_images/gather.png"><img alt="Gather" src="../_images/gather.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Gather</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="../_images/reduce.png"><img alt="Reduce" src="../_images/reduce.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Reduce</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
</td>
<td><div class="figure align-center" id="id6">
<a class="reference internal image-reference" href="../_images/all_reduce.png"><img alt="All-Reduce" src="../_images/all_reduce.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">All-Reduce</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="figure align-center" id="id7">
<a class="reference internal image-reference" href="../_images/broadcast.png"><img alt="Broadcast" src="../_images/broadcast.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">Broadcast</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
</td>
<td><div class="figure align-center" id="id8">
<a class="reference internal image-reference" href="../_images/all_gather.png"><img alt="All-Gather" src="../_images/all_gather.png" style="width: 100%;" /></a>
<p class="caption"><span class="caption-text">All-Gather</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
</tbody>
</table>
<p>지점-대-지점 간 통신과 달리 집합 통신은 <strong>그룹</strong> 의 모든 프로세스에 걸친 통신 패턴을
허용합니다. 그룹은 모든 프로세스의 부분 집합입니다. 그룹을 생성하기 위해서는
<code class="docutils literal notranslate"><span class="pre">dist.new_group(group)</span></code> 에 순서(rank) 목록을 전달합니다. 기본적으로, 집합 통신은
<strong>월드(world)</strong> 라고 부르는 전체 프로세스에서 실행됩니다. 예를 들어, 모든 프로세스에
존재하는 모든 Tensor들의 합을 얻기 위해서는 <code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 을
사용하면 됩니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; All-Reduce 예제 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; 간단한 지점-대-지점 간 통신 &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>그룹 내의 모든 Tensor들의 합이 필요하기 떄문에, <code class="docutils literal notranslate"><span class="pre">dist.reduce_op.SUM</span></code> 을
리듀스(reduce) 연산자로 사용하였습니다. 일반적으로, 교환 법칙이 허용되는(commutative)
모든 수학 연산을 연산자로 사용할 수 있습니다. PyTorch는 요소별(element-wise)로
동작하는 기본적으로 4개의 연산자를 제공합니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.SUM</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.PRODUCT</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.MAX</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce_op.MIN</span></code>.</p></li>
</ul>
<p>PyTorch에는 현재 <code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 외에도 6개의 집합 통신이
구현되어 있습니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.broadcast(tensor,</span> <span class="pre">src,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">src</span></code> 의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 모든 프로세스에
복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce(tensor,</span> <span class="pre">dst,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">op</span></code> 를 모든 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 적용한 뒤
결과를 <code class="docutils literal notranslate"><span class="pre">dst</span></code> 에 저장합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: 리듀스와 동일하지만, 결과가 모든
프로세스에 저장됩니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.scatter(tensor,</span> <span class="pre">src,</span> <span class="pre">scatter_list,</span> <span class="pre">group)</span></code>: <span class="math notranslate nohighlight">\(i^{\text{번째}}\)</span> Tensor
<code class="docutils literal notranslate"><span class="pre">scatter_list[i]</span></code> 를 <span class="math notranslate nohighlight">\(i^{\text{번째}}\)</span> 프로세스에 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.gather(tensor,</span> <span class="pre">dst,</span> <span class="pre">gather_list,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">dst</span></code> 의 모든 프로세스에서
<code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.all_gather(tensor_list,</span> <span class="pre">tensor,</span> <span class="pre">group)</span></code>: 모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를
모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor_list</span></code> 에 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.barrier(group)</span></code>: <cite>group</cite> 내의 모든 프로세스가 이 함수에 진입할 때까지
<cite>group</cite> 내의 모든 프로세스를 멈춥(block)니다.</p></li>
</ul>
</div>
<div class="section" id="distributed-training">
<h2>분산 학습(Distributed Training)<a class="headerlink" href="#distributed-training" title="Permalink to this headline">¶</a></h2>
<!--
* Gloo Backend
* Simple all_reduce on the gradients
* Point to optimized DistributedDataParallel

TODO: Custom ring-allreduce
--><p><strong>참고:</strong> 이 섹션의 예제 스크립트들은 <a class="reference external" href="https://github.com/seba-1511/dist_tuto.pth/">이 GitHub 저장소</a>
에서 찾아보실 수 있습니다.</p>
<p>이제 분산 모듈이 어떻게 동작하는지 이해했으므로, 유용한 뭔가를 작성해보겠습니다.
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a> 의
기능을 복제해보는 것이 목표입니다. 물론, 이것은 교훈적인(didactic) 예제이므로
실제 상황에서는 위에 링크된 잘 테스트되고 최적화된 공식 버전을 사용해야 합니다.</p>
<p>매우 간단하게 확률적 경사 하강법(SGD)의 분산 버전을 구현해보겠습니다. 스크립트는
모든 프로세스가 각자의 데이터 배치(batch)에서 각자의 모델의 변화도(gradient)를
계산한 후 평균을 계산합니다. 프로세스의 수를 변경해도 유사한 수렴 결과를 보장하기
위해, 먼저 데이터셋을 분할해야 합니다.
(아래 코드 대신 <a class="reference external" href="https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4">tnt.dataset.SplitDataset</a>
을 사용해도 됩니다.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 데이터셋 분할 헬퍼(helper) &quot;&quot;&quot;</span>
<span class="k">class</span> <span class="nc">Partition</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">data_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">DataPartitioner</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1234</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">Random</span><span class="p">()</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">data_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_len</span><span class="p">)]</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">frac</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="n">part_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">data_len</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indexes</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">part_len</span><span class="p">])</span>
            <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexes</span><span class="p">[</span><span class="n">part_len</span><span class="p">:]</span>

    <span class="k">def</span> <span class="nf">use</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Partition</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">[</span><span class="n">partition</span><span class="p">])</span>
</pre></div>
</div>
<p>위 코드를 사용하여 어떤 데이터셋도 몇 줄의 코드로 간단히 분할할 수 있습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; MNIST 데이터셋 분할 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">partition_dataset</span><span class="p">():</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                             <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">bsz</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">partition_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">DataPartitioner</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">partition_sizes</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">())</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">partition</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="n">bsz</span><span class="p">,</span>
                                         <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span>
</pre></div>
</div>
<p>2개의 복제본이 있다고 가정하고, 각각의 프로세스가 60000 / 2 = 30000 샘플의
<code class="docutils literal notranslate"><span class="pre">train_set</span></code> 을 가질 것입니다. 또한 <strong>전체</strong> 배치 크기를 128로 유지하기 위해
배치 크기를 복제본 수로 나누도록 하겠습니다.</p>
<p>이제 일반적인 순전파-역전파-최적화 학습 코드를 작성하고, 모델의 변화도 평균을
계산하는 함수를 추가하겠습니다. (아래 코드는 공식
<a class="reference external" href="https://github.com/pytorch/examples/blob/master/mnist/main.py">PyTorch MNIST 예제</a>
에서 많은 부분을 차용하였습니다.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 분산 동기(synchronous) SGD 예제 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
    <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">=</span> <span class="n">partition_dataset</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">num_batches</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">bsz</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_set</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span> <span class="s1">&#39;, epoch &#39;</span><span class="p">,</span>
              <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;: &#39;</span><span class="p">,</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">)</span>
</pre></div>
</div>
<p>모델을 받아 전체 월드(world)의 평균 변화도를 계산하는 <code class="docutils literal notranslate"><span class="pre">average_gradients(model)</span></code>
함수를 구현하는 것이 남았습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 변화도 평균 계산하기 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">size</span>
</pre></div>
</div>
<p><em>완성(Et voilà)</em>! 분산 동기(synchronous) SGD를 성공적으로 구현했으며 어떤 모델도
대형 연산 클러스터에서 학습할 수 있습니다.</p>
<p><strong>참고:</strong> 마지막 문장은 <em>기술적으로는</em> 참이지만, 동기식 SGD를 상용 수준(production-level)으로
구현하기 위해서는 <a class="reference external" href="https://seba-1511.github.io/dist_blog">더 많은 트릭</a> 이 필요합니다.
다시 말씀드리지만, <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel">테스트되고 최적화된</a>
것을 사용하십시오.</p>
<div class="section" id="ring-allreduce">
<h3>사용자 정의 링-올리듀스(Ring-Allreduce)<a class="headerlink" href="#ring-allreduce" title="Permalink to this headline">¶</a></h3>
<p>추가로 DeepSpeech의 효율적인 링 올리듀스(ring allreduce)를 구현하고 싶다고 가정해보겠습니다.
이것은 지점-대-지점 집합 통신(point-to-point collectives)으로 쉽게 구현할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 링-리듀스(ring-reduce) 구현 &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">allreduce</span><span class="p">(</span><span class="n">send</span><span class="p">,</span> <span class="n">recv</span><span class="p">):</span>
   <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
   <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
   <span class="n">send_buff</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
   <span class="n">recv_buff</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
   <span class="n">accum</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

   <span class="n">left</span> <span class="o">=</span> <span class="p">((</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">size</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>
   <span class="n">right</span> <span class="o">=</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>

   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
       <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
           <span class="c1"># Send send_buff</span>
           <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
           <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
           <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">recv_buff</span><span class="p">[:]</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="c1"># Send recv_buff</span>
           <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
           <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
           <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">send_buff</span><span class="p">[:]</span>
       <span class="n">send_req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
   <span class="n">recv</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">accum</span><span class="p">[:]</span>
</pre></div>
</div>
<p>위 스크립트에서, <code class="docutils literal notranslate"><span class="pre">allreduct(send,</span> <span class="pre">recv)</span></code> 함수는 PyTorch에 있는 것과는 약간
다른 특징을 가지고 있습니다. 이는 <code class="docutils literal notranslate"><span class="pre">recv</span></code> Tensor를 받은 후 모든 <code class="docutils literal notranslate"><span class="pre">send</span></code> Tensor의
합을 저장합니다. 여기에서 구현한 것과 DeepSpeech와는 다른 부분이 여전히 다른 부분이
있는데, 이것은 숙제로 남겨두도록 하겠습니다: DeepSpeech의 구현은 통신 대역폭을
최적으로 확용하기 위해 변화도 Tensor를 <em>덩어리(chunk)</em> 로 나눕니다.
(힌트: <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.chunk">torch.chunk</a>)</p>
</div>
</div>
<div class="section" id="advanced-topics">
<h2>고급 주제(Advanced Topics)<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h2>
<p>이제 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 보다 진보된 기능들을 살펴볼 준비가 되었습니다.
다루어야 할 주제들이 많으므로, 이 섹션을 다음과 같이 2개의 하위 섹션으로 나누도록
하겠습니다:</p>
<ol class="arabic simple">
<li><p>통신 백엔드: GPU와 GPU 간의 통신을 위해 MPI와 Gloo를 어떻게 사용해야 할지 배웁니다.</p></li>
<li><p>초기화 방법: <code class="docutils literal notranslate"><span class="pre">dist.init_process_group()</span></code> 에서 초기 구성 단계를 잘 설정하는 방법을
이해합니다.</p></li>
</ol>
<div class="section" id="communication-backends">
<h3>통신 백엔드(Communication Backends)<a class="headerlink" href="#communication-backends" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 가장 우아한 면 중 하나는 다른 백엔드를 기반으로 추상화하고
구축하는 기능입니다. 앞에서 언급한 것처럼 현재 PyTorch에는 Gloo, NCLL 및 MPI의
세 가지 백엔드가 구현되어 있습니다. 각각은 원하는 사용 사례에 따라 서로 다른 스펙과
트레이드오프(tradeoffs)를 갖습니다. 지원하는 기능의 비교표는
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#module-torch.distributed">여기</a>
에서 찾아보실 수 있습니다.</p>
<p><strong>Gloo 백엔드</strong></p>
<p>지금껏 우리는 <a class="reference external" href="https://github.com/facebookincubator/gloo">Gloo backend</a> 를
광범위하게 사용했습니다. 이것은 미리 컴파일된 PyTOrch 바이너리가 포함되어 있으며
Linux(0.2 이상)와 macOS(1.3 이상)을 모두 지원하고 있어 개발 플랫폼으로 매우 편리합니다.
또한 CPU에서는 모든 저짐-대-지점 및 집합 연산들을, GPU에서는 집합 연산을 지원합니다.
CUDA Tensor에 대한 집합 연산 구현은 NCCL 백엔드에서 제공하는 것만큼 최적화되어
있지는 않습니다.</p>
<p>알고 계시겠지만, 위에서 만든 분산 SGD 예제는 GPU에 <code class="docutils literal notranslate"><span class="pre">model</span></code> 을 올리면 동작하지
않습니다. 여러 GPU를 사용하기 위해서는 아래와 같이 수정이 필요합니다:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">=</span> <span class="pre">torch.device(&quot;cuda:{}&quot;.format(rank))</span></code> 사용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net()</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net().to(device)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data,</span> <span class="pre">target</span> <span class="pre">=</span> <span class="pre">data.to(device),</span> <span class="pre">target.to(device)</span></code> 사용</p></li>
</ol>
<p>위와 같이 변경하고 나면 이제 2개의 GPU에서 모델이 학습을 하며, <code class="docutils literal notranslate"><span class="pre">watch</span> <span class="pre">nvidia-smi</span></code>
로 사용률을 모니터링할 수 있습니다.</p>
<p><strong>MPI 백엔드</strong></p>
<p>MPI(Message Passing Interface)는 고성능 컴퓨팅 분야의 표준 도구입니다.
이는 지점-대-지점 간 통신과 집합 통신을 허용하며 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 API에
영감을 주었습니다. 다양한 목적에 따라 최적화된 몇몇 MPI 구현체들(예.
<a class="reference external" href="https://www.open-mpi.org/">Open-MPI</a>,
<a class="reference external" href="http://mvapich.cse.ohio-state.edu/">MVAPICH2</a>,
<a class="reference external" href="https://software.intel.com/en-us/intel-mpi-library">Intel MPI</a> )이 있습니다.
MPI 백엔드를 사용하는 이점은 대규모 연산 클러스에서의 MPI의 폭넓은 가용성(과 높은
수준의 최적화)에 있습니다. 또한, <a class="reference external" href="https://developer.nvidia.com/mvapich">일부</a>
<a class="reference external" href="https://developer.nvidia.com/ibm-spectrum-mpi">최신</a>
<a class="reference external" href="https://www.open-mpi.org/">구현체들</a> 은 CPU를 통한 메모리 복사를 방지하기 위해
CUDA IPC와 GPU Direct 기술을 활용하고 있습니다.</p>
<p>불행하게도 PyTorch 바이너리는 MPI 구현을 포함할 수 없으므로 직접 재컴파일해야
합니다. 다행히도 이 과정은 매우 간단해서 PyTorch가 <em>스스로</em> 사용 가능한 MPI 구현체를
찾아볼 것입니다. 다음 단계들은 PyTorch를 <a class="reference external" href="https://github.com/pytorch/pytorch#from-source">소스로부터</a>
설치함으로써 MPI 백엔드를 설치하는 과정입니다.</p>
<ol class="arabic simple">
<li><p>아나콘다(Anaconda) 환경을 생성하고 활성화한 뒤
<a class="reference external" href="https://github.com/pytorch/pytorch#from-source">이 가이드</a> 를 따라서 모든
필요 사항들을 설치하시되, <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 은 아직 실행하지 <strong>마십시오.</strong></p></li>
<li><p>선호하는 MPI 구현체를 선택하고 설치하십시오. CUDA를 인식하는 MPI를 활성화하기
위해서는 추가적인 단계가 필요할 수 있습니다. 여기에서는 Open-MPI를 GPU <em>없이</em>
사용하도록 하겠습니다: <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">openmpi</span></code></p></li>
<li><p>이제, 복제해둔 PyTorch 저장소로 가서 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 을 실행하겠습니다.</p></li>
</ol>
<p>새로 설치한 백엔드를 테스트해보기 위해, 약간의 수정을 해보겠습니다.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">'__main__':</span></code> 아래 내용을 <code class="docutils literal notranslate"><span class="pre">init_process(0,</span> <span class="pre">0,</span> <span class="pre">run,</span> <span class="pre">backend='mpi')</span></code>
으로 변경합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">4</span> <span class="pre">python</span> <span class="pre">myscript.py</span></code> 을 실행합니다.</p></li>
</ol>
<p>이러한 변경 사항은 MPI가 프로세스를 생성(spawn)하기 전에 자체적인 환경을 만들기
위해 필요합니다. MPI는 자신의 프로세스를 생성하고 <a class="reference external" href="#initialization-methods">초기화 방법</a>
에 설명된 핸드쉐이크(handshake)를 수행하여 <code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> 의 <code class="docutils literal notranslate"><span class="pre">rank</span></code> 와
<code class="docutils literal notranslate"><span class="pre">size</span></code> 인자를 불필요하게 만듭니다. 이는 각 프로세스에 연산 리소스를 조절(tailor)할
수 있도록 추가적인 인자를 <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> 으로 전달할 수 있기 때문에 매우 강력합니다.
(프로세스당 코어 개수, 장비(machine)의 우선 순위 수동 할당 및
<a class="reference external" href="https://www.open-mpi.org/faq/?category=running#mpirun-hostfile">기타 다른 것</a>)
이렇게 함으로써, 다른 통신 백엔드와 같은 유사한 결과를 얻을 수 있습니다.</p>
<p><strong>NCCL 백엔드</strong></p>
<p><a class="reference external" href="https://github.com/nvidia/nccl">NCCL 백엔드</a> 는 CUDA Tensor들에 대한 집합 연산의
최적화된 구현체를 제공합니다. 집합 연산에 CUDA Tensor만 사용하는 경우, 동급 최고
성능을 위해 이 백엔드를 사용하는 것을 고려해보시기 바랍니다. NCCL 백엔드는 미리
빌드(pre-built)된 바이너리에 CUDA 지원과 함께 포함되어 있습니다.</p>
</div>
<div class="section" id="initialization-methods">
<h3>초기화 방법(Initialization Methods)<a class="headerlink" href="#initialization-methods" title="Permalink to this headline">¶</a></h3>
<p>마지막으로, 처음 호출했던 함수를 알아보겠습니다: <code class="docutils literal notranslate"><span class="pre">dist.init_process_group(backend,</span> <span class="pre">init_method)</span></code>
특히 각 프로세스 간의 초기 조정(initial coordination) 단계를 담당하는 다양한 초기화
방법들을 살펴보도록 하겠습니다. 이러한 방법들은 어떻게 이러한 조정이 수행되는지를
정의할 수 있게 합니다. 하드웨어 설정에 따라 이러한 방법들 중 하나가 다른 방법들보다
더 적합할 수 있습니다.
다음 섹션 외에도 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#initialization">공식 문서</a>
를 참고하실 수 있습니다.</p>
<p><strong>환경 변수</strong></p>
<p>이 튜토리얼에서 지금까지는 환경 변수의 초기화 메소드를 사용해왔습니다. 모든 기기에서
아래 네가지 환경 변수를 설정하게 되면, 모든 프로세스들이 마스터(master)에 적합하게
연결하고, 다른 프로세스들의 정보를 얻은 후 핸드쉐이크까지 할 수 있습니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>: 0-순위의 프로세스를 호스트할 기기의 비어있는 포트 번호(free port)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code>: 0-순위의 프로세스를 호스트할 기기의 IP 주소</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code>: 전체 프로세스 수 - 마스터가 얼마나 많은 워커들을 기다릴지 알 수 있습니다</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK</span></code>: 각 프로세스의 우선순위 - 워커의 마스터 여부를 확인할 수 있습니다.</p></li>
</ul>
<p><strong>공유 파일 시스템</strong></p>
<p>공유 파일 시스템은 모든 프로세스가 공유된 파일에의 접근 및 프로세스들간의 공유 파일을
조정(coordinate)하기 위해 필요합니다. 이것은 각 프로세스가 파일을 열고, 정보를 쓰고,
다른 프로세스들이 작업을 완료할 때까지 기다리게 하는 것을 뜻합니다. 필요한 모든
정보는 모든 프로세스들이 쉽게 사용할 수 있도록 합니다. 경쟁 조건(race conditions)을
피하기 위해, 파일 시스템은 반드시 <a class="reference external" href="http://man7.org/linux/man-pages/man2/fcntl.2.html">fcntl</a>
을 이용한 잠금을 지원해야 합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;file:///mnt/nfs/sharedfile&#39;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>TCP</strong></p>
<p>0-순위 프로세스의 IP 주소와 접근 가능한 포트 번호가 있으면 TCP를 통한 초기화를 할
수 있습니다. 모든 워커들은 0-순위의 프로세스에 연결하고 서로 정보를 교환하는 방법에
대한 정보를 공유합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://10.1.1.20:23456&#39;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<!--
## Internals
* The magic behind init_process_group:

1. validate and parse the arguments
2. resolve the backend: name2channel.at()
3. Drop GIL & THDProcessGroupInit: instantiate the channel and add address of master from config
4. rank 0 inits master, others workers
5. master: create sockets for all workers -> wait for all workers to connect -> send them each the info about location of other processes
6. worker: create socket to master, send own info, receive info about each worker, and then handshake with each of them
7. By this time everyone has handshake with everyone.
--><center><p><strong>감사의 말</strong></p>
</center><p>PyTorch 개발자분들께 구현, 문서화 및 테스트를 잘해주신 것에 감사드립니다. 코드가
불분명할 때는 언제나 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">문서</a>
또는 <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py">테스트</a>
에서 답을 찾을 수 있었습니다. 또한 튜토리얼 초안에 대해 통찰력있는 의견과 질문에
답변을 해주신 Soumith Chintala, Adam Paszke 그리고 Natalia Gimelshei께도 감사드립니다.</p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rpc_tutorial.html" class="btn btn-neutral float-right" title="Getting Started with Distributed RPC Framework" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="ddp_tutorial.html" class="btn btn-neutral" title="Getting Started with Distributed Data Parallel" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="helpful-hr hr-top">
      <div class="helpful-container">
        <div class="helpful-question">이 문서가 도움이 되었나요?</div>
        <div class="helpful-question yes-link" data-behavior="was-this-helpful-event" data-response="yes">네</div>
        <div class="helpful-question no-link" data-behavior="was-this-helpful-event" data-response="no">아니오</div>
        <div class="was-helpful-thank-you">피드백을 주셔서 감사합니다.</div>
      </div>
    <hr class="helpful-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch로 분산 어플리케이션 개발하기</a><ul>
<li><a class="reference internal" href="#setup">설정(Setup)</a></li>
<li><a class="reference internal" href="#point-to-point">지점-대-지점 간(Point-to-Point) 통신</a></li>
<li><a class="reference internal" href="#collective-communication">집합 통신(Collective Communication)</a></li>
<li><a class="reference internal" href="#distributed-training">분산 학습(Distributed Training)</a><ul>
<li><a class="reference internal" href="#ring-allreduce">사용자 정의 링-올리듀스(Ring-Allreduce)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-topics">고급 주제(Advanced Topics)</a><ul>
<li><a class="reference internal" href="#communication-backends">통신 백엔드(Communication Backends)</a></li>
<li><a class="reference internal" href="#initialization-methods">초기화 방법(Initialization Methods)</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-3', 'auto');
  ga('send', 'pageview');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: 'Download',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   $("[data-behavior='was-this-helpful-event']").on('click', function(){
    $(".helpful-question").hide();
    $(".was-helpful-thank-you").show();
    ga('send', {
      hitType: 'event',
      eventCategory: 'Was this Helpful?',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   if (location.pathname == "/") {
     $(".helpful-container").hide();
     $(".hr-bottom").hide();
   }
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="footer-container container">
      <div class="footer-logo-wrapper"><a href="https://pytorch.kr" class="footer-logo"></a></div>
      <div class="footer-links-wrapper pb-2">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org">PyTorch 홈페이지 (공식)</a></li>
            <li><a href="https://pytorch.org">공식 홈페이지</a></li>
            <li><a href="https://pytorch.org/tutorials">공식 튜토리얼</a></li>
            <li><a href="https://pytorch.org/docs">공식 문서</a></li>
          </ul>
        </div>
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr">한국어 홈페이지 (비공식)</a></li>
            <li><a href="https://pytorch.kr/about" class="">사이트 소개</a></li>
            <li><a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a></li>
            <li><a href="https://github.com/9bow/PyTorch-tutorials-kr" target="_blank">한국어 튜토리얼 저장소</a></li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>