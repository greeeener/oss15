"""
Pytorch 프로파일러
====================================
이 레시피에서는 어떻게 Pytorch 프로파일러를 사용하는지, 또한 모델 운용자의 메모리 소비와 시간측정을 살펴보겠습니다.

개요
------------
Pytorch는 사용자가 모델안에서 가장 비싼 운용자를 정하는게 필요할때 유용하고 간단한 프로파일러 API 포함하고 있습니다.

이 레시피에서는 간단한 Resnet 모델을 사용하여 어떻게 프로파일러가 모델 퍼포먼스를 분석하는지 살펴보겠습니다.

설정
-----
``torch`` 와 ``torchvision`` 을 설치하기 위해서 아래의 커맨드를 입력합니다. 


::

   pip install torch torchvision


"""


######################################################################
# 단계(Steps)
# -----
#
# 1. 필요한 라이브러리들 불러오기
# 2. 간단한 Resnet 모델 인스턴스화 하기
# 3. 프로파일러를 사용하여 실행시간 분석하기
# 4. 프로파일러를 사용하여 메모리 소비 분석하기
# 5. 추적기능 사용하기
#
# 1. 필요한 라이브러리들 불러오기
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# 이 레시피에서는 ``torch`` 와 ``torchvision.models``
# 그리고 ``profiler`` 모듈을 사용합니다:
#

import torch
import torchvision.models as models
import torch.autograd.profiler as profiler


######################################################################
# 2. 간단한 Resnet 모델 인스턴스화 하기
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Resnet 모델 인스턴스를 만들고 입력값을
# 준비합니다 :
#

model = models.resnet18()
inputs = torch.randn(5, 3, 224, 224)

######################################################################
# 3. 프로파일러를 사용하여 실행시간 분석하기
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Pytorch 프로파일러는 컨텍스트 메니저를 통해 실행할 수 있고
# 몇개의 매개변수를 받아드립니다. 그중 가장 유용한 것들은 아래와 같이 있습니다. :
#
# - ``record_shapes`` - 운용자의 인풋의 모양을 기록;
# - ``profile_memory`` - 모델의 텐서 메모리 소비값을 보고;
# - ``use_cuda`` - CUDA 커넬의 실행시간을 측정;
#
# 프로파일러를 사용하여 어떻게 실행시간을 분석하는지 보겠습니다. :

with profiler.profile(record_shapes=True) as prof:
    with profiler.record_function("model_inference"):
        model(inputs)

######################################################################
# ``record_function`` 컨텍스트 관리자를 사용하여
# 임의 코드 범위에 사용자가 제공한 이름을 표시할 수 있습니다.
# (``model_inference`` 은 위의 예제에서 라벨표기에 사용됐습니다.)
# 프로파일러는 프로파일러 컨텍스트 관리자로 포장된 코드 범위의
# 실행 중에 호출된 운용자들을 확인할 수 있습니다.
# 만약 여러 프로파일러 범위들이 동시에 활성화(평행한 Pytorch 쓰레드)된 경우,
# 각 프로파일링 컨텍스트 관리자는 해당범위의 운용자만 추적합니다.
# 프로파일러는 또한 ``torch.jit._fork`` 로 실행된 비동기 작업과
# backward pass의 경우 ``backward()`` 호출로
# 실행된 backward pass 운용자를 자동으로 프로파일링합니다.
#
# 위의 실행 통계를 인쇄해 봅시다 :

print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

######################################################################
# 출력값 예시 (몇몇 열은 제외):

# -------------------------  --------------  ----------  ------------  ---------
# Name                       Self CPU total   CPU total  CPU time avg  # Calls
# -------------------------  --------------  ----------  ------------  ---------
# model_inference            3.541ms         69.571ms    69.571ms      1
# conv2d                     69.122us        40.556ms    2.028ms       20
# convolution                79.100us        40.487ms    2.024ms       20
# _convolution               349.533us       40.408ms    2.020ms       20
# mkldnn_convolution         39.822ms        39.988ms    1.999ms       20
# batch_norm                 105.559us       15.523ms    776.134us     20
# _batch_norm_impl_index     103.697us       15.417ms    770.856us     20
# native_batch_norm          9.387ms         15.249ms    762.471us     20
# max_pool2d                 29.400us        7.200ms     7.200ms       1
# max_pool2d_with_indices    7.154ms         7.170ms     7.170ms       1
# -------------------------  --------------  ----------  ------------  ---------

######################################################################
# 여기서 우리는 예상대로 대부분의 시간이 콘볼루션에서 보낸다는 것이 보입니다.
# (특히 MKL-DNN 지원으로 컴파일된 Pytorch안에서의 ``mkldnn_convolution`` 기능)
# 자체 CPU 시간과 CPU 시간 간의 차이점에 유의하셔야 합니다. 운용자들는 다른 운용자들을 호출을 할 수 있으며,
# 자체 CPU 시간은 하위 운용자 호출에 소비되는 시간을 제외하며, 총 CPU 시간은 이를 포함합니다.
#
# 보다 세부적인 결과 정보를 얻고 운용자 입력 구조을 포함하려면 ``group_by_input_shape=True`` 를 패스합니다 :

print(prof.key_averages(group_by_input_shape=True).table(
    sort_by="cpu_time_total", row_limit=10))

# (몇몇 열은 제외)
# -------------------------  -----------  --------  -------------------------------------
# Name                       CPU total    # Calls         Input Shapes
# -------------------------  -----------  --------  -------------------------------------
# model_inference            69.571ms     1         []
# conv2d                     9.019ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]
# convolution                9.006ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]
# _convolution               8.982ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]
# mkldnn_convolution         8.894ms      4         [[5, 64, 56, 56], [64, 64, 3, 3], []]
# max_pool2d                 7.200ms      1         [[5, 64, 112, 112]]
# conv2d                     7.189ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]
# convolution                7.180ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]
# _convolution               7.171ms      3         [[5, 512, 7, 7], [512, 512, 3, 3], []]
# max_pool2d_with_indices    7.170ms      1         [[5, 64, 112, 112]]
# -------------------------  -----------  --------  --------------------------------------


######################################################################
# 4. 프로파일러를 사용하여 메모리 소비 분석하기
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Pytorch 프로파일러는 모델의 운용자 실행중 할당되거나 해제된
# (모델의 텐서들이 사용된) 메모리 양을 보여줍니다.
# 아래의 출력값에서, 'self' 메모리는 운용자에 의해 다른 운용자 할당된 (혹은 방출된) 메모리와 일치합니다.
# 아래 출력에서 'self' 메모리는 다른 운영자에 대한 자식 호출을
# 제외하고 운영자가 할당(해제)한 메모리에 해당합니다.
# 메모리 프로파일링 기능을 실행하려면 ``profile_memory=True`` 를 패스하세요.

with profiler.profile(profile_memory=True, record_shapes=True) as prof:
    model(inputs)

print(prof.key_averages().table(sort_by="self_cpu_memory_usage", row_limit=10))

# (몇몇 열은 제외)
# ---------------------------  ---------------  ---------------  ---------------
# Name                         CPU Mem          Self CPU Mem     Number of Calls
# ---------------------------  ---------------  ---------------  ---------------
# empty                        94.79 Mb         94.79 Mb         123
# resize_                      11.48 Mb         11.48 Mb         2
# addmm                        19.53 Kb         19.53 Kb         1
# empty_strided                4 b              4 b              1
# conv2d                       47.37 Mb         0 b              20
# ---------------------------  ---------------  ---------------  ---------------

print(prof.key_averages().table(sort_by="cpu_memory_usage", row_limit=10))

# (몇몇 열은 제외)
# ---------------------------  ---------------  ---------------  ---------------
# Name                         CPU Mem          Self CPU Mem     Number of Calls
# ---------------------------  ---------------  ---------------  ---------------
# empty                        94.79 Mb         94.79 Mb         123
# batch_norm                   47.41 Mb         0 b              20
# _batch_norm_impl_index       47.41 Mb         0 b              20
# native_batch_norm            47.41 Mb         0 b              20
# conv2d                       47.37 Mb         0 b              20
# convolution                  47.37 Mb         0 b              20
# _convolution                 47.37 Mb         0 b              20
# mkldnn_convolution           47.37 Mb         0 b              20
# empty_like                   47.37 Mb         0 b              20
# max_pool2d                   11.48 Mb         0 b              1
# max_pool2d_with_indices      11.48 Mb         0 b              1
# resize_                      11.48 Mb         11.48 Mb         2
# addmm                        19.53 Kb         19.53 Kb         1
# adaptive_avg_pool2d          10.00 Kb         0 b              1
# mean                         10.00 Kb         0 b              1
# ---------------------------  ---------------  ---------------  ---------------

######################################################################
# 5. 추적기능 사용하기
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# 프로파일링 결과는 .json trace 파일로 출력할 수 있습니다. :

with profiler.profile() as prof:
    with profiler.record_function("model_inference"):
        model(inputs)

prof.export_chrome_trace("trace.json")

######################################################################
# 사용자는 프로파일된 운용자 순서를 trace 파일을 불러 온 후
# 크롬에서 검토할 수 있습니다. (``chrome://tracing``):
#
# .. image:: ../../_static/img/trace_img.png
#    :scale: 25 %

######################################################################
# 더 알아보기
# ----------
#
# 아래의 튜토리얼을 통해 어떻게 TensorBoard를 사용하여 당신의 모델을 시각화 하는지 살펴보세요 :
#
# -  `모델 시각화, 데이터, 그리고 TensorBoard를 사용한 트레이닝 <https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html>`_ 튜토리얼
#
